{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gaze Detection - Eye Contact Analysis\n",
                "\n",
                "This notebook implements eye contact detection using MediaPipe Face Mesh.\n",
                "\n",
                "## Goals\n",
                "1. Set up MediaPipe Face Mesh for landmark detection\n",
                "2. Calculate gaze direction from iris position\n",
                "3. Define \"looking at camera\" threshold\n",
                "4. Test on sample videos/webcam\n",
                "5. Export utility functions for backend integration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies if needed\n",
                "# !pip install mediapipe opencv-python numpy matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# Add project root to path\n",
                "PROJECT_ROOT = Path(os.getcwd()).parent.parent.parent\n",
                "sys.path.insert(0, str(PROJECT_ROOT))\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import mediapipe as mp\n",
                "\n",
                "print(f\"OpenCV version: {cv2.__version__}\")\n",
                "print(f\"MediaPipe version: {mp.__version__}\")\n",
                "print(f\"Project root: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize MediaPipe Face Mesh\n",
                "\n",
                "MediaPipe Face Mesh provides 468 facial landmarks including eye and iris landmarks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize MediaPipe components\n",
                "mp_face_mesh = mp.solutions.face_mesh\n",
                "mp_drawing = mp.solutions.drawing_utils\n",
                "mp_drawing_styles = mp.solutions.drawing_styles\n",
                "\n",
                "# Face Mesh with refine landmarks for iris tracking\n",
                "face_mesh = mp_face_mesh.FaceMesh(\n",
                "    max_num_faces=1,\n",
                "    refine_landmarks=True,  # Enables iris tracking\n",
                "    min_detection_confidence=0.5,\n",
                "    min_tracking_confidence=0.5\n",
                ")\n",
                "\n",
                "print(\"MediaPipe Face Mesh initialized with iris tracking\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Key Landmark Indices\n",
                "\n",
                "MediaPipe provides specific landmarks for eyes and iris."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Eye landmark indices (MediaPipe Face Mesh)\n",
                "# Left eye (from viewer's perspective, subject's right eye)\n",
                "LEFT_EYE_INDICES = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]\n",
                "\n",
                "# Right eye (from viewer's perspective, subject's left eye)\n",
                "RIGHT_EYE_INDICES = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]\n",
                "\n",
                "# Iris landmarks (with refine_landmarks=True)\n",
                "# Left iris\n",
                "LEFT_IRIS_INDICES = [474, 475, 476, 477]\n",
                "LEFT_IRIS_CENTER = 473\n",
                "\n",
                "# Right iris\n",
                "RIGHT_IRIS_INDICES = [469, 470, 471, 472]\n",
                "RIGHT_IRIS_CENTER = 468\n",
                "\n",
                "# Eye corners for calculating gaze direction\n",
                "LEFT_EYE_INNER_CORNER = 362\n",
                "LEFT_EYE_OUTER_CORNER = 263\n",
                "RIGHT_EYE_INNER_CORNER = 133\n",
                "RIGHT_EYE_OUTER_CORNER = 33\n",
                "\n",
                "print(\"Eye landmark indices defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_landmark_coords(landmarks, indices, img_width, img_height):\n",
                "    \"\"\"\n",
                "    Get pixel coordinates for given landmark indices.\n",
                "    \n",
                "    Args:\n",
                "        landmarks: MediaPipe landmark list\n",
                "        indices: List of landmark indices\n",
                "        img_width: Image width in pixels\n",
                "        img_height: Image height in pixels\n",
                "    \n",
                "    Returns:\n",
                "        numpy array of (x, y) coordinates\n",
                "    \"\"\"\n",
                "    coords = []\n",
                "    for idx in indices:\n",
                "        lm = landmarks[idx]\n",
                "        x = int(lm.x * img_width)\n",
                "        y = int(lm.y * img_height)\n",
                "        coords.append([x, y])\n",
                "    return np.array(coords)\n",
                "\n",
                "\n",
                "def get_single_landmark(landmarks, idx, img_width, img_height):\n",
                "    \"\"\"Get single landmark as (x, y) tuple.\"\"\"\n",
                "    lm = landmarks[idx]\n",
                "    return (int(lm.x * img_width), int(lm.y * img_height))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Gaze Direction Calculation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_gaze_ratio(landmarks, img_width, img_height):\n",
                "    \"\"\"\n",
                "    Calculate gaze ratio for each eye.\n",
                "    \n",
                "    The ratio indicates where the iris is positioned within the eye:\n",
                "    - 0.0: Looking far left\n",
                "    - 0.5: Looking center (at camera)\n",
                "    - 1.0: Looking far right\n",
                "    \n",
                "    Returns:\n",
                "        tuple: (left_eye_ratio, right_eye_ratio, average_ratio)\n",
                "    \"\"\"\n",
                "    # Get iris centers\n",
                "    left_iris = get_single_landmark(landmarks, LEFT_IRIS_CENTER, img_width, img_height)\n",
                "    right_iris = get_single_landmark(landmarks, RIGHT_IRIS_CENTER, img_width, img_height)\n",
                "    \n",
                "    # Get eye corners\n",
                "    left_inner = get_single_landmark(landmarks, LEFT_EYE_INNER_CORNER, img_width, img_height)\n",
                "    left_outer = get_single_landmark(landmarks, LEFT_EYE_OUTER_CORNER, img_width, img_height)\n",
                "    right_inner = get_single_landmark(landmarks, RIGHT_EYE_INNER_CORNER, img_width, img_height)\n",
                "    right_outer = get_single_landmark(landmarks, RIGHT_EYE_OUTER_CORNER, img_width, img_height)\n",
                "    \n",
                "    # Calculate horizontal ratio for left eye\n",
                "    left_eye_width = abs(left_outer[0] - left_inner[0])\n",
                "    if left_eye_width > 0:\n",
                "        left_ratio = (left_iris[0] - min(left_inner[0], left_outer[0])) / left_eye_width\n",
                "    else:\n",
                "        left_ratio = 0.5\n",
                "    \n",
                "    # Calculate horizontal ratio for right eye\n",
                "    right_eye_width = abs(right_outer[0] - right_inner[0])\n",
                "    if right_eye_width > 0:\n",
                "        right_ratio = (right_iris[0] - min(right_inner[0], right_outer[0])) / right_eye_width\n",
                "    else:\n",
                "        right_ratio = 0.5\n",
                "    \n",
                "    # Average both eyes\n",
                "    avg_ratio = (left_ratio + right_ratio) / 2\n",
                "    \n",
                "    return left_ratio, right_ratio, avg_ratio\n",
                "\n",
                "\n",
                "def get_gaze_direction(gaze_ratio, center_threshold=0.15):\n",
                "    \"\"\"\n",
                "    Determine gaze direction from ratio.\n",
                "    \n",
                "    Args:\n",
                "        gaze_ratio: Value between 0-1 (0.5 is center)\n",
                "        center_threshold: How far from 0.5 is still considered center\n",
                "    \n",
                "    Returns:\n",
                "        str: 'center', 'left', or 'right'\n",
                "    \"\"\"\n",
                "    center = 0.5\n",
                "    \n",
                "    if abs(gaze_ratio - center) <= center_threshold:\n",
                "        return 'center'\n",
                "    elif gaze_ratio < center:\n",
                "        return 'left'\n",
                "    else:\n",
                "        return 'right'\n",
                "\n",
                "\n",
                "print(\"Gaze calculation functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Head Pose Estimation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def estimate_head_pose(landmarks, img_width, img_height):\n",
                "    \"\"\"\n",
                "    Estimate head pose (pitch, yaw, roll) using facial landmarks.\n",
                "    \n",
                "    Uses PnP (Perspective-n-Point) algorithm with 3D model points.\n",
                "    \n",
                "    Returns:\n",
                "        dict: pitch, yaw, roll in degrees\n",
                "    \"\"\"\n",
                "    # 3D model points (generic face model)\n",
                "    model_points = np.array([\n",
                "        [0.0, 0.0, 0.0],          # Nose tip\n",
                "        [0.0, -330.0, -65.0],     # Chin\n",
                "        [-225.0, 170.0, -135.0],  # Left eye corner\n",
                "        [225.0, 170.0, -135.0],   # Right eye corner\n",
                "        [-150.0, -150.0, -125.0], # Left mouth corner\n",
                "        [150.0, -150.0, -125.0]   # Right mouth corner\n",
                "    ], dtype=np.float64)\n",
                "    \n",
                "    # 2D image points\n",
                "    # Key landmark indices: nose tip (1), chin (152), left eye outer (33), \n",
                "    # right eye outer (263), left mouth corner (61), right mouth corner (291)\n",
                "    landmark_indices = [1, 152, 33, 263, 61, 291]\n",
                "    image_points = np.array([\n",
                "        get_single_landmark(landmarks, idx, img_width, img_height)\n",
                "        for idx in landmark_indices\n",
                "    ], dtype=np.float64)\n",
                "    \n",
                "    # Camera matrix (approximate)\n",
                "    focal_length = img_width\n",
                "    center = (img_width / 2, img_height / 2)\n",
                "    camera_matrix = np.array([\n",
                "        [focal_length, 0, center[0]],\n",
                "        [0, focal_length, center[1]],\n",
                "        [0, 0, 1]\n",
                "    ], dtype=np.float64)\n",
                "    \n",
                "    # Assume no lens distortion\n",
                "    dist_coeffs = np.zeros((4, 1))\n",
                "    \n",
                "    # Solve PnP\n",
                "    success, rotation_vector, translation_vector = cv2.solvePnP(\n",
                "        model_points, image_points, camera_matrix, dist_coeffs,\n",
                "        flags=cv2.SOLVEPNP_ITERATIVE\n",
                "    )\n",
                "    \n",
                "    if not success:\n",
                "        return {'pitch': 0, 'yaw': 0, 'roll': 0}\n",
                "    \n",
                "    # Convert rotation vector to Euler angles\n",
                "    rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
                "    \n",
                "    # Extract Euler angles\n",
                "    sy = np.sqrt(rotation_matrix[0, 0]**2 + rotation_matrix[1, 0]**2)\n",
                "    \n",
                "    if sy > 1e-6:\n",
                "        pitch = np.arctan2(rotation_matrix[2, 1], rotation_matrix[2, 2])\n",
                "        yaw = np.arctan2(-rotation_matrix[2, 0], sy)\n",
                "        roll = np.arctan2(rotation_matrix[1, 0], rotation_matrix[0, 0])\n",
                "    else:\n",
                "        pitch = np.arctan2(-rotation_matrix[1, 2], rotation_matrix[1, 1])\n",
                "        yaw = np.arctan2(-rotation_matrix[2, 0], sy)\n",
                "        roll = 0\n",
                "    \n",
                "    # Convert to degrees\n",
                "    return {\n",
                "        'pitch': np.degrees(pitch),\n",
                "        'yaw': np.degrees(yaw),\n",
                "        'roll': np.degrees(roll)\n",
                "    }\n",
                "\n",
                "print(\"Head pose estimation function defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Eye Contact Score Calculation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_eye_contact_score(\n",
                "    gaze_ratio: float,\n",
                "    head_yaw: float,\n",
                "    head_pitch: float,\n",
                "    gaze_center_threshold: float = 0.15,\n",
                "    head_yaw_threshold: float = 20.0,\n",
                "    head_pitch_threshold: float = 15.0\n",
                ") -> dict:\n",
                "    \"\"\"\n",
                "    Calculate eye contact score (0-100).\n",
                "    \n",
                "    Components:\n",
                "    - Gaze direction (60%): Is the iris centered?\n",
                "    - Head yaw (25%): Is the head facing forward?\n",
                "    - Head pitch (15%): Is the head level?\n",
                "    \n",
                "    Returns:\n",
                "        dict: Score breakdown\n",
                "    \"\"\"\n",
                "    # Gaze score (higher when closer to 0.5)\n",
                "    gaze_deviation = abs(gaze_ratio - 0.5)\n",
                "    gaze_score = max(0, 100 - (gaze_deviation / 0.5) * 100)\n",
                "    \n",
                "    # Head yaw score (higher when facing forward)\n",
                "    yaw_deviation = abs(head_yaw)\n",
                "    if yaw_deviation <= head_yaw_threshold:\n",
                "        yaw_score = 100 - (yaw_deviation / head_yaw_threshold) * 30\n",
                "    else:\n",
                "        yaw_score = max(0, 70 - (yaw_deviation - head_yaw_threshold) * 2)\n",
                "    \n",
                "    # Head pitch score (higher when level)\n",
                "    pitch_deviation = abs(head_pitch)\n",
                "    if pitch_deviation <= head_pitch_threshold:\n",
                "        pitch_score = 100 - (pitch_deviation / head_pitch_threshold) * 30\n",
                "    else:\n",
                "        pitch_score = max(0, 70 - (pitch_deviation - head_pitch_threshold) * 2)\n",
                "    \n",
                "    # Determine if looking at camera\n",
                "    is_looking_at_camera = (\n",
                "        gaze_deviation <= gaze_center_threshold and\n",
                "        yaw_deviation <= head_yaw_threshold and\n",
                "        pitch_deviation <= head_pitch_threshold\n",
                "    )\n",
                "    \n",
                "    # Weighted average\n",
                "    overall_score = gaze_score * 0.6 + yaw_score * 0.25 + pitch_score * 0.15\n",
                "    \n",
                "    return {\n",
                "        'overall_score': round(overall_score, 1),\n",
                "        'gaze_ratio': round(gaze_ratio, 3),\n",
                "        'gaze_score': round(gaze_score, 1),\n",
                "        'gaze_direction': get_gaze_direction(gaze_ratio, gaze_center_threshold),\n",
                "        'head_yaw': round(head_yaw, 1),\n",
                "        'yaw_score': round(yaw_score, 1),\n",
                "        'head_pitch': round(head_pitch, 1),\n",
                "        'pitch_score': round(pitch_score, 1),\n",
                "        'is_looking_at_camera': is_looking_at_camera\n",
                "    }\n",
                "\n",
                "# Test with example values\n",
                "test_cases = [\n",
                "    (0.5, 0, 0, \"Looking directly at camera\"),\n",
                "    (0.3, 15, 5, \"Slightly looking left\"),\n",
                "    (0.7, -25, 10, \"Looking right with head turned\"),\n",
                "    (0.5, 0, 30, \"Looking at camera but head tilted down\"),\n",
                "]\n",
                "\n",
                "print(\"Eye Contact Score Tests:\")\n",
                "print(\"-\" * 60)\n",
                "for gaze, yaw, pitch, desc in test_cases:\n",
                "    result = calculate_eye_contact_score(gaze, yaw, pitch)\n",
                "    print(f\"\\n{desc}\")\n",
                "    print(f\"  Overall: {result['overall_score']}/100\")\n",
                "    print(f\"  Direction: {result['gaze_direction']}\")\n",
                "    print(f\"  Looking at camera: {result['is_looking_at_camera']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Process Single Frame"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_frame_gaze(frame):\n",
                "    \"\"\"\n",
                "    Analyze a single frame for eye contact.\n",
                "    \n",
                "    Args:\n",
                "        frame: BGR image (numpy array)\n",
                "    \n",
                "    Returns:\n",
                "        dict: Gaze analysis results or None if no face detected\n",
                "    \"\"\"\n",
                "    # Convert BGR to RGB\n",
                "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
                "    h, w = frame.shape[:2]\n",
                "    \n",
                "    # Process with MediaPipe\n",
                "    results = face_mesh.process(rgb_frame)\n",
                "    \n",
                "    if not results.multi_face_landmarks:\n",
                "        return {\n",
                "            'face_detected': False,\n",
                "            'overall_score': 0,\n",
                "            'gaze_direction': 'unknown',\n",
                "            'is_looking_at_camera': False\n",
                "        }\n",
                "    \n",
                "    # Get first face landmarks\n",
                "    landmarks = results.multi_face_landmarks[0].landmark\n",
                "    \n",
                "    # Calculate gaze ratio\n",
                "    left_ratio, right_ratio, avg_ratio = calculate_gaze_ratio(landmarks, w, h)\n",
                "    \n",
                "    # Estimate head pose\n",
                "    head_pose = estimate_head_pose(landmarks, w, h)\n",
                "    \n",
                "    # Calculate score\n",
                "    score_result = calculate_eye_contact_score(\n",
                "        avg_ratio,\n",
                "        head_pose['yaw'],\n",
                "        head_pose['pitch']\n",
                "    )\n",
                "    \n",
                "    # Add face detected flag\n",
                "    score_result['face_detected'] = True\n",
                "    score_result['head_roll'] = round(head_pose['roll'], 1)\n",
                "    \n",
                "    return score_result\n",
                "\n",
                "\n",
                "def draw_gaze_overlay(frame, results):\n",
                "    \"\"\"\n",
                "    Draw gaze information overlay on frame.\n",
                "    \"\"\"\n",
                "    overlay = frame.copy()\n",
                "    \n",
                "    if not results.get('face_detected', False):\n",
                "        cv2.putText(overlay, \"No face detected\", (10, 30),\n",
                "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
                "        return overlay\n",
                "    \n",
                "    # Color based on score\n",
                "    score = results['overall_score']\n",
                "    if score >= 80:\n",
                "        color = (0, 255, 0)  # Green\n",
                "    elif score >= 60:\n",
                "        color = (0, 255, 255)  # Yellow\n",
                "    else:\n",
                "        color = (0, 0, 255)  # Red\n",
                "    \n",
                "    # Draw info\n",
                "    y = 30\n",
                "    texts = [\n",
                "        f\"Eye Contact: {score:.0f}%\",\n",
                "        f\"Direction: {results['gaze_direction']}\",\n",
                "        f\"Yaw: {results['head_yaw']:.1f} deg\",\n",
                "        f\"At Camera: {'Yes' if results['is_looking_at_camera'] else 'No'}\"\n",
                "    ]\n",
                "    \n",
                "    for text in texts:\n",
                "        cv2.putText(overlay, text, (10, y),\n",
                "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
                "        y += 25\n",
                "    \n",
                "    return overlay\n",
                "\n",
                "print(\"Frame analysis functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Test with Sample Image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a test frame (solid color with simulated face or use webcam)\n",
                "# For actual testing, you can load a real image or capture from webcam\n",
                "\n",
                "def test_with_webcam(duration_seconds=5):\n",
                "    \"\"\"\n",
                "    Test gaze detection with webcam for a few seconds.\n",
                "    Press 'q' to quit early.\n",
                "    \"\"\"\n",
                "    cap = cv2.VideoCapture(0)\n",
                "    \n",
                "    if not cap.isOpened():\n",
                "        print(\"Cannot open webcam\")\n",
                "        return\n",
                "    \n",
                "    print(f\"Testing for {duration_seconds} seconds... Press 'q' to quit.\")\n",
                "    \n",
                "    frame_count = 0\n",
                "    scores = []\n",
                "    \n",
                "    import time\n",
                "    start_time = time.time()\n",
                "    \n",
                "    while time.time() - start_time < duration_seconds:\n",
                "        ret, frame = cap.read()\n",
                "        if not ret:\n",
                "            break\n",
                "        \n",
                "        # Analyze frame\n",
                "        results = analyze_frame_gaze(frame)\n",
                "        \n",
                "        # Draw overlay\n",
                "        display_frame = draw_gaze_overlay(frame, results)\n",
                "        \n",
                "        # Show frame\n",
                "        cv2.imshow('Gaze Detection Test', display_frame)\n",
                "        \n",
                "        if results['face_detected']:\n",
                "            scores.append(results['overall_score'])\n",
                "        \n",
                "        frame_count += 1\n",
                "        \n",
                "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
                "            break\n",
                "    \n",
                "    cap.release()\n",
                "    cv2.destroyAllWindows()\n",
                "    \n",
                "    print(f\"\\nProcessed {frame_count} frames\")\n",
                "    if scores:\n",
                "        print(f\"Average eye contact score: {np.mean(scores):.1f}\")\n",
                "        print(f\"Min: {np.min(scores):.1f}, Max: {np.max(scores):.1f}\")\n",
                "\n",
                "# Uncomment to test with webcam:\n",
                "# test_with_webcam(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Export Utility Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create utils module for backend integration\n",
                "utils_code = '''\n",
                "\"\"\"\n",
                "Gaze Analysis Utilities\n",
                "\n",
                "Provides functions for analyzing eye contact including:\n",
                "- Gaze direction detection\n",
                "- Head pose estimation\n",
                "- Eye contact scoring\n",
                "\n",
                "Generated from notebook: 02_gaze_detection.ipynb\n",
                "\"\"\"\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "from typing import Dict, Tuple, Optional\n",
                "\n",
                "# MediaPipe landmark indices\n",
                "LEFT_IRIS_CENTER = 473\n",
                "RIGHT_IRIS_CENTER = 468\n",
                "LEFT_EYE_INNER_CORNER = 362\n",
                "LEFT_EYE_OUTER_CORNER = 263\n",
                "RIGHT_EYE_INNER_CORNER = 133\n",
                "RIGHT_EYE_OUTER_CORNER = 33\n",
                "\n",
                "\n",
                "def get_single_landmark(landmarks, idx: int, img_width: int, img_height: int) -> Tuple[int, int]:\n",
                "    \"\"\"Get single landmark as (x, y) tuple.\"\"\"\n",
                "    lm = landmarks[idx]\n",
                "    return (int(lm.x * img_width), int(lm.y * img_height))\n",
                "\n",
                "\n",
                "def calculate_gaze_ratio(landmarks, img_width: int, img_height: int) -> Tuple[float, float, float]:\n",
                "    \"\"\"Calculate gaze ratio for each eye.\"\"\"\n",
                "    left_iris = get_single_landmark(landmarks, LEFT_IRIS_CENTER, img_width, img_height)\n",
                "    right_iris = get_single_landmark(landmarks, RIGHT_IRIS_CENTER, img_width, img_height)\n",
                "    \n",
                "    left_inner = get_single_landmark(landmarks, LEFT_EYE_INNER_CORNER, img_width, img_height)\n",
                "    left_outer = get_single_landmark(landmarks, LEFT_EYE_OUTER_CORNER, img_width, img_height)\n",
                "    right_inner = get_single_landmark(landmarks, RIGHT_EYE_INNER_CORNER, img_width, img_height)\n",
                "    right_outer = get_single_landmark(landmarks, RIGHT_EYE_OUTER_CORNER, img_width, img_height)\n",
                "    \n",
                "    left_eye_width = abs(left_outer[0] - left_inner[0])\n",
                "    left_ratio = (left_iris[0] - min(left_inner[0], left_outer[0])) / left_eye_width if left_eye_width > 0 else 0.5\n",
                "    \n",
                "    right_eye_width = abs(right_outer[0] - right_inner[0])\n",
                "    right_ratio = (right_iris[0] - min(right_inner[0], right_outer[0])) / right_eye_width if right_eye_width > 0 else 0.5\n",
                "    \n",
                "    return left_ratio, right_ratio, (left_ratio + right_ratio) / 2\n",
                "\n",
                "\n",
                "def get_gaze_direction(gaze_ratio: float, center_threshold: float = 0.15) -> str:\n",
                "    \"\"\"Determine gaze direction from ratio.\"\"\"\n",
                "    if abs(gaze_ratio - 0.5) <= center_threshold:\n",
                "        return \"center\"\n",
                "    return \"left\" if gaze_ratio < 0.5 else \"right\"\n",
                "\n",
                "\n",
                "def calculate_eye_contact_score(\n",
                "    gaze_ratio: float,\n",
                "    head_yaw: float,\n",
                "    head_pitch: float,\n",
                "    gaze_center_threshold: float = 0.15,\n",
                "    head_yaw_threshold: float = 20.0,\n",
                "    head_pitch_threshold: float = 15.0\n",
                ") -> Dict:\n",
                "    \"\"\"Calculate eye contact score (0-100).\"\"\"\n",
                "    gaze_deviation = abs(gaze_ratio - 0.5)\n",
                "    gaze_score = max(0, 100 - (gaze_deviation / 0.5) * 100)\n",
                "    \n",
                "    yaw_deviation = abs(head_yaw)\n",
                "    yaw_score = 100 - (yaw_deviation / head_yaw_threshold) * 30 if yaw_deviation <= head_yaw_threshold else max(0, 70 - (yaw_deviation - head_yaw_threshold) * 2)\n",
                "    \n",
                "    pitch_deviation = abs(head_pitch)\n",
                "    pitch_score = 100 - (pitch_deviation / head_pitch_threshold) * 30 if pitch_deviation <= head_pitch_threshold else max(0, 70 - (pitch_deviation - head_pitch_threshold) * 2)\n",
                "    \n",
                "    is_looking_at_camera = (\n",
                "        gaze_deviation <= gaze_center_threshold and\n",
                "        yaw_deviation <= head_yaw_threshold and\n",
                "        pitch_deviation <= head_pitch_threshold\n",
                "    )\n",
                "    \n",
                "    overall_score = gaze_score * 0.6 + yaw_score * 0.25 + pitch_score * 0.15\n",
                "    \n",
                "    return {\n",
                "        \"overall_score\": round(overall_score, 1),\n",
                "        \"gaze_ratio\": round(gaze_ratio, 3),\n",
                "        \"gaze_score\": round(gaze_score, 1),\n",
                "        \"gaze_direction\": get_gaze_direction(gaze_ratio, gaze_center_threshold),\n",
                "        \"head_yaw\": round(head_yaw, 1),\n",
                "        \"head_pitch\": round(head_pitch, 1),\n",
                "        \"is_looking_at_camera\": is_looking_at_camera\n",
                "    }\n",
                "'''\n",
                "\n",
                "# Save to training directory\n",
                "utils_path = PROJECT_ROOT / 'ml' / 'softskills' / 'training' / 'gaze_utils.py'\n",
                "utils_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "with open(utils_path, 'w') as f:\n",
                "    f.write(utils_code)\n",
                "\n",
                "print(f\"Exported utilities to: {utils_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Summary\n",
                "\n",
                "### Key Findings\n",
                "1. MediaPipe Face Mesh provides 468 landmarks including iris tracking\n",
                "2. Gaze ratio of 0.5 indicates looking at center/camera\n",
                "3. Head pose (yaw, pitch) affects perceived eye contact\n",
                "4. Combined score weights: Gaze 60%, Head Yaw 25%, Pitch 15%\n",
                "\n",
                "### Thresholds for \"Looking at Camera\"\n",
                "- Gaze deviation: ≤ 0.15 from center (0.5)\n",
                "- Head yaw: ≤ 20 degrees\n",
                "- Head pitch: ≤ 15 degrees\n",
                "\n",
                "### Next Steps\n",
                "1. Add blink detection for engagement analysis\n",
                "2. Implement temporal smoothing for stability\n",
                "3. Integrate with backend `gaze_analyzer.py` service"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}