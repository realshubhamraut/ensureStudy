{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Integration Pipeline - Soft Skills Analysis\n",
                "\n",
                "This notebook combines all analyzers into a unified pipeline for real-time soft skills evaluation.\n",
                "\n",
                "## Goals\n",
                "1. Combine fluency, gaze, gesture, and posture analyzers\n",
                "2. Create unified scoring system\n",
                "3. Benchmark real-time performance\n",
                "4. Export complete pipeline for backend integration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# Add project root to path\n",
                "PROJECT_ROOT = Path(os.getcwd()).parent.parent.parent\n",
                "sys.path.insert(0, str(PROJECT_ROOT))\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "import time\n",
                "import mediapipe as mp\n",
                "from collections import deque\n",
                "from dataclasses import dataclass, field\n",
                "from typing import Dict, List, Optional, Tuple\n",
                "\n",
                "print(f\"Project root: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Utility Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import from training utils (generated by previous notebooks)\n",
                "from ml.softskills.training import fluency_utils\n",
                "from ml.softskills.training import gaze_utils\n",
                "from ml.softskills.training import gesture_utils\n",
                "from ml.softskills.training import posture_utils\n",
                "\n",
                "print(\"Imported utility modules from training/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Unified Data Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class VisualMetrics:\n",
                "    \"\"\"Visual analysis results from a single frame.\"\"\"\n",
                "    # Eye contact\n",
                "    eye_contact_score: float = 0.0\n",
                "    gaze_direction: str = 'unknown'\n",
                "    is_looking_at_camera: bool = False\n",
                "    \n",
                "    # Hand gestures\n",
                "    gesture_score: float = 0.0\n",
                "    hands_visible: bool = False\n",
                "    num_hands: int = 0\n",
                "    \n",
                "    # Posture\n",
                "    posture_score: float = 0.0\n",
                "    is_upright: bool = True\n",
                "    shoulders_level: bool = True\n",
                "    \n",
                "    # Face detection\n",
                "    face_detected: bool = False\n",
                "    body_detected: bool = False\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class AudioMetrics:\n",
                "    \"\"\"Audio/speech analysis results.\"\"\"\n",
                "    # Fluency\n",
                "    fluency_score: float = 0.0\n",
                "    wpm: float = 0.0\n",
                "    filler_count: int = 0\n",
                "    fillers_detected: List[str] = field(default_factory=list)\n",
                "    \n",
                "    # Grammar (future)\n",
                "    grammar_score: float = 0.0\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class SoftSkillsScore:\n",
                "    \"\"\"Final combined soft skills score.\"\"\"\n",
                "    overall_score: float = 0.0\n",
                "    \n",
                "    # Individual scores\n",
                "    fluency_score: float = 0.0\n",
                "    grammar_score: float = 0.0\n",
                "    eye_contact_score: float = 0.0\n",
                "    gesture_score: float = 0.0\n",
                "    posture_score: float = 0.0\n",
                "    expression_score: float = 0.0\n",
                "    \n",
                "    # Weights used\n",
                "    weights: Dict[str, float] = field(default_factory=lambda: {\n",
                "        'fluency': 0.30,\n",
                "        'grammar': 0.20,\n",
                "        'eye_contact': 0.15,\n",
                "        'gesture': 0.10,\n",
                "        'posture': 0.10,\n",
                "        'expression': 0.10\n",
                "    })\n",
                "    \n",
                "    def calculate_overall(self):\n",
                "        \"\"\"Calculate weighted overall score.\"\"\"\n",
                "        self.overall_score = (\n",
                "            self.fluency_score * self.weights['fluency'] +\n",
                "            self.grammar_score * self.weights['grammar'] +\n",
                "            self.eye_contact_score * self.weights['eye_contact'] +\n",
                "            self.gesture_score * self.weights['gesture'] +\n",
                "            self.posture_score * self.weights['posture'] +\n",
                "            self.expression_score * self.weights['expression']\n",
                "        )\n",
                "        return self.overall_score\n",
                "\n",
                "print(\"Data classes defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Unified Pipeline Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SoftSkillsPipeline:\n",
                "    \"\"\"\n",
                "    Unified pipeline for real-time soft skills analysis.\n",
                "    \n",
                "    Combines:\n",
                "    - Eye contact detection (MediaPipe Face Mesh)\n",
                "    - Hand gesture analysis (MediaPipe Hands)\n",
                "    - Posture estimation (MediaPipe Pose)\n",
                "    - Speech fluency analysis (text-based)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, fps: int = 30, enable_tracking: bool = True):\n",
                "        self.fps = fps\n",
                "        self.enable_tracking = enable_tracking\n",
                "        \n",
                "        # Initialize MediaPipe components\n",
                "        self.mp_face_mesh = mp.solutions.face_mesh\n",
                "        self.mp_hands = mp.solutions.hands\n",
                "        self.mp_pose = mp.solutions.pose\n",
                "        \n",
                "        # Create detectors\n",
                "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
                "            max_num_faces=1,\n",
                "            refine_landmarks=True,\n",
                "            min_detection_confidence=0.5,\n",
                "            min_tracking_confidence=0.5\n",
                "        )\n",
                "        \n",
                "        self.hands = self.mp_hands.Hands(\n",
                "            static_image_mode=False,\n",
                "            max_num_hands=2,\n",
                "            min_detection_confidence=0.5,\n",
                "            min_tracking_confidence=0.5\n",
                "        )\n",
                "        \n",
                "        self.pose = self.mp_pose.Pose(\n",
                "            static_image_mode=False,\n",
                "            model_complexity=1,\n",
                "            smooth_landmarks=True,\n",
                "            min_detection_confidence=0.5,\n",
                "            min_tracking_confidence=0.5\n",
                "        )\n",
                "        \n",
                "        # Trackers for temporal analysis\n",
                "        if enable_tracking:\n",
                "            self.hand_tracker = gesture_utils.HandMovementTracker(fps=fps)\n",
                "            self.posture_tracker = posture_utils.PostureTracker()\n",
                "        else:\n",
                "            self.hand_tracker = None\n",
                "            self.posture_tracker = None\n",
                "        \n",
                "        # Aggregated scores\n",
                "        self.score_history = deque(maxlen=fps * 60)  # 1 minute of scores\n",
                "        \n",
                "        print(\"SoftSkillsPipeline initialized\")\n",
                "    \n",
                "    def process_frame(self, frame: np.ndarray) -> VisualMetrics:\n",
                "        \"\"\"\n",
                "        Process a single video frame for visual metrics.\n",
                "        \n",
                "        Args:\n",
                "            frame: BGR image (numpy array)\n",
                "        \n",
                "        Returns:\n",
                "            VisualMetrics with all visual analysis results\n",
                "        \"\"\"\n",
                "        # Convert to RGB once\n",
                "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
                "        h, w = frame.shape[:2]\n",
                "        \n",
                "        metrics = VisualMetrics()\n",
                "        \n",
                "        # 1. Gaze/Eye Contact Analysis\n",
                "        face_results = self.face_mesh.process(rgb_frame)\n",
                "        if face_results.multi_face_landmarks:\n",
                "            metrics.face_detected = True\n",
                "            landmarks = face_results.multi_face_landmarks[0].landmark\n",
                "            \n",
                "            # Calculate gaze\n",
                "            _, _, avg_ratio = gaze_utils.calculate_gaze_ratio(landmarks, w, h)\n",
                "            metrics.gaze_direction = gaze_utils.get_gaze_direction(avg_ratio)\n",
                "            \n",
                "            # Simple head pose estimation (using nose and eye positions)\n",
                "            # For full implementation, use the estimate_head_pose from notebook\n",
                "            head_yaw = 0  # Simplified\n",
                "            head_pitch = 0\n",
                "            \n",
                "            score_result = gaze_utils.calculate_eye_contact_score(avg_ratio, head_yaw, head_pitch)\n",
                "            metrics.eye_contact_score = score_result['overall_score']\n",
                "            metrics.is_looking_at_camera = score_result['is_looking_at_camera']\n",
                "        \n",
                "        # 2. Hand Gesture Analysis\n",
                "        hand_results = self.hands.process(rgb_frame)\n",
                "        left_center = None\n",
                "        right_center = None\n",
                "        \n",
                "        if hand_results.multi_hand_landmarks:\n",
                "            for hand_landmarks, handedness in zip(\n",
                "                hand_results.multi_hand_landmarks,\n",
                "                hand_results.multi_handedness\n",
                "            ):\n",
                "                label = handedness.classification[0].label\n",
                "                center = gesture_utils.get_hand_center(hand_landmarks, w, h)\n",
                "                \n",
                "                if label == 'Left':\n",
                "                    left_center = center\n",
                "                else:\n",
                "                    right_center = center\n",
                "            \n",
                "            metrics.hands_visible = True\n",
                "            metrics.num_hands = len(hand_results.multi_hand_landmarks)\n",
                "        \n",
                "        # Update hand tracker\n",
                "        if self.hand_tracker:\n",
                "            self.hand_tracker.update(left_center, right_center)\n",
                "            tracker_metrics = self.hand_tracker.get_metrics()\n",
                "            velocity = max(\n",
                "                tracker_metrics['left_hand'].get('velocity', 0),\n",
                "                tracker_metrics['right_hand'].get('velocity', 0)\n",
                "            )\n",
                "            stability = tracker_metrics['avg_stability']\n",
                "        else:\n",
                "            velocity = 50  # Default\n",
                "            stability = 80\n",
                "        \n",
                "        gesture_result = gesture_utils.calculate_gesture_score(\n",
                "            metrics.hands_visible,\n",
                "            metrics.num_hands,\n",
                "            stability,\n",
                "            velocity\n",
                "        )\n",
                "        metrics.gesture_score = gesture_result['overall_score']\n",
                "        \n",
                "        # 3. Posture Analysis\n",
                "        pose_results = self.pose.process(rgb_frame)\n",
                "        if pose_results.pose_landmarks:\n",
                "            metrics.body_detected = True\n",
                "            \n",
                "            shoulder_metrics = posture_utils.calculate_shoulder_metrics(\n",
                "                pose_results.pose_landmarks, w, h\n",
                "            )\n",
                "            lean_metrics = posture_utils.calculate_body_lean(\n",
                "                pose_results.pose_landmarks, w, h\n",
                "            )\n",
                "            \n",
                "            # Update posture tracker\n",
                "            if self.posture_tracker:\n",
                "                self.posture_tracker.update(\n",
                "                    shoulder_metrics['center'],\n",
                "                    shoulder_metrics['tilt_angle'],\n",
                "                    lean_metrics['lean_angle']\n",
                "                )\n",
                "                posture_stability = self.posture_tracker.calculate_stability()['overall_stability']\n",
                "            else:\n",
                "                posture_stability = 80\n",
                "            \n",
                "            posture_result = posture_utils.calculate_posture_score(\n",
                "                shoulder_metrics['tilt_angle'],\n",
                "                lean_metrics['lean_angle'],\n",
                "                posture_stability\n",
                "            )\n",
                "            metrics.posture_score = posture_result['overall_score']\n",
                "            metrics.is_upright = lean_metrics['is_upright']\n",
                "            metrics.shoulders_level = shoulder_metrics['is_level']\n",
                "        \n",
                "        return metrics\n",
                "    \n",
                "    def process_audio(self, transcript: str, duration_seconds: float) -> AudioMetrics:\n",
                "        \"\"\"\n",
                "        Process speech transcript for audio metrics.\n",
                "        \n",
                "        Args:\n",
                "            transcript: Speech transcript text\n",
                "            duration_seconds: Audio duration\n",
                "        \n",
                "        Returns:\n",
                "            AudioMetrics with fluency analysis\n",
                "        \"\"\"\n",
                "        metrics = AudioMetrics()\n",
                "        \n",
                "        if not transcript or duration_seconds <= 0:\n",
                "            return metrics\n",
                "        \n",
                "        fluency_result = fluency_utils.calculate_fluency_score(\n",
                "            transcript, duration_seconds\n",
                "        )\n",
                "        \n",
                "        metrics.fluency_score = fluency_result['overall_score']\n",
                "        metrics.wpm = fluency_result['wpm']\n",
                "        metrics.filler_count = fluency_result['filler_count']\n",
                "        metrics.fillers_detected = fluency_result['fillers_detected']\n",
                "        \n",
                "        # Grammar score (placeholder - requires NLP integration)\n",
                "        metrics.grammar_score = 75.0  # Default\n",
                "        \n",
                "        return metrics\n",
                "    \n",
                "    def calculate_final_score(\n",
                "        self,\n",
                "        visual_metrics: VisualMetrics,\n",
                "        audio_metrics: AudioMetrics\n",
                "    ) -> SoftSkillsScore:\n",
                "        \"\"\"\n",
                "        Calculate final weighted soft skills score.\n",
                "        \"\"\"\n",
                "        score = SoftSkillsScore(\n",
                "            fluency_score=audio_metrics.fluency_score,\n",
                "            grammar_score=audio_metrics.grammar_score,\n",
                "            eye_contact_score=visual_metrics.eye_contact_score,\n",
                "            gesture_score=visual_metrics.gesture_score,\n",
                "            posture_score=visual_metrics.posture_score,\n",
                "            expression_score=75.0  # Placeholder\n",
                "        )\n",
                "        score.calculate_overall()\n",
                "        return score\n",
                "    \n",
                "    def close(self):\n",
                "        \"\"\"Release resources.\"\"\"\n",
                "        self.face_mesh.close()\n",
                "        self.hands.close()\n",
                "        self.pose.close()\n",
                "\n",
                "print(\"SoftSkillsPipeline class defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Performance Benchmark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def benchmark_pipeline(num_frames: int = 100, frame_size: Tuple[int, int] = (640, 480)):\n",
                "    \"\"\"\n",
                "    Benchmark pipeline performance.\n",
                "    \n",
                "    Target: 5 FPS (200ms per frame) for real-time analysis.\n",
                "    \"\"\"\n",
                "    pipeline = SoftSkillsPipeline(enable_tracking=True)\n",
                "    \n",
                "    # Create synthetic test frames\n",
                "    test_frame = np.random.randint(0, 255, (frame_size[1], frame_size[0], 3), dtype=np.uint8)\n",
                "    \n",
                "    times = []\n",
                "    \n",
                "    print(f\"Benchmarking with {num_frames} frames at {frame_size}...\")\n",
                "    \n",
                "    for i in range(num_frames):\n",
                "        start = time.time()\n",
                "        metrics = pipeline.process_frame(test_frame)\n",
                "        elapsed = time.time() - start\n",
                "        times.append(elapsed)\n",
                "    \n",
                "    pipeline.close()\n",
                "    \n",
                "    avg_time = np.mean(times) * 1000  # ms\n",
                "    max_time = np.max(times) * 1000\n",
                "    min_time = np.min(times) * 1000\n",
                "    fps = 1000 / avg_time\n",
                "    \n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"Performance Results:\")\n",
                "    print(f\"{'='*50}\")\n",
                "    print(f\"Average time per frame: {avg_time:.1f} ms\")\n",
                "    print(f\"Min: {min_time:.1f} ms, Max: {max_time:.1f} ms\")\n",
                "    print(f\"Estimated FPS: {fps:.1f}\")\n",
                "    print(f\"\")\n",
                "    print(f\"Target: 5 FPS (200ms/frame)\")\n",
                "    if avg_time <= 200:\n",
                "        print(f\"âœ… PASS - Real-time capable\")\n",
                "    else:\n",
                "        print(f\"âš ï¸ NEEDS OPTIMIZATION - Consider reducing model complexity\")\n",
                "    \n",
                "    return {\n",
                "        'avg_ms': avg_time,\n",
                "        'fps': fps,\n",
                "        'pass': avg_time <= 200\n",
                "    }\n",
                "\n",
                "# Run benchmark (note: will be slow without GPU on synthetic data)\n",
                "# benchmark_pipeline(50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Complete Usage Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def demo_soft_skills_analysis():\n",
                "    \"\"\"\n",
                "    Demo the complete soft skills analysis pipeline.\n",
                "    \"\"\"\n",
                "    print(\"Soft Skills Analysis Demo\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    # Initialize pipeline\n",
                "    pipeline = SoftSkillsPipeline(fps=30, enable_tracking=True)\n",
                "    \n",
                "    # Simulate video frames (in real use, from webcam)\n",
                "    # For demo, create a blank frame\n",
                "    dummy_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
                "    \n",
                "    # Process some frames\n",
                "    print(\"\\nProcessing 10 frames...\")\n",
                "    for i in range(10):\n",
                "        visual_metrics = pipeline.process_frame(dummy_frame)\n",
                "    \n",
                "    # Simulate speech transcript\n",
                "    transcript = \"\"\"\n",
                "    Well, um, I think I would be a good fit for this position because, \n",
                "    you know, I have experience in software development and, like, \n",
                "    I really enjoy solving complex problems.\n",
                "    \"\"\"\n",
                "    duration = 15.0  # seconds\n",
                "    \n",
                "    # Process audio\n",
                "    audio_metrics = pipeline.process_audio(transcript, duration)\n",
                "    \n",
                "    # Calculate final score\n",
                "    final_score = pipeline.calculate_final_score(visual_metrics, audio_metrics)\n",
                "    \n",
                "    print(\"\\n\" + \"=\" * 50)\n",
                "    print(\"FINAL SOFT SKILLS SCORE\")\n",
                "    print(\"=\" * 50)\n",
                "    print(f\"\\nðŸŽ¯ Overall Score: {final_score.overall_score:.1f}/100\")\n",
                "    print(\"\\nBreakdown:\")\n",
                "    print(f\"  ðŸ“£ Fluency:     {final_score.fluency_score:.1f} (weight: {final_score.weights['fluency']*100:.0f}%)\")\n",
                "    print(f\"  ðŸ“ Grammar:     {final_score.grammar_score:.1f} (weight: {final_score.weights['grammar']*100:.0f}%)\")\n",
                "    print(f\"  ðŸ‘ï¸  Eye Contact: {final_score.eye_contact_score:.1f} (weight: {final_score.weights['eye_contact']*100:.0f}%)\")\n",
                "    print(f\"  ðŸ¤š Gestures:    {final_score.gesture_score:.1f} (weight: {final_score.weights['gesture']*100:.0f}%)\")\n",
                "    print(f\"  ðŸ§ Posture:     {final_score.posture_score:.1f} (weight: {final_score.weights['posture']*100:.0f}%)\")\n",
                "    print(f\"  ðŸ˜Š Expression:  {final_score.expression_score:.1f} (weight: {final_score.weights['expression']*100:.0f}%)\")\n",
                "    \n",
                "    print(\"\\nFluency Details:\")\n",
                "    print(f\"  WPM: {audio_metrics.wpm:.0f}\")\n",
                "    print(f\"  Fillers detected: {audio_metrics.filler_count}\")\n",
                "    print(f\"  Filler words: {audio_metrics.fillers_detected}\")\n",
                "    \n",
                "    pipeline.close()\n",
                "    \n",
                "    return final_score\n",
                "\n",
                "# Run demo\n",
                "# demo_soft_skills_analysis()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Export Pipeline for Backend"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The pipeline class will be exported to backend in Phase 4\n",
                "# See: backend/ai-service/app/services/softskills_pipeline.py\n",
                "\n",
                "print(\"\"\"\n",
                "Pipeline Export Notes:\n",
                "======================\n",
                "\n",
                "The SoftSkillsPipeline class should be adapted for the backend:\n",
                "\n",
                "1. File: backend/ai-service/app/services/softskills_pipeline.py\n",
                "2. Use FastAPI WebSocket for real-time frame streaming\n",
                "3. Add async processing support\n",
                "4. Integrate with PostgreSQL for storing results\n",
                "5. Add Redis caching for session state\n",
                "\n",
                "See implementation_plan.md for full details.\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary\n",
                "\n",
                "### Pipeline Components\n",
                "\n",
                "| Component | Input | Output | Weight |\n",
                "|-----------|-------|--------|--------|\n",
                "| Fluency Analyzer | Transcript + Duration | WPM, Fillers, Score | 30% |\n",
                "| Grammar Analyzer | Transcript | Error count, Score | 20% |\n",
                "| Eye Contact | Video Frame | Gaze direction, Score | 15% |\n",
                "| Gesture Analyzer | Video Frame | Hand visibility, Score | 10% |\n",
                "| Posture Analyzer | Video Frame | Lean, Stability, Score | 10% |\n",
                "| Expression Analyzer | Video Frame | (Future) | 10% |\n",
                "\n",
                "### Performance Targets\n",
                "- **Frame processing**: < 200ms (5 FPS)\n",
                "- **Latency**: < 500ms end-to-end\n",
                "- **Memory**: < 500MB GPU memory\n",
                "\n",
                "### Next Steps\n",
                "1. Create database migration (Phase 3)\n",
                "2. Implement backend services (Phase 4)\n",
                "3. Integrate with frontend WebSocket (Phase 5)\n",
                "4. Add grammar analysis using LanguageTool\n",
                "5. Add expression/emotion detection"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}