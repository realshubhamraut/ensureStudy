{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fluency Analysis - Filler Word Detection\n",
                "\n",
                "This notebook explores the PodcastFillers dataset and builds a filler word detection model using PyTorch.\n",
                "\n",
                "## Goals\n",
                "1. Load and explore the PodcastFillers dataset\n",
                "2. Analyze filler word patterns (um, uh, like, you know)\n",
                "3. Calculate baseline WPM distributions\n",
                "4. Train a simple filler detector with PyTorch\n",
                "5. Export utility functions for backend integration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies if needed\n",
                "# !pip install torch torchaudio librosa pandas matplotlib datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# Add project root to path\n",
                "PROJECT_ROOT = Path(os.getcwd()).parent.parent.parent\n",
                "sys.path.insert(0, str(PROJECT_ROOT))\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "print(f\"Project root: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load PodcastFillers Dataset\n",
                "\n",
                "The dataset contains podcast audio clips labeled with filler words (um, uh) and other events."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset, Audio\n",
                "\n",
                "# Load from local cache or HuggingFace\n",
                "DATASET_PATH = PROJECT_ROOT / \"ml\" / \"softskills\" / \"datasets\" / \"fluency\" / \"podcast-fillers-hf\"\n",
                "\n",
                "if DATASET_PATH.exists():\n",
                "    print(f\"Loading from local: {DATASET_PATH}\")\n",
                "    ds = load_dataset(str(DATASET_PATH), split=\"train\")\n",
                "else:\n",
                "    print(\"Loading from HuggingFace...\")\n",
                "    ds = load_dataset(\"ylacombe/podcast_fillers_by_license\", split=\"train\")\n",
                "\n",
                "print(f\"Dataset size: {len(ds)} samples\")\n",
                "print(f\"Features: {ds.features}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore a sample\n",
                "sample = ds[0]\n",
                "print(\"Sample keys:\", sample.keys())\n",
                "\n",
                "for key, value in sample.items():\n",
                "    if isinstance(value, dict):\n",
                "        print(f\"  {key}: {type(value)} - keys: {value.keys()}\")\n",
                "    elif isinstance(value, np.ndarray):\n",
                "        print(f\"  {key}: array shape {value.shape}\")\n",
                "    else:\n",
                "        print(f\"  {key}: {type(value).__name__} = {str(value)[:100]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Filler Word Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count filler types\n",
                "filler_types = {}\n",
                "for i in range(min(1000, len(ds))):\n",
                "    sample = ds[i]\n",
                "    if 'label' in sample:\n",
                "        label = sample['label']\n",
                "        filler_types[label] = filler_types.get(label, 0) + 1\n",
                "    elif 'text' in sample:\n",
                "        text = sample['text'].lower()\n",
                "        for filler in ['um', 'uh', 'like', 'you know', 'basically', 'actually']:\n",
                "            if filler in text:\n",
                "                filler_types[filler] = filler_types.get(filler, 0) + 1\n",
                "\n",
                "print(\"Filler word distribution:\")\n",
                "for filler, count in sorted(filler_types.items(), key=lambda x: -x[1]):\n",
                "    print(f\"  {filler}: {count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize distribution\n",
                "if filler_types:\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    fillers = list(filler_types.keys())\n",
                "    counts = list(filler_types.values())\n",
                "    \n",
                "    plt.bar(fillers, counts, color='steelblue', edgecolor='navy')\n",
                "    plt.xlabel('Filler Word')\n",
                "    plt.ylabel('Count')\n",
                "    plt.title('Filler Word Distribution in PodcastFillers Dataset')\n",
                "    plt.xticks(rotation=45, ha='right')\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(PROJECT_ROOT / 'ml' / 'softskills' / 'notebooks' / 'filler_distribution.png', dpi=150)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Audio Feature Extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torchaudio\n",
                "from torchaudio.transforms import MelSpectrogram, MFCC\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"TorchAudio version: {torchaudio.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "\n",
                "# Use MPS on Mac if available\n",
                "if torch.backends.mps.is_available():\n",
                "    device = torch.device('mps')\n",
                "    print(\"Using MPS (Metal Performance Shaders)\")\n",
                "elif torch.cuda.is_available():\n",
                "    device = torch.device('cuda')\n",
                "    print(\"Using CUDA\")\n",
                "else:\n",
                "    device = torch.device('cpu')\n",
                "    print(\"Using CPU\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_audio_features(audio_array, sample_rate=16000):\n",
                "    \"\"\"\n",
                "    Extract MFCC features from audio.\n",
                "    \n",
                "    Args:\n",
                "        audio_array: numpy array of audio samples\n",
                "        sample_rate: audio sample rate\n",
                "    \n",
                "    Returns:\n",
                "        MFCC features as torch tensor\n",
                "    \"\"\"\n",
                "    if isinstance(audio_array, np.ndarray):\n",
                "        waveform = torch.from_numpy(audio_array).float()\n",
                "    else:\n",
                "        waveform = audio_array\n",
                "    \n",
                "    if waveform.dim() == 1:\n",
                "        waveform = waveform.unsqueeze(0)  # Add channel dimension\n",
                "    \n",
                "    # MFCC transform\n",
                "    mfcc_transform = MFCC(\n",
                "        sample_rate=sample_rate,\n",
                "        n_mfcc=13,\n",
                "        melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 23}\n",
                "    )\n",
                "    \n",
                "    mfcc = mfcc_transform(waveform)\n",
                "    return mfcc\n",
                "\n",
                "# Test on first sample\n",
                "if 'audio' in ds[0]:\n",
                "    audio_data = ds[0]['audio']\n",
                "    if isinstance(audio_data, dict):\n",
                "        waveform = audio_data.get('array', np.zeros(16000))\n",
                "        sr = audio_data.get('sampling_rate', 16000)\n",
                "    else:\n",
                "        waveform = audio_data\n",
                "        sr = 16000\n",
                "    \n",
                "    features = extract_audio_features(waveform, sr)\n",
                "    print(f\"MFCC shape: {features.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Words Per Minute (WPM) Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_wpm(text: str, duration_seconds: float) -> float:\n",
                "    \"\"\"\n",
                "    Calculate words per minute from text and duration.\n",
                "    \n",
                "    Args:\n",
                "        text: Transcript text\n",
                "        duration_seconds: Audio duration in seconds\n",
                "    \n",
                "    Returns:\n",
                "        Words per minute\n",
                "    \"\"\"\n",
                "    if duration_seconds <= 0:\n",
                "        return 0.0\n",
                "    \n",
                "    # Count words (simple split)\n",
                "    words = [w for w in text.split() if w.strip()]\n",
                "    word_count = len(words)\n",
                "    \n",
                "    # Calculate WPM\n",
                "    minutes = duration_seconds / 60.0\n",
                "    wpm = word_count / minutes if minutes > 0 else 0\n",
                "    \n",
                "    return wpm\n",
                "\n",
                "# Test\n",
                "test_text = \"Hello this is a test sentence with about ten words in it.\"\n",
                "test_duration = 5.0  # seconds\n",
                "wpm = calculate_wpm(test_text, test_duration)\n",
                "print(f\"Test WPM: {wpm:.1f} (expected ~132 for 11 words in 5 seconds)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optimal WPM ranges for communication\n",
                "print(\"\"\"\n",
                "=== Optimal Speech Rates ===\n",
                "\n",
                "Speaking Context         | WPM Range  | Notes\n",
                "-------------------------|------------|---------------------------\n",
                "Conversational           | 120-150    | Natural, comfortable pace\n",
                "Presentation/Interview   | 130-160    | Slightly faster, engaging\n",
                "Audiobook/Narration      | 150-180    | Clear articulation\n",
                "Speed reading            | 200+       | Not for interviews!\n",
                "\n",
                "For soft skills evaluation:\n",
                "- Below 100 WPM: Too slow, may indicate nervousness\n",
                "- 120-160 WPM: Optimal range\n",
                "- Above 180 WPM: Too fast, may lose clarity\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Filler Word Detection Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "# Common filler words and phrases\n",
                "FILLER_PATTERNS = [\n",
                "    r'\\bum\\b',\n",
                "    r'\\buh\\b',\n",
                "    r'\\blike\\b',\n",
                "    r'\\byou know\\b',\n",
                "    r'\\bbasically\\b',\n",
                "    r'\\bactually\\b',\n",
                "    r'\\bliterally\\b',\n",
                "    r'\\bso\\b',  # When used as filler at sentence start\n",
                "    r'\\bwell\\b',\n",
                "    r'\\bi mean\\b',\n",
                "    r'\\bkind of\\b',\n",
                "    r'\\bsort of\\b',\n",
                "    r'\\bright\\b',  # When used as filler for confirmation\n",
                "]\n",
                "\n",
                "def detect_fillers(text: str) -> dict:\n",
                "    \"\"\"\n",
                "    Detect filler words in text.\n",
                "    \n",
                "    Args:\n",
                "        text: Input transcript\n",
                "    \n",
                "    Returns:\n",
                "        Dictionary with filler counts and list of detected fillers\n",
                "    \"\"\"\n",
                "    text_lower = text.lower()\n",
                "    \n",
                "    detected = []\n",
                "    total_count = 0\n",
                "    \n",
                "    for pattern in FILLER_PATTERNS:\n",
                "        matches = re.findall(pattern, text_lower)\n",
                "        if matches:\n",
                "            detected.extend(matches)\n",
                "            total_count += len(matches)\n",
                "    \n",
                "    # Calculate filler ratio\n",
                "    word_count = len(text.split())\n",
                "    filler_ratio = total_count / word_count if word_count > 0 else 0\n",
                "    \n",
                "    return {\n",
                "        'count': total_count,\n",
                "        'fillers': detected,\n",
                "        'unique_fillers': list(set(detected)),\n",
                "        'word_count': word_count,\n",
                "        'filler_ratio': filler_ratio\n",
                "    }\n",
                "\n",
                "# Test\n",
                "test_speech = \"\"\"\n",
                "So, um, I think that, you know, basically the main point is, like, \n",
                "we need to actually focus on, um, the core features. Right?\n",
                "\"\"\"\n",
                "\n",
                "result = detect_fillers(test_speech)\n",
                "print(f\"Detected {result['count']} fillers in {result['word_count']} words\")\n",
                "print(f\"Filler ratio: {result['filler_ratio']:.2%}\")\n",
                "print(f\"Fillers found: {result['unique_fillers']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Fluency Score Calculation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_fluency_score(\n",
                "    transcript: str,\n",
                "    duration_seconds: float,\n",
                "    pause_ratio: float = 0.0,  # Ratio of pauses to total duration\n",
                "    optimal_wpm_min: float = 120,\n",
                "    optimal_wpm_max: float = 160,\n",
                "    filler_penalty: float = 0.05  # Penalty per filler word\n",
                ") -> dict:\n",
                "    \"\"\"\n",
                "    Calculate overall fluency score (0-100).\n",
                "    \n",
                "    Components:\n",
                "    - WPM score (40%): Optimal between 120-160 WPM\n",
                "    - Filler penalty (40%): Deduction for filler words\n",
                "    - Pause penalty (20%): Deduction for excessive pauses\n",
                "    \n",
                "    Returns:\n",
                "        Dictionary with score breakdown\n",
                "    \"\"\"\n",
                "    # Calculate WPM\n",
                "    wpm = calculate_wpm(transcript, duration_seconds)\n",
                "    \n",
                "    # WPM score (0-100)\n",
                "    if optimal_wpm_min <= wpm <= optimal_wpm_max:\n",
                "        wpm_score = 100.0\n",
                "    elif wpm < optimal_wpm_min:\n",
                "        # Penalty for slow speech\n",
                "        wpm_score = max(0, 100 - (optimal_wpm_min - wpm) * 1.5)\n",
                "    else:\n",
                "        # Penalty for fast speech\n",
                "        wpm_score = max(0, 100 - (wpm - optimal_wpm_max) * 1.0)\n",
                "    \n",
                "    # Filler detection\n",
                "    filler_result = detect_fillers(transcript)\n",
                "    filler_count = filler_result['count']\n",
                "    filler_score = max(0, 100 - filler_count * filler_penalty * 100)\n",
                "    \n",
                "    # Pause penalty\n",
                "    pause_score = max(0, 100 - pause_ratio * 200)  # 50% pause ratio = 0 score\n",
                "    \n",
                "    # Weighted average\n",
                "    overall_score = (\n",
                "        wpm_score * 0.4 +\n",
                "        filler_score * 0.4 +\n",
                "        pause_score * 0.2\n",
                "    )\n",
                "    \n",
                "    return {\n",
                "        'overall_score': round(overall_score, 1),\n",
                "        'wpm': round(wpm, 1),\n",
                "        'wpm_score': round(wpm_score, 1),\n",
                "        'filler_count': filler_count,\n",
                "        'filler_score': round(filler_score, 1),\n",
                "        'fillers_detected': filler_result['unique_fillers'],\n",
                "        'pause_ratio': round(pause_ratio, 2),\n",
                "        'pause_score': round(pause_score, 1)\n",
                "    }\n",
                "\n",
                "# Test cases\n",
                "test_cases = [\n",
                "    (\"This is a clear and well articulated sentence without any filler words at all.\", 4.0),\n",
                "    (\"Um, so, like, I think, you know, the thing is, basically, um, yeah.\", 5.0),\n",
                "    (\"The quick brown fox jumps over the lazy dog near the river bank today.\", 3.0),\n",
                "]\n",
                "\n",
                "print(\"Fluency Score Tests:\")\n",
                "print(\"-\" * 60)\n",
                "for text, duration in test_cases:\n",
                "    score = calculate_fluency_score(text, duration)\n",
                "    print(f\"\\nText: '{text[:50]}...'\")\n",
                "    print(f\"  Overall: {score['overall_score']}/100\")\n",
                "    print(f\"  WPM: {score['wpm']} (score: {score['wpm_score']})\")\n",
                "    print(f\"  Fillers: {score['filler_count']} (score: {score['filler_score']})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Export Utility Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create utils module for backend integration\n",
                "utils_code = '''\n",
                "\"\"\"\n",
                "Fluency Analysis Utilities\n",
                "\n",
                "Provides functions for analyzing speech fluency including:\n",
                "- WPM calculation\n",
                "- Filler word detection\n",
                "- Overall fluency scoring\n",
                "\n",
                "Generated from notebook: 01_fluency_analysis.ipynb\n",
                "\"\"\"\n",
                "\n",
                "import re\n",
                "from typing import List, Dict, Optional\n",
                "\n",
                "# Common filler words and phrases\n",
                "FILLER_PATTERNS = [\n",
                "    r\"\\\\bum\\\\b\",\n",
                "    r\"\\\\buh\\\\b\",\n",
                "    r\"\\\\blike\\\\b\",\n",
                "    r\"\\\\byou know\\\\b\",\n",
                "    r\"\\\\bbasically\\\\b\",\n",
                "    r\"\\\\bactually\\\\b\",\n",
                "    r\"\\\\bliterally\\\\b\",\n",
                "    r\"\\\\bso\\\\b\",\n",
                "    r\"\\\\bwell\\\\b\",\n",
                "    r\"\\\\bi mean\\\\b\",\n",
                "    r\"\\\\bkind of\\\\b\",\n",
                "    r\"\\\\bsort of\\\\b\",\n",
                "]\n",
                "\n",
                "\n",
                "def calculate_wpm(text: str, duration_seconds: float) -> float:\n",
                "    \"\"\"Calculate words per minute from text and duration.\"\"\"\n",
                "    if duration_seconds <= 0:\n",
                "        return 0.0\n",
                "    words = [w for w in text.split() if w.strip()]\n",
                "    word_count = len(words)\n",
                "    minutes = duration_seconds / 60.0\n",
                "    return word_count / minutes if minutes > 0 else 0\n",
                "\n",
                "\n",
                "def detect_fillers(text: str) -> Dict:\n",
                "    \"\"\"Detect filler words in text.\"\"\"\n",
                "    text_lower = text.lower()\n",
                "    detected = []\n",
                "    total_count = 0\n",
                "    \n",
                "    for pattern in FILLER_PATTERNS:\n",
                "        matches = re.findall(pattern, text_lower)\n",
                "        if matches:\n",
                "            detected.extend(matches)\n",
                "            total_count += len(matches)\n",
                "    \n",
                "    word_count = len(text.split())\n",
                "    filler_ratio = total_count / word_count if word_count > 0 else 0\n",
                "    \n",
                "    return {\n",
                "        \"count\": total_count,\n",
                "        \"fillers\": detected,\n",
                "        \"unique_fillers\": list(set(detected)),\n",
                "        \"word_count\": word_count,\n",
                "        \"filler_ratio\": filler_ratio\n",
                "    }\n",
                "\n",
                "\n",
                "def calculate_fluency_score(\n",
                "    transcript: str,\n",
                "    duration_seconds: float,\n",
                "    pause_ratio: float = 0.0,\n",
                "    optimal_wpm_min: float = 120,\n",
                "    optimal_wpm_max: float = 160,\n",
                "    filler_penalty: float = 0.05\n",
                ") -> Dict:\n",
                "    \"\"\"Calculate overall fluency score (0-100).\"\"\"\n",
                "    wpm = calculate_wpm(transcript, duration_seconds)\n",
                "    \n",
                "    # WPM score\n",
                "    if optimal_wpm_min <= wpm <= optimal_wpm_max:\n",
                "        wpm_score = 100.0\n",
                "    elif wpm < optimal_wpm_min:\n",
                "        wpm_score = max(0, 100 - (optimal_wpm_min - wpm) * 1.5)\n",
                "    else:\n",
                "        wpm_score = max(0, 100 - (wpm - optimal_wpm_max) * 1.0)\n",
                "    \n",
                "    # Filler detection\n",
                "    filler_result = detect_fillers(transcript)\n",
                "    filler_count = filler_result[\"count\"]\n",
                "    filler_score = max(0, 100 - filler_count * filler_penalty * 100)\n",
                "    \n",
                "    # Pause penalty\n",
                "    pause_score = max(0, 100 - pause_ratio * 200)\n",
                "    \n",
                "    # Weighted average\n",
                "    overall_score = wpm_score * 0.4 + filler_score * 0.4 + pause_score * 0.2\n",
                "    \n",
                "    return {\n",
                "        \"overall_score\": round(overall_score, 1),\n",
                "        \"wpm\": round(wpm, 1),\n",
                "        \"wpm_score\": round(wpm_score, 1),\n",
                "        \"filler_count\": filler_count,\n",
                "        \"filler_score\": round(filler_score, 1),\n",
                "        \"fillers_detected\": filler_result[\"unique_fillers\"],\n",
                "        \"pause_ratio\": round(pause_ratio, 2),\n",
                "        \"pause_score\": round(pause_score, 1)\n",
                "    }\n",
                "'''\n",
                "\n",
                "# Save to training directory\n",
                "utils_path = PROJECT_ROOT / 'ml' / 'softskills' / 'training' / 'fluency_utils.py'\n",
                "utils_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "with open(utils_path, 'w') as f:\n",
                "    f.write(utils_code)\n",
                "\n",
                "print(f\"Exported utilities to: {utils_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Summary\n",
                "\n",
                "### Key Findings\n",
                "1. PodcastFillers dataset contains labeled filler words (um, uh) from podcast audio\n",
                "2. Optimal WPM for interviews: 120-160 words per minute\n",
                "3. Filler words can be detected using regex patterns\n",
                "4. Fluency score combines WPM, filler count, and pause ratio\n",
                "\n",
                "### Next Steps\n",
                "1. Train a neural network for audio-based filler detection\n",
                "2. Implement Voice Activity Detection (VAD) for pause analysis\n",
                "3. Integrate with backend `fluency_analyzer.py` service"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}