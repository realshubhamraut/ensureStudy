{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gesture Recognition - Hand Movement Analysis\n",
                "\n",
                "This notebook implements hand gesture analysis using MediaPipe Hands.\n",
                "\n",
                "## Goals\n",
                "1. Set up MediaPipe Hands for landmark detection\n",
                "2. Analyze the HaGRID gesture dataset\n",
                "3. Calculate movement frequency and stability\n",
                "4. Create gesture scoring function\n",
                "5. Export utility functions for backend integration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies if needed\n",
                "# !pip install mediapipe opencv-python numpy matplotlib torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# Add project root to path\n",
                "PROJECT_ROOT = Path(os.getcwd()).parent.parent.parent\n",
                "sys.path.insert(0, str(PROJECT_ROOT))\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import mediapipe as mp\n",
                "from collections import deque\n",
                "\n",
                "print(f\"OpenCV version: {cv2.__version__}\")\n",
                "print(f\"MediaPipe version: {mp.__version__}\")\n",
                "print(f\"Project root: {PROJECT_ROOT}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Initialize MediaPipe Hands"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize MediaPipe Hands\n",
                "mp_hands = mp.solutions.hands\n",
                "mp_drawing = mp.solutions.drawing_utils\n",
                "mp_drawing_styles = mp.solutions.drawing_styles\n",
                "\n",
                "# Hands detector\n",
                "hands = mp_hands.Hands(\n",
                "    static_image_mode=False,\n",
                "    max_num_hands=2,\n",
                "    min_detection_confidence=0.5,\n",
                "    min_tracking_confidence=0.5\n",
                ")\n",
                "\n",
                "print(\"MediaPipe Hands initialized\")\n",
                "print(f\"Max hands: 2\")\n",
                "print(f\"21 landmarks per hand (wrist + 4 per finger)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Explore HaGRID Dataset Structure"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if HaGRID dataset is available\n",
                "HAGRID_PATH = PROJECT_ROOT / \"ml\" / \"softskills\" / \"datasets\" / \"gestures\" / \"hagrid\"\n",
                "\n",
                "if HAGRID_PATH.exists():\n",
                "    print(f\"HaGRID dataset found at: {HAGRID_PATH}\")\n",
                "    print(\"\\nDirectory contents:\")\n",
                "    for item in sorted(HAGRID_PATH.iterdir()):\n",
                "        if item.is_dir():\n",
                "            print(f\"  üìÅ {item.name}/\")\n",
                "        else:\n",
                "            print(f\"  üìÑ {item.name}\")\n",
                "else:\n",
                "    print(\"HaGRID dataset not found. Using simulated data.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# HaGRID gesture classes\n",
                "HAGRID_GESTURES = [\n",
                "    'call',          # Phone gesture\n",
                "    'dislike',       # Thumbs down\n",
                "    'fist',          # Closed fist\n",
                "    'four',          # Four fingers\n",
                "    'like',          # Thumbs up\n",
                "    'mute',          # Finger on lips (silence)\n",
                "    'ok',            # OK sign\n",
                "    'one',           # Index finger up\n",
                "    'palm',          # Open palm\n",
                "    'peace',         # Peace sign\n",
                "    'peace_inverted',\n",
                "    'rock',          # Rock and roll\n",
                "    'stop',          # Stop gesture\n",
                "    'stop_inverted',\n",
                "    'three',         # Three fingers\n",
                "    'three2',        # Alternative three\n",
                "    'two_up',        # Two fingers up\n",
                "    'two_up_inverted',\n",
                "    'no_gesture'     # No specific gesture\n",
                "]\n",
                "\n",
                "print(f\"HaGRID contains {len(HAGRID_GESTURES)} gesture classes\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hand Landmark Processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_hand_center(landmarks, img_width, img_height):\n",
                "    \"\"\"\n",
                "    Calculate center of hand from landmarks.\n",
                "    \n",
                "    Args:\n",
                "        landmarks: MediaPipe hand landmarks\n",
                "        img_width: Image width\n",
                "        img_height: Image height\n",
                "    \n",
                "    Returns:\n",
                "        (x, y) center coordinates\n",
                "    \"\"\"\n",
                "    x_coords = [lm.x * img_width for lm in landmarks.landmark]\n",
                "    y_coords = [lm.y * img_height for lm in landmarks.landmark]\n",
                "    \n",
                "    return (\n",
                "        sum(x_coords) / len(x_coords),\n",
                "        sum(y_coords) / len(y_coords)\n",
                "    )\n",
                "\n",
                "\n",
                "def get_hand_bbox(landmarks, img_width, img_height):\n",
                "    \"\"\"\n",
                "    Get bounding box of hand.\n",
                "    \n",
                "    Returns:\n",
                "        (x_min, y_min, x_max, y_max)\n",
                "    \"\"\"\n",
                "    x_coords = [lm.x * img_width for lm in landmarks.landmark]\n",
                "    y_coords = [lm.y * img_height for lm in landmarks.landmark]\n",
                "    \n",
                "    return (\n",
                "        min(x_coords),\n",
                "        min(y_coords),\n",
                "        max(x_coords),\n",
                "        max(y_coords)\n",
                "    )\n",
                "\n",
                "\n",
                "def get_hand_spread(landmarks):\n",
                "    \"\"\"\n",
                "    Calculate hand spread (openness).\n",
                "    Higher value = more open hand.\n",
                "    \n",
                "    Returns:\n",
                "        Float representing relative spread\n",
                "    \"\"\"\n",
                "    # Fingertip indices: thumb=4, index=8, middle=12, ring=16, pinky=20\n",
                "    fingertips = [4, 8, 12, 16, 20]\n",
                "    wrist = landmarks.landmark[0]\n",
                "    \n",
                "    # Calculate average distance from wrist to fingertips\n",
                "    distances = []\n",
                "    for tip_idx in fingertips:\n",
                "        tip = landmarks.landmark[tip_idx]\n",
                "        dist = np.sqrt((tip.x - wrist.x)**2 + (tip.y - wrist.y)**2 + (tip.z - wrist.z)**2)\n",
                "        distances.append(dist)\n",
                "    \n",
                "    return sum(distances) / len(distances)\n",
                "\n",
                "print(\"Hand processing functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Movement Tracking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class HandMovementTracker:\n",
                "    \"\"\"\n",
                "    Tracks hand movements over time to calculate:\n",
                "    - Movement frequency\n",
                "    - Movement amplitude\n",
                "    - Stability score\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, history_size=30, fps=30):\n",
                "        self.history_size = history_size\n",
                "        self.fps = fps\n",
                "        self.left_hand_history = deque(maxlen=history_size)\n",
                "        self.right_hand_history = deque(maxlen=history_size)\n",
                "    \n",
                "    def update(self, left_hand_center=None, right_hand_center=None):\n",
                "        \"\"\"\n",
                "        Update position history.\n",
                "        \n",
                "        Args:\n",
                "            left_hand_center: (x, y) or None if not detected\n",
                "            right_hand_center: (x, y) or None if not detected\n",
                "        \"\"\"\n",
                "        self.left_hand_history.append(left_hand_center)\n",
                "        self.right_hand_history.append(right_hand_center)\n",
                "    \n",
                "    def _calculate_movement(self, history):\n",
                "        \"\"\"\n",
                "        Calculate movement metrics from position history.\n",
                "        \"\"\"\n",
                "        # Filter out None values\n",
                "        valid_positions = [p for p in history if p is not None]\n",
                "        \n",
                "        if len(valid_positions) < 2:\n",
                "            return {\n",
                "                'visible': False,\n",
                "                'movement_distance': 0,\n",
                "                'velocity': 0,\n",
                "                'stability': 100\n",
                "            }\n",
                "        \n",
                "        # Calculate frame-to-frame distances\n",
                "        distances = []\n",
                "        for i in range(1, len(valid_positions)):\n",
                "            prev = valid_positions[i-1]\n",
                "            curr = valid_positions[i]\n",
                "            dist = np.sqrt((curr[0] - prev[0])**2 + (curr[1] - prev[1])**2)\n",
                "            distances.append(dist)\n",
                "        \n",
                "        total_distance = sum(distances)\n",
                "        avg_velocity = np.mean(distances) * self.fps  # pixels per second\n",
                "        \n",
                "        # Stability: lower movement = higher stability\n",
                "        # Normalized to 0-100 scale\n",
                "        stability = max(0, 100 - avg_velocity * 0.5)\n",
                "        \n",
                "        return {\n",
                "            'visible': True,\n",
                "            'visibility_ratio': len(valid_positions) / len(history),\n",
                "            'movement_distance': total_distance,\n",
                "            'velocity': avg_velocity,\n",
                "            'stability': stability\n",
                "        }\n",
                "    \n",
                "    def get_metrics(self):\n",
                "        \"\"\"\n",
                "        Get movement metrics for both hands.\n",
                "        \"\"\"\n",
                "        left = self._calculate_movement(self.left_hand_history)\n",
                "        right = self._calculate_movement(self.right_hand_history)\n",
                "        \n",
                "        # Combine metrics\n",
                "        hands_visible = left['visible'] or right['visible']\n",
                "        num_hands = int(left['visible']) + int(right['visible'])\n",
                "        \n",
                "        if not hands_visible:\n",
                "            avg_stability = 0\n",
                "        else:\n",
                "            stabilities = []\n",
                "            if left['visible']:\n",
                "                stabilities.append(left['stability'])\n",
                "            if right['visible']:\n",
                "                stabilities.append(right['stability'])\n",
                "            avg_stability = np.mean(stabilities)\n",
                "        \n",
                "        return {\n",
                "            'hands_visible': hands_visible,\n",
                "            'num_hands': num_hands,\n",
                "            'left_hand': left,\n",
                "            'right_hand': right,\n",
                "            'avg_stability': avg_stability\n",
                "        }\n",
                "\n",
                "# Test\n",
                "tracker = HandMovementTracker(history_size=10)\n",
                "\n",
                "# Simulate some movement\n",
                "for i in range(10):\n",
                "    tracker.update(\n",
                "        left_hand_center=(100 + i*5, 200 + i*2),\n",
                "        right_hand_center=(300 + i*3, 200 - i*1)\n",
                "    )\n",
                "\n",
                "metrics = tracker.get_metrics()\n",
                "print(\"Movement Tracker Test:\")\n",
                "print(f\"  Hands visible: {metrics['hands_visible']}\")\n",
                "print(f\"  Num hands: {metrics['num_hands']}\")\n",
                "print(f\"  Avg stability: {metrics['avg_stability']:.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Gesture Scoring for Interview Context"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_gesture_score(\n",
                "    hands_visible: bool,\n",
                "    num_hands: int,\n",
                "    stability: float,\n",
                "    movement_velocity: float,\n",
                "    visibility_ratio: float = 1.0,\n",
                "    optimal_velocity_min: float = 20,   # Some movement is good\n",
                "    optimal_velocity_max: float = 100,  # But not too much\n",
                "    stability_weight: float = 0.4,\n",
                "    movement_weight: float = 0.3,\n",
                "    visibility_weight: float = 0.3\n",
                ") -> dict:\n",
                "    \"\"\"\n",
                "    Calculate gesture score for interview context.\n",
                "    \n",
                "    Good gestures in interviews:\n",
                "    - Visible hands (builds trust)\n",
                "    - Moderate, natural movement (engages audience)\n",
                "    - Stable, controlled gestures (shows confidence)\n",
                "    - Not too static (robotic) or too fidgety (nervous)\n",
                "    \n",
                "    Returns:\n",
                "        dict: Score breakdown\n",
                "    \"\"\"\n",
                "    # Visibility score\n",
                "    if hands_visible:\n",
                "        visibility_score = min(100, visibility_ratio * 100)\n",
                "    else:\n",
                "        visibility_score = 30  # Penalty for hidden hands\n",
                "    \n",
                "    # Movement score (optimal range)\n",
                "    if optimal_velocity_min <= movement_velocity <= optimal_velocity_max:\n",
                "        movement_score = 100\n",
                "    elif movement_velocity < optimal_velocity_min:\n",
                "        # Too static\n",
                "        movement_score = 70 + (movement_velocity / optimal_velocity_min) * 30\n",
                "    else:\n",
                "        # Too fidgety\n",
                "        excess = movement_velocity - optimal_velocity_max\n",
                "        movement_score = max(30, 100 - excess * 0.5)\n",
                "    \n",
                "    # Stability score (already 0-100)\n",
                "    stability_score = stability\n",
                "    \n",
                "    # Weighted average\n",
                "    overall_score = (\n",
                "        stability_score * stability_weight +\n",
                "        movement_score * movement_weight +\n",
                "        visibility_score * visibility_weight\n",
                "    )\n",
                "    \n",
                "    # Determine assessment\n",
                "    if overall_score >= 80:\n",
                "        assessment = 'excellent'\n",
                "    elif overall_score >= 60:\n",
                "        assessment = 'good'\n",
                "    elif overall_score >= 40:\n",
                "        assessment = 'needs_improvement'\n",
                "    else:\n",
                "        assessment = 'poor'\n",
                "    \n",
                "    return {\n",
                "        'overall_score': round(overall_score, 1),\n",
                "        'visibility_score': round(visibility_score, 1),\n",
                "        'movement_score': round(movement_score, 1),\n",
                "        'stability_score': round(stability_score, 1),\n",
                "        'hands_visible': hands_visible,\n",
                "        'num_hands': num_hands,\n",
                "        'assessment': assessment\n",
                "    }\n",
                "\n",
                "# Test cases\n",
                "test_cases = [\n",
                "    (True, 2, 85, 50, 1.0, \"Natural gesturing\"),\n",
                "    (True, 1, 95, 10, 0.8, \"Minimal movement (too static)\"),\n",
                "    (True, 2, 40, 150, 1.0, \"Fidgety/nervous\"),\n",
                "    (False, 0, 0, 0, 0.0, \"Hidden hands\"),\n",
                "]\n",
                "\n",
                "print(\"Gesture Score Tests:\")\n",
                "print(\"-\" * 60)\n",
                "for visible, num, stability, velocity, vis_ratio, desc in test_cases:\n",
                "    result = calculate_gesture_score(visible, num, stability, velocity, vis_ratio)\n",
                "    print(f\"\\n{desc}\")\n",
                "    print(f\"  Overall: {result['overall_score']}/100 ({result['assessment']})\")\n",
                "    print(f\"  Visibility: {result['visibility_score']}, Movement: {result['movement_score']}, Stability: {result['stability_score']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Frame Analysis Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_frame_hands(frame, tracker=None):\n",
                "    \"\"\"\n",
                "    Analyze a single frame for hand gestures.\n",
                "    \n",
                "    Args:\n",
                "        frame: BGR image\n",
                "        tracker: Optional HandMovementTracker for temporal analysis\n",
                "    \n",
                "    Returns:\n",
                "        dict: Hand analysis results\n",
                "    \"\"\"\n",
                "    # Convert BGR to RGB\n",
                "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
                "    h, w = frame.shape[:2]\n",
                "    \n",
                "    # Process with MediaPipe\n",
                "    results = hands.process(rgb_frame)\n",
                "    \n",
                "    left_center = None\n",
                "    right_center = None\n",
                "    hand_spreads = []\n",
                "    \n",
                "    if results.multi_hand_landmarks:\n",
                "        for hand_landmarks, handedness in zip(\n",
                "            results.multi_hand_landmarks,\n",
                "            results.multi_handedness\n",
                "        ):\n",
                "            # Get hand label (Left/Right)\n",
                "            label = handedness.classification[0].label\n",
                "            center = get_hand_center(hand_landmarks, w, h)\n",
                "            spread = get_hand_spread(hand_landmarks)\n",
                "            hand_spreads.append(spread)\n",
                "            \n",
                "            if label == 'Left':\n",
                "                left_center = center\n",
                "            else:\n",
                "                right_center = center\n",
                "    \n",
                "    # Update tracker if provided\n",
                "    if tracker is not None:\n",
                "        tracker.update(left_center, right_center)\n",
                "        metrics = tracker.get_metrics()\n",
                "    else:\n",
                "        # Single frame analysis (no temporal info)\n",
                "        metrics = {\n",
                "            'hands_visible': left_center is not None or right_center is not None,\n",
                "            'num_hands': sum([left_center is not None, right_center is not None]),\n",
                "            'avg_stability': 100 if (left_center or right_center) else 0\n",
                "        }\n",
                "    \n",
                "    # Calculate gesture score\n",
                "    velocity = 0\n",
                "    if 'left_hand' in metrics and metrics['left_hand'].get('visible'):\n",
                "        velocity = max(velocity, metrics['left_hand'].get('velocity', 0))\n",
                "    if 'right_hand' in metrics and metrics['right_hand'].get('visible'):\n",
                "        velocity = max(velocity, metrics['right_hand'].get('velocity', 0))\n",
                "    \n",
                "    score_result = calculate_gesture_score(\n",
                "        metrics['hands_visible'],\n",
                "        metrics['num_hands'],\n",
                "        metrics['avg_stability'],\n",
                "        velocity\n",
                "    )\n",
                "    \n",
                "    # Add hand positions\n",
                "    score_result['left_hand_center'] = left_center\n",
                "    score_result['right_hand_center'] = right_center\n",
                "    score_result['avg_spread'] = np.mean(hand_spreads) if hand_spreads else 0\n",
                "    \n",
                "    return score_result\n",
                "\n",
                "print(\"Frame analysis function defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Export Utility Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create utils module for backend integration\n",
                "utils_code = '''\n",
                "\"\"\"\n",
                "Gesture Analysis Utilities\n",
                "\n",
                "Provides functions for analyzing hand gestures including:\n",
                "- Hand detection and tracking\n",
                "- Movement frequency/stability calculation\n",
                "- Gesture scoring for interviews\n",
                "\n",
                "Generated from notebook: 03_gesture_recognition.ipynb\n",
                "\"\"\"\n",
                "\n",
                "import numpy as np\n",
                "from collections import deque\n",
                "from typing import Dict, Tuple, Optional, List\n",
                "\n",
                "\n",
                "def get_hand_center(landmarks, img_width: int, img_height: int) -> Tuple[float, float]:\n",
                "    \"\"\"Calculate center of hand from landmarks.\"\"\"\n",
                "    x_coords = [lm.x * img_width for lm in landmarks.landmark]\n",
                "    y_coords = [lm.y * img_height for lm in landmarks.landmark]\n",
                "    return (sum(x_coords) / len(x_coords), sum(y_coords) / len(y_coords))\n",
                "\n",
                "\n",
                "def get_hand_spread(landmarks) -> float:\n",
                "    \"\"\"Calculate hand spread (openness).\"\"\"\n",
                "    fingertips = [4, 8, 12, 16, 20]\n",
                "    wrist = landmarks.landmark[0]\n",
                "    distances = []\n",
                "    for tip_idx in fingertips:\n",
                "        tip = landmarks.landmark[tip_idx]\n",
                "        dist = np.sqrt((tip.x - wrist.x)**2 + (tip.y - wrist.y)**2 + (tip.z - wrist.z)**2)\n",
                "        distances.append(dist)\n",
                "    return sum(distances) / len(distances)\n",
                "\n",
                "\n",
                "class HandMovementTracker:\n",
                "    \"\"\"Tracks hand movements over time.\"\"\"\n",
                "    \n",
                "    def __init__(self, history_size: int = 30, fps: int = 30):\n",
                "        self.history_size = history_size\n",
                "        self.fps = fps\n",
                "        self.left_hand_history = deque(maxlen=history_size)\n",
                "        self.right_hand_history = deque(maxlen=history_size)\n",
                "    \n",
                "    def update(self, left_hand_center=None, right_hand_center=None):\n",
                "        self.left_hand_history.append(left_hand_center)\n",
                "        self.right_hand_history.append(right_hand_center)\n",
                "    \n",
                "    def _calculate_movement(self, history) -> Dict:\n",
                "        valid_positions = [p for p in history if p is not None]\n",
                "        if len(valid_positions) < 2:\n",
                "            return {\"visible\": False, \"movement_distance\": 0, \"velocity\": 0, \"stability\": 100}\n",
                "        \n",
                "        distances = []\n",
                "        for i in range(1, len(valid_positions)):\n",
                "            prev, curr = valid_positions[i-1], valid_positions[i]\n",
                "            dist = np.sqrt((curr[0] - prev[0])**2 + (curr[1] - prev[1])**2)\n",
                "            distances.append(dist)\n",
                "        \n",
                "        avg_velocity = np.mean(distances) * self.fps\n",
                "        stability = max(0, 100 - avg_velocity * 0.5)\n",
                "        \n",
                "        return {\n",
                "            \"visible\": True,\n",
                "            \"visibility_ratio\": len(valid_positions) / len(history),\n",
                "            \"movement_distance\": sum(distances),\n",
                "            \"velocity\": avg_velocity,\n",
                "            \"stability\": stability\n",
                "        }\n",
                "    \n",
                "    def get_metrics(self) -> Dict:\n",
                "        left = self._calculate_movement(self.left_hand_history)\n",
                "        right = self._calculate_movement(self.right_hand_history)\n",
                "        \n",
                "        hands_visible = left[\"visible\"] or right[\"visible\"]\n",
                "        stabilities = [h[\"stability\"] for h in [left, right] if h[\"visible\"]]\n",
                "        \n",
                "        return {\n",
                "            \"hands_visible\": hands_visible,\n",
                "            \"num_hands\": int(left[\"visible\"]) + int(right[\"visible\"]),\n",
                "            \"left_hand\": left,\n",
                "            \"right_hand\": right,\n",
                "            \"avg_stability\": np.mean(stabilities) if stabilities else 0\n",
                "        }\n",
                "\n",
                "\n",
                "def calculate_gesture_score(\n",
                "    hands_visible: bool,\n",
                "    num_hands: int,\n",
                "    stability: float,\n",
                "    movement_velocity: float,\n",
                "    visibility_ratio: float = 1.0\n",
                ") -> Dict:\n",
                "    \"\"\"Calculate gesture score for interview context.\"\"\"\n",
                "    visibility_score = min(100, visibility_ratio * 100) if hands_visible else 30\n",
                "    \n",
                "    if 20 <= movement_velocity <= 100:\n",
                "        movement_score = 100\n",
                "    elif movement_velocity < 20:\n",
                "        movement_score = 70 + (movement_velocity / 20) * 30\n",
                "    else:\n",
                "        movement_score = max(30, 100 - (movement_velocity - 100) * 0.5)\n",
                "    \n",
                "    overall_score = stability * 0.4 + movement_score * 0.3 + visibility_score * 0.3\n",
                "    \n",
                "    return {\n",
                "        \"overall_score\": round(overall_score, 1),\n",
                "        \"visibility_score\": round(visibility_score, 1),\n",
                "        \"movement_score\": round(movement_score, 1),\n",
                "        \"stability_score\": round(stability, 1),\n",
                "        \"hands_visible\": hands_visible,\n",
                "        \"num_hands\": num_hands\n",
                "    }\n",
                "'''\n",
                "\n",
                "# Save to training directory\n",
                "utils_path = PROJECT_ROOT / 'ml' / 'softskills' / 'training' / 'gesture_utils.py'\n",
                "utils_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "with open(utils_path, 'w') as f:\n",
                "    f.write(utils_code)\n",
                "\n",
                "print(f\"Exported utilities to: {utils_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Summary\n",
                "\n",
                "### Key Findings\n",
                "1. MediaPipe Hands provides 21 landmarks per hand\n",
                "2. Hand visibility is important for trust in interviews\n",
                "3. Optimal gesturing: moderate movement, not too static or fidgety\n",
                "4. Stability indicates confidence and control\n",
                "\n",
                "### Scoring Criteria\n",
                "- **Visibility (30%)**: Are hands visible? Builds trust.\n",
                "- **Movement (30%)**: Natural, moderate gesturing (20-100 px/s)\n",
                "- **Stability (40%)**: Controlled, not shaky or nervous\n",
                "\n",
                "### Next Steps\n",
                "1. Classify specific gesture types if needed\n",
                "2. Add self-touch detection (face touching, etc.)\n",
                "3. Integrate with backend `gesture_analyzer.py` service"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}