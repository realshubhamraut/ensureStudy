{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Speech Fluency Analysis Model Training\n",
                "\n",
                "**Purpose**: Analyze mock interview speech for fluency scoring\n",
                "\n",
                "**Architecture**: wav2vec2 + GRU\n",
                "\n",
                "**Outputs**:\n",
                "- Fluency score (0-10)\n",
                "- Hesitation count\n",
                "- Filler word detection"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (run once)\n",
                "# !pip install torch torchaudio transformers librosa soundfile datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import torchaudio\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os\n",
                "import random\n",
                "from pathlib import Path\n",
                "\n",
                "# Reproducibility\n",
                "SEED = 42\n",
                "torch.manual_seed(SEED)\n",
                "np.random.seed(SEED)\n",
                "random.seed(SEED)\n",
                "\n",
                "# Device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Using device: {device}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Loading & Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
                "\n",
                "# Load wav2vec2 processor and model\n",
                "MODEL_NAME = \"facebook/wav2vec2-base\"\n",
                "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
                "wav2vec2 = Wav2Vec2Model.from_pretrained(MODEL_NAME)\n",
                "wav2vec2.eval()  # Freeze for feature extraction\n",
                "\n",
                "print(f'Loaded {MODEL_NAME}')\n",
                "print(f'Hidden size: {wav2vec2.config.hidden_size}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SpeechFluencyDataset(Dataset):\n",
                "    \"\"\"\n",
                "    Dataset for speech fluency analysis.\n",
                "    \n",
                "    Expected CSV format:\n",
                "    - audio_path: path to audio file\n",
                "    - fluency_score: 0-10 score\n",
                "    - hesitation_count: number of hesitations\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, csv_path, audio_dir, max_length_seconds=30, sample_rate=16000):\n",
                "        self.df = pd.read_csv(csv_path)\n",
                "        self.audio_dir = Path(audio_dir)\n",
                "        self.max_length = max_length_seconds * sample_rate\n",
                "        self.sample_rate = sample_rate\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        \n",
                "        # Load audio\n",
                "        audio_path = self.audio_dir / row['audio_path']\n",
                "        waveform, sr = torchaudio.load(audio_path)\n",
                "        \n",
                "        # Resample if needed\n",
                "        if sr != self.sample_rate:\n",
                "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
                "            waveform = resampler(waveform)\n",
                "        \n",
                "        # Convert to mono\n",
                "        if waveform.shape[0] > 1:\n",
                "            waveform = waveform.mean(dim=0, keepdim=True)\n",
                "        \n",
                "        # Truncate or pad\n",
                "        waveform = waveform.squeeze(0)\n",
                "        if len(waveform) > self.max_length:\n",
                "            waveform = waveform[:self.max_length]\n",
                "        else:\n",
                "            waveform = torch.nn.functional.pad(waveform, (0, self.max_length - len(waveform)))\n",
                "        \n",
                "        # Labels\n",
                "        fluency_score = float(row['fluency_score']) / 10.0  # Normalize to 0-1\n",
                "        hesitation_count = float(row.get('hesitation_count', 0))\n",
                "        \n",
                "        return {\n",
                "            'waveform': waveform,\n",
                "            'fluency_score': torch.tensor(fluency_score, dtype=torch.float),\n",
                "            'hesitation_count': torch.tensor(hesitation_count, dtype=torch.float)\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_synthetic_dataset(n_samples=500, output_dir='../datasets/speech_samples'):\n",
                "    \"\"\"\n",
                "    Generate synthetic training data (for demo purposes).\n",
                "    In production, use real labeled interview recordings.\n",
                "    \"\"\"\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    \n",
                "    data = []\n",
                "    for i in range(n_samples):\n",
                "        # Random fluency characteristics\n",
                "        fluency = np.random.uniform(4, 10)  # 4-10 range\n",
                "        hesitations = int(max(0, 10 - fluency) + np.random.randint(0, 3))\n",
                "        \n",
                "        # Create synthetic audio (sine wave with noise)\n",
                "        duration = np.random.uniform(5, 15)  # 5-15 seconds\n",
                "        sr = 16000\n",
                "        t = np.linspace(0, duration, int(sr * duration))\n",
                "        \n",
                "        # Base frequency varies with \"confidence\"\n",
                "        freq = 200 + fluency * 10\n",
                "        audio = np.sin(2 * np.pi * freq * t) * 0.3\n",
                "        audio += np.random.randn(len(audio)) * (0.05 + (10 - fluency) * 0.01)\n",
                "        \n",
                "        # Save audio\n",
                "        audio_path = f'sample_{i:04d}.wav'\n",
                "        full_path = os.path.join(output_dir, audio_path)\n",
                "        torchaudio.save(full_path, torch.tensor(audio).unsqueeze(0).float(), sr)\n",
                "        \n",
                "        data.append({\n",
                "            'audio_path': audio_path,\n",
                "            'fluency_score': round(fluency, 1),\n",
                "            'hesitation_count': hesitations\n",
                "        })\n",
                "    \n",
                "    # Save CSV\n",
                "    df = pd.DataFrame(data)\n",
                "    csv_path = os.path.join(output_dir, 'labels.csv')\n",
                "    df.to_csv(csv_path, index=False)\n",
                "    \n",
                "    print(f'Generated {n_samples} samples')\n",
                "    print(f'Saved to: {output_dir}')\n",
                "    \n",
                "    return df\n",
                "\n",
                "# Generate synthetic data (comment out if using real data)\n",
                "# df = generate_synthetic_dataset()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Definition"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SpeechFluencyModel(nn.Module):\n",
                "    \"\"\"\n",
                "    Speech Fluency Analysis Model\n",
                "    \n",
                "    Architecture:\n",
                "    - wav2vec2 for audio feature extraction (frozen)\n",
                "    - Bidirectional GRU for temporal modeling\n",
                "    - MLP heads for fluency score and hesitation count\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        wav2vec2_model,\n",
                "        hidden_size=256,\n",
                "        num_gru_layers=2,\n",
                "        dropout=0.3,\n",
                "        freeze_wav2vec=True\n",
                "    ):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.wav2vec2 = wav2vec2_model\n",
                "        wav2vec_hidden = wav2vec2_model.config.hidden_size  # 768\n",
                "        \n",
                "        # Freeze wav2vec2 initially\n",
                "        if freeze_wav2vec:\n",
                "            for param in self.wav2vec2.parameters():\n",
                "                param.requires_grad = False\n",
                "        \n",
                "        # Bidirectional GRU\n",
                "        self.gru = nn.GRU(\n",
                "            input_size=wav2vec_hidden,\n",
                "            hidden_size=hidden_size,\n",
                "            num_layers=num_gru_layers,\n",
                "            batch_first=True,\n",
                "            bidirectional=True,\n",
                "            dropout=dropout if num_gru_layers > 1 else 0\n",
                "        )\n",
                "        \n",
                "        gru_output_size = hidden_size * 2  # Bidirectional\n",
                "        \n",
                "        # Fluency score head (regression: 0-1)\n",
                "        self.fluency_head = nn.Sequential(\n",
                "            nn.Linear(gru_output_size, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(128, 1),\n",
                "            nn.Sigmoid()\n",
                "        )\n",
                "        \n",
                "        # Hesitation count head (regression)\n",
                "        self.hesitation_head = nn.Sequential(\n",
                "            nn.Linear(gru_output_size, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(128, 1),\n",
                "            nn.ReLU()  # Count is non-negative\n",
                "        )\n",
                "    \n",
                "    def forward(self, waveforms):\n",
                "        # Extract wav2vec2 features\n",
                "        with torch.no_grad() if not self.training else torch.enable_grad():\n",
                "            outputs = self.wav2vec2(waveforms)\n",
                "            features = outputs.last_hidden_state  # (batch, seq_len, 768)\n",
                "        \n",
                "        # GRU\n",
                "        gru_out, _ = self.gru(features)  # (batch, seq_len, hidden*2)\n",
                "        \n",
                "        # Global average pooling\n",
                "        pooled = gru_out.mean(dim=1)  # (batch, hidden*2)\n",
                "        \n",
                "        # Predictions\n",
                "        fluency_score = self.fluency_head(pooled).squeeze(-1)\n",
                "        hesitation_count = self.hesitation_head(pooled).squeeze(-1)\n",
                "        \n",
                "        return {\n",
                "            'fluency_score': fluency_score,\n",
                "            'hesitation_count': hesitation_count\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize model\n",
                "model = SpeechFluencyModel(wav2vec2, hidden_size=256, num_gru_layers=2)\n",
                "model = model.to(device)\n",
                "\n",
                "# Count parameters\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "\n",
                "print(f'Trainable parameters: {trainable_params:,}')\n",
                "print(f'Total parameters: {total_params:,}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training config\n",
                "CONFIG = {\n",
                "    'epochs': 30,\n",
                "    'batch_size': 8,\n",
                "    'learning_rate': 1e-4,\n",
                "    'weight_decay': 0.01,\n",
                "    'fluency_weight': 0.7,  # Weight for fluency loss\n",
                "    'hesitation_weight': 0.3,  # Weight for hesitation loss\n",
                "    'early_stopping_patience': 5,\n",
                "    'checkpoint_dir': '../models/speech_model'\n",
                "}\n",
                "\n",
                "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Loss and optimizer\n",
                "mse_loss = nn.MSELoss()\n",
                "optimizer = optim.AdamW(\n",
                "    filter(lambda p: p.requires_grad, model.parameters()),\n",
                "    lr=CONFIG['learning_rate'],\n",
                "    weight_decay=CONFIG['weight_decay']\n",
                ")\n",
                "\n",
                "# Learning rate scheduler\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
                "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, dataloader, optimizer, device):\n",
                "    model.train()\n",
                "    total_loss = 0.0\n",
                "    fluency_mae = 0.0\n",
                "    hesitation_mae = 0.0\n",
                "    \n",
                "    for batch in dataloader:\n",
                "        waveforms = batch['waveform'].to(device)\n",
                "        fluency_targets = batch['fluency_score'].to(device)\n",
                "        hesitation_targets = batch['hesitation_count'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Forward pass\n",
                "        outputs = model(waveforms)\n",
                "        \n",
                "        # Losses\n",
                "        fluency_loss = mse_loss(outputs['fluency_score'], fluency_targets)\n",
                "        hesitation_loss = mse_loss(outputs['hesitation_count'], hesitation_targets)\n",
                "        \n",
                "        loss = (\n",
                "            CONFIG['fluency_weight'] * fluency_loss +\n",
                "            CONFIG['hesitation_weight'] * hesitation_loss\n",
                "        )\n",
                "        \n",
                "        # Backward pass\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
                "        optimizer.step()\n",
                "        \n",
                "        # Metrics\n",
                "        total_loss += loss.item()\n",
                "        fluency_mae += torch.abs(outputs['fluency_score'] - fluency_targets).mean().item()\n",
                "        hesitation_mae += torch.abs(outputs['hesitation_count'] - hesitation_targets).mean().item()\n",
                "    \n",
                "    n_batches = len(dataloader)\n",
                "    return {\n",
                "        'loss': total_loss / n_batches,\n",
                "        'fluency_mae': fluency_mae / n_batches,\n",
                "        'hesitation_mae': hesitation_mae / n_batches\n",
                "    }\n",
                "\n",
                "\n",
                "def validate(model, dataloader, device):\n",
                "    model.eval()\n",
                "    total_loss = 0.0\n",
                "    fluency_mae = 0.0\n",
                "    hesitation_mae = 0.0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in dataloader:\n",
                "            waveforms = batch['waveform'].to(device)\n",
                "            fluency_targets = batch['fluency_score'].to(device)\n",
                "            hesitation_targets = batch['hesitation_count'].to(device)\n",
                "            \n",
                "            outputs = model(waveforms)\n",
                "            \n",
                "            fluency_loss = mse_loss(outputs['fluency_score'], fluency_targets)\n",
                "            hesitation_loss = mse_loss(outputs['hesitation_count'], hesitation_targets)\n",
                "            \n",
                "            loss = (\n",
                "                CONFIG['fluency_weight'] * fluency_loss +\n",
                "                CONFIG['hesitation_weight'] * hesitation_loss\n",
                "            )\n",
                "            \n",
                "            total_loss += loss.item()\n",
                "            fluency_mae += torch.abs(outputs['fluency_score'] - fluency_targets).mean().item()\n",
                "            hesitation_mae += torch.abs(outputs['hesitation_count'] - hesitation_targets).mean().item()\n",
                "    \n",
                "    n_batches = len(dataloader)\n",
                "    return {\n",
                "        'loss': total_loss / n_batches,\n",
                "        'fluency_mae': fluency_mae / n_batches,\n",
                "        'hesitation_mae': hesitation_mae / n_batches\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Main training function\n",
                "def train_model(model, train_loader, val_loader, epochs, device):\n",
                "    best_val_loss = float('inf')\n",
                "    patience_counter = 0\n",
                "    history = {'train_loss': [], 'val_loss': [], 'fluency_mae': [], 'hesitation_mae': []}\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        # Train\n",
                "        train_metrics = train_epoch(model, train_loader, optimizer, device)\n",
                "        \n",
                "        # Validate\n",
                "        val_metrics = validate(model, val_loader, device)\n",
                "        \n",
                "        # Update scheduler\n",
                "        scheduler.step(val_metrics['loss'])\n",
                "        \n",
                "        # Log\n",
                "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
                "        print(f\"  Train Loss: {train_metrics['loss']:.4f}\")\n",
                "        print(f\"  Val Loss: {val_metrics['loss']:.4f}\")\n",
                "        print(f\"  Fluency MAE: {val_metrics['fluency_mae']:.4f}\")\n",
                "        print(f\"  Hesitation MAE: {val_metrics['hesitation_mae']:.4f}\")\n",
                "        \n",
                "        # History\n",
                "        history['train_loss'].append(train_metrics['loss'])\n",
                "        history['val_loss'].append(val_metrics['loss'])\n",
                "        history['fluency_mae'].append(val_metrics['fluency_mae'])\n",
                "        history['hesitation_mae'].append(val_metrics['hesitation_mae'])\n",
                "        \n",
                "        # Save best model\n",
                "        if val_metrics['loss'] < best_val_loss:\n",
                "            best_val_loss = val_metrics['loss']\n",
                "            patience_counter = 0\n",
                "            torch.save({\n",
                "                'epoch': epoch,\n",
                "                'model_state_dict': model.state_dict(),\n",
                "                'optimizer_state_dict': optimizer.state_dict(),\n",
                "                'val_loss': val_metrics['loss']\n",
                "            }, f\"{CONFIG['checkpoint_dir']}/best_model.pth\")\n",
                "            print(f\"  âœ“ Saved best model\")\n",
                "        else:\n",
                "            patience_counter += 1\n",
                "            if patience_counter >= CONFIG['early_stopping_patience']:\n",
                "                print(f\"Early stopping at epoch {epoch+1}\")\n",
                "                break\n",
                "        \n",
                "        print()\n",
                "    \n",
                "    return history\n",
                "\n",
                "# Note: Uncomment to run training\n",
                "# history = train_model(model, train_loader, val_loader, CONFIG['epochs'], device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_model(model, test_loader, device):\n",
                "    \"\"\"Comprehensive evaluation on test set\"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    all_fluency_pred = []\n",
                "    all_fluency_true = []\n",
                "    all_hesitation_pred = []\n",
                "    all_hesitation_true = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in test_loader:\n",
                "            waveforms = batch['waveform'].to(device)\n",
                "            outputs = model(waveforms)\n",
                "            \n",
                "            all_fluency_pred.extend(outputs['fluency_score'].cpu().numpy() * 10)  # Scale back to 0-10\n",
                "            all_fluency_true.extend(batch['fluency_score'].numpy() * 10)\n",
                "            all_hesitation_pred.extend(outputs['hesitation_count'].cpu().numpy())\n",
                "            all_hesitation_true.extend(batch['hesitation_count'].numpy())\n",
                "    \n",
                "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "    from scipy.stats import pearsonr, spearmanr\n",
                "    \n",
                "    fluency_mae = mean_absolute_error(all_fluency_true, all_fluency_pred)\n",
                "    fluency_rmse = np.sqrt(mean_squared_error(all_fluency_true, all_fluency_pred))\n",
                "    fluency_pearson, _ = pearsonr(all_fluency_true, all_fluency_pred)\n",
                "    \n",
                "    hesitation_mae = mean_absolute_error(all_hesitation_true, all_hesitation_pred)\n",
                "    hesitation_rmse = np.sqrt(mean_squared_error(all_hesitation_true, all_hesitation_pred))\n",
                "    \n",
                "    print(\"=\" * 50)\n",
                "    print(\"EVALUATION RESULTS\")\n",
                "    print(\"=\" * 50)\n",
                "    print(f\"\\nFluency Score (0-10):\")\n",
                "    print(f\"  MAE: {fluency_mae:.3f}\")\n",
                "    print(f\"  RMSE: {fluency_rmse:.3f}\")\n",
                "    print(f\"  Pearson r: {fluency_pearson:.3f}\")\n",
                "    print(f\"\\nHesitation Count:\")\n",
                "    print(f\"  MAE: {hesitation_mae:.3f}\")\n",
                "    print(f\"  RMSE: {hesitation_rmse:.3f}\")\n",
                "    \n",
                "    return {\n",
                "        'fluency_mae': fluency_mae,\n",
                "        'fluency_rmse': fluency_rmse,\n",
                "        'fluency_pearson': fluency_pearson,\n",
                "        'hesitation_mae': hesitation_mae,\n",
                "        'hesitation_rmse': hesitation_rmse\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Model Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def export_model(model, output_dir='../models/speech_model'):\n",
                "    \"\"\"Export model for production use\"\"\"\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    \n",
                "    # Save PyTorch model\n",
                "    torch.save(model.state_dict(), f'{output_dir}/speech_fluency_model.pth')\n",
                "    \n",
                "    # Save model config\n",
                "    config = {\n",
                "        'wav2vec2_model': MODEL_NAME,\n",
                "        'hidden_size': 256,\n",
                "        'num_gru_layers': 2,\n",
                "        'sample_rate': 16000,\n",
                "        'max_length_seconds': 30\n",
                "    }\n",
                "    \n",
                "    import json\n",
                "    with open(f'{output_dir}/config.json', 'w') as f:\n",
                "        json.dump(config, f, indent=2)\n",
                "    \n",
                "    print(f'Model exported to: {output_dir}')\n",
                "\n",
                "# export_model(model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Inference Demo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_fluency(model, audio_path, device):\n",
                "    \"\"\"Predict fluency for a single audio file\"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    # Load and preprocess audio\n",
                "    waveform, sr = torchaudio.load(audio_path)\n",
                "    \n",
                "    # Resample to 16kHz\n",
                "    if sr != 16000:\n",
                "        resampler = torchaudio.transforms.Resample(sr, 16000)\n",
                "        waveform = resampler(waveform)\n",
                "    \n",
                "    # Convert to mono\n",
                "    if waveform.shape[0] > 1:\n",
                "        waveform = waveform.mean(dim=0, keepdim=True)\n",
                "    \n",
                "    waveform = waveform.squeeze(0).to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(waveform.unsqueeze(0))\n",
                "        fluency_score = outputs['fluency_score'].item() * 10  # Scale to 0-10\n",
                "        hesitation_count = int(round(outputs['hesitation_count'].item()))\n",
                "    \n",
                "    print(f\"Audio: {audio_path}\")\n",
                "    print(f\"Fluency Score: {fluency_score:.1f}/10\")\n",
                "    print(f\"Estimated Hesitations: {hesitation_count}\")\n",
                "    \n",
                "    return fluency_score, hesitation_count\n",
                "\n",
                "# Demo: predict_fluency(model, 'path/to/audio.wav', device)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}