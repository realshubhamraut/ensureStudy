{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üì∑ Proctoring Feature Extraction\n",
                "\n",
                "Extract features from webcam frames for training proctoring models.\n",
                "\n",
                "**Based on**: AutoOEP/Proctor/feature_extractor.py\n",
                "\n",
                "## What This Notebook Does\n",
                "\n",
                "1. Process video frames (front + side cameras)\n",
                "2. Extract features: face detection, gaze, head pose, prohibited objects\n",
                "3. Save features to CSV for model training\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "- Dataset with labeled frames (cheating/not cheating)\n",
                "- YOLO model weights for object detection\n",
                "- MediaPipe face landmarker task file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from datetime import datetime\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "print(\"‚úÖ Basic imports loaded\")\n",
                "\n",
                "# Check optional dependencies\n",
                "try:\n",
                "    import cv2\n",
                "    print(\"‚úÖ OpenCV available\")\n",
                "except ImportError:\n",
                "    print(\"‚ùå OpenCV not installed: pip install opencv-python\")\n",
                "\n",
                "try:\n",
                "    import mediapipe as mp\n",
                "    print(\"‚úÖ MediaPipe available\")\n",
                "except ImportError:\n",
                "    print(\"‚ùå MediaPipe not installed: pip install mediapipe\")\n",
                "\n",
                "try:\n",
                "    from ultralytics import YOLO\n",
                "    print(\"‚úÖ Ultralytics (YOLO) available\")\n",
                "except ImportError:\n",
                "    print(\"‚ùå YOLO not installed: pip install ultralytics\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Paths - UPDATE THESE\n",
                "BASE_PATH = os.path.dirname(os.getcwd())  # ml/ directory\n",
                "PROJECT_ROOT = os.path.dirname(BASE_PATH)  # ensureStudy/\n",
                "\n",
                "# Dataset path (update to your dataset location)\n",
                "DATASET_PATH = os.path.join(PROJECT_ROOT, 'AutoOEP', 'Dataset_Parser', 'Dataset')\n",
                "\n",
                "# Model paths\n",
                "YOLO_MODEL_PATH = os.path.join(PROJECT_ROOT, 'AutoOEP', 'Models', 'OEP_YOLOv11n.pt')\n",
                "MEDIAPIPE_TASK_PATH = os.path.join(PROJECT_ROOT, 'AutoOEP', 'Models', 'face_landmarker.task')\n",
                "\n",
                "# Target image for face verification\n",
                "TARGET_IMAGE_PATH = None  # Set to student ID photo path\n",
                "\n",
                "# Output\n",
                "OUTPUT_DIR = os.path.join(BASE_PATH, 'data', 'proctoring')\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Dataset path: {DATASET_PATH}\")\n",
                "print(f\"Output dir: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature Columns\n",
                "\n",
                "Features extracted from each frame pair:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "FEATURE_COLUMNS = [\n",
                "    # Timestamp & Metadata\n",
                "    'timestamp',\n",
                "    \n",
                "    # Face Verification\n",
                "    'verification_result',  # 0/1 - does face match registered student\n",
                "    'num_faces',            # Number of faces detected\n",
                "    \n",
                "    # Eye/Gaze Features\n",
                "    'iris_pos',             # center/left/right\n",
                "    'iris_ratio',           # Ratio indicating gaze direction\n",
                "    'gaze_direction',       # forward/left/right/up/down\n",
                "    'gaze_zone',            # white/yellow/red (risk level)\n",
                "    \n",
                "    # Mouth Features\n",
                "    'mouth_zone',           # GREEN/YELLOW/ORANGE/RED\n",
                "    'mouth_area',           # Mouth openness\n",
                "    \n",
                "    # Head Pose\n",
                "    'x_rotation',           # Pitch\n",
                "    'y_rotation',           # Yaw  \n",
                "    'z_rotation',           # Roll\n",
                "    'radial_distance',      # Distance from center\n",
                "    \n",
                "    # Prohibited Objects (1 if detected, 0 if not)\n",
                "    'watch',\n",
                "    'headphone',\n",
                "    'closedbook',\n",
                "    'earpiece',\n",
                "    'cell phone',\n",
                "    'openbook',\n",
                "    'chits',\n",
                "    'sheet',\n",
                "    \n",
                "    # Hand Features\n",
                "    'H-Distance',           # Hand distance from camera\n",
                "    'F-Distance',           # Face distance from camera\n",
                "    \n",
                "    # Labels\n",
                "    'split',                # Train/Test\n",
                "    'video',                # Video/session ID\n",
                "    'is_cheating'           # Target label (0/1)\n",
                "]\n",
                "\n",
                "print(f\"Total features: {len(FEATURE_COLUMNS)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_timestamp(filename):\n",
                "    \"\"\"Extract timestamp from frame filename.\"\"\"\n",
                "    match = re.search(r'_(\\d+[:\\-]\\d+[:\\-]\\d+[\\.\\-]\\d+)\\.jpg$', filename)\n",
                "    return match.group(1) if match else None\n",
                "\n",
                "\n",
                "def get_all_timestamps(video_path):\n",
                "    \"\"\"Get all unique timestamps in a video directory.\"\"\"\n",
                "    timestamps = set()\n",
                "    for folder_type in ['front', 'side']:\n",
                "        for label_type in ['cheating_frames', 'not_cheating_frames']:\n",
                "            directory = os.path.join(video_path, folder_type, label_type)\n",
                "            if os.path.exists(directory):\n",
                "                for file in os.listdir(directory):\n",
                "                    if file.endswith('.jpg'):\n",
                "                        ts = extract_timestamp(file)\n",
                "                        if ts:\n",
                "                            timestamps.add(ts)\n",
                "    return timestamps\n",
                "\n",
                "\n",
                "def find_frame_paths(video_path, timestamp):\n",
                "    \"\"\"Find face and hand frame paths for a timestamp.\"\"\"\n",
                "    face_path, face_label = None, None\n",
                "    hand_path, hand_label = None, None\n",
                "    \n",
                "    for is_cheating, label in [(True, 1), (False, 0)]:\n",
                "        cheating_str = 'cheating_frames' if is_cheating else 'not_cheating_frames'\n",
                "        \n",
                "        # Face frame\n",
                "        if not face_path:\n",
                "            face_dir = os.path.join(video_path, 'front', cheating_str)\n",
                "            if os.path.exists(face_dir):\n",
                "                for file in os.listdir(face_dir):\n",
                "                    if file.endswith('.jpg') and extract_timestamp(file) == timestamp:\n",
                "                        face_path = os.path.join(face_dir, file)\n",
                "                        face_label = label\n",
                "                        break\n",
                "        \n",
                "        # Hand frame\n",
                "        if not hand_path:\n",
                "            hand_dir = os.path.join(video_path, 'side', cheating_str)\n",
                "            if os.path.exists(hand_dir):\n",
                "                for file in os.listdir(hand_dir):\n",
                "                    if file.endswith('.jpg') and extract_timestamp(file) == timestamp:\n",
                "                        hand_path = os.path.join(hand_dir, file)\n",
                "                        hand_label = label\n",
                "                        break\n",
                "    \n",
                "    return face_path, face_label, hand_path, hand_label\n",
                "\n",
                "print(\"‚úÖ Helper functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Simplified Feature Extraction\n",
                "\n",
                "**Note**: For full feature extraction, use the AutoOEP FeatureExtractor.\n",
                "This simplified version demonstrates the workflow."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_basic_features(face_frame, hand_frame):\n",
                "    \"\"\"\n",
                "    Extract basic features from frame pair.\n",
                "    \n",
                "    For production, use AutoOEP/Proctor/feature_extractor.py\n",
                "    which includes YOLO object detection, MediaPipe landmarks, etc.\n",
                "    \"\"\"\n",
                "    features = {\n",
                "        'verification_result': 0,\n",
                "        'num_faces': 1,\n",
                "        'iris_pos': 0,  # center\n",
                "        'iris_ratio': 1.0,\n",
                "        'mouth_zone': 0,  # GREEN\n",
                "        'mouth_area': 0.0,\n",
                "        'x_rotation': 0.0,\n",
                "        'y_rotation': 0.0,\n",
                "        'z_rotation': 0.0,\n",
                "        'radial_distance': 0.0,\n",
                "        'gaze_direction': 0,  # forward\n",
                "        'gaze_zone': 0,  # white\n",
                "    }\n",
                "    \n",
                "    # Prohibited objects (default: none detected)\n",
                "    for obj in ['watch', 'headphone', 'closedbook', 'earpiece', \n",
                "                'cell phone', 'openbook', 'chits', 'sheet']:\n",
                "        features[obj] = 0\n",
                "    \n",
                "    features['H-Distance'] = 10000.0\n",
                "    features['F-Distance'] = 10000.0\n",
                "    \n",
                "    return features\n",
                "\n",
                "print(\"‚úÖ Feature extractor defined\")\n",
                "print(\"\\n‚ö†Ô∏è  Note: This is a simplified extractor.\")\n",
                "print(\"For full features, use: AutoOEP/Proctor/feature_extractor.py\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Process Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_dataset(dataset_path, output_dir):\n",
                "    \"\"\"Process dataset and save features to CSV.\"\"\"\n",
                "    all_results = []\n",
                "    \n",
                "    for split_name in ['Train', 'Test']:\n",
                "        split_path = os.path.join(dataset_path, split_name)\n",
                "        if not os.path.exists(split_path):\n",
                "            print(f\"‚ö†Ô∏è  {split_name} directory not found\")\n",
                "            continue\n",
                "        \n",
                "        print(f\"\\nProcessing {split_name}...\")\n",
                "        video_dirs = sorted([d for d in os.listdir(split_path) \n",
                "                            if os.path.isdir(os.path.join(split_path, d))])\n",
                "        \n",
                "        for video_name in tqdm(video_dirs, desc=split_name):\n",
                "            video_path = os.path.join(split_path, video_name)\n",
                "            timestamps = get_all_timestamps(video_path)\n",
                "            \n",
                "            if not timestamps:\n",
                "                continue\n",
                "            \n",
                "            for timestamp in timestamps:\n",
                "                face_path, face_label, hand_path, hand_label = find_frame_paths(\n",
                "                    video_path, timestamp\n",
                "                )\n",
                "                \n",
                "                if not face_path or not hand_path:\n",
                "                    continue\n",
                "                \n",
                "                # Load frames\n",
                "                face_frame = cv2.imread(face_path)\n",
                "                hand_frame = cv2.imread(hand_path)\n",
                "                \n",
                "                if face_frame is None or hand_frame is None:\n",
                "                    continue\n",
                "                \n",
                "                # Extract features\n",
                "                features = extract_basic_features(face_frame, hand_frame)\n",
                "                \n",
                "                # Add metadata\n",
                "                features['timestamp'] = timestamp\n",
                "                features['split'] = split_name\n",
                "                features['video'] = video_name\n",
                "                features['is_cheating'] = 1 if (face_label == 1 or hand_label == 1) else 0\n",
                "                \n",
                "                all_results.append(features)\n",
                "    \n",
                "    if not all_results:\n",
                "        print(\"‚ùå No features extracted!\")\n",
                "        return None\n",
                "    \n",
                "    # Save to CSV\n",
                "    df = pd.DataFrame(all_results)\n",
                "    output_path = os.path.join(output_dir, 'extracted_features.csv')\n",
                "    df.to_csv(output_path, index=False)\n",
                "    \n",
                "    print(f\"\\n‚úÖ Saved {len(df)} samples to {output_path}\")\n",
                "    print(f\"\\nClass distribution:\")\n",
                "    print(df['is_cheating'].value_counts())\n",
                "    \n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run feature extraction\n",
                "if os.path.exists(DATASET_PATH):\n",
                "    df = process_dataset(DATASET_PATH, OUTPUT_DIR)\n",
                "else:\n",
                "    print(f\"‚ùå Dataset not found at: {DATASET_PATH}\")\n",
                "    print(\"\\nPlease update DATASET_PATH to your dataset location.\")\n",
                "    print(\"\\nExpected structure:\")\n",
                "    print(\"  Dataset/\")\n",
                "    print(\"  ‚îú‚îÄ‚îÄ Train/\")\n",
                "    print(\"  ‚îÇ   ‚îî‚îÄ‚îÄ video_001/\")\n",
                "    print(\"  ‚îÇ       ‚îú‚îÄ‚îÄ front/\")\n",
                "    print(\"  ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ cheating_frames/\")\n",
                "    print(\"  ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ not_cheating_frames/\")\n",
                "    print(\"  ‚îÇ       ‚îî‚îÄ‚îÄ side/\")\n",
                "    print(\"  ‚îî‚îÄ‚îÄ Test/\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Using AutoOEP Feature Extractor (Recommended)\n",
                "\n",
                "For full feature extraction with YOLO and MediaPipe:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Full feature extraction using AutoOEP (requires models)\n",
                "\n",
                "USE_AUTOOEP = False  # Set to True if you have the models\n",
                "\n",
                "if USE_AUTOOEP:\n",
                "    import sys\n",
                "    sys.path.insert(0, os.path.join(PROJECT_ROOT, 'AutoOEP'))\n",
                "    \n",
                "    from Proctor.feature_extractor import process_dataset_and_save_csv\n",
                "    \n",
                "    process_dataset_and_save_csv(\n",
                "        dataset_path=DATASET_PATH,\n",
                "        target_frame_path=TARGET_IMAGE_PATH,\n",
                "        output_dir=OUTPUT_DIR,\n",
                "        face_landmarker_path=MEDIAPIPE_TASK_PATH,\n",
                "        yolo_model_path=YOLO_MODEL_PATH\n",
                "    )\n",
                "else:\n",
                "    print(\"Skipping AutoOEP extraction.\")\n",
                "    print(\"Set USE_AUTOOEP = True after setting up:\")\n",
                "    print(\"  - YOLO model: OEP_YOLOv11n.pt\")\n",
                "    print(\"  - MediaPipe: face_landmarker.task\")\n",
                "    print(\"  - Target image for face verification\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}