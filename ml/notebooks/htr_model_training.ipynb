{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ✍️ Handwriting Text Recognition (HTR) Model\n",
                "\n",
                "This notebook covers:\n",
                "1. **Architecture**: CNN + BiLSTM + CTC Loss\n",
                "2. **Datasets**: IAM, EMNIST integration\n",
                "3. **Training**: PyTorch implementation\n",
                "4. **Inference**: Text extraction from images\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies if needed\n",
                "# !pip install torch torchvision torchaudio\n",
                "# !pip install matplotlib numpy pillow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "PyTorch version: 2.9.1\n",
                        "Device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "from pathlib import Path\n",
                "from typing import List, Tuple, Dict, Optional\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Check device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Character Set & Encoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Vocabulary size: 87\n",
                        "Encode 'F = ma': [32, 63, 73, 63, 13, 1]\n",
                        "Decode [6, 0, 62, 0, 13, 1]: f9ma\n"
                    ]
                }
            ],
            "source": [
                "class CharacterSet:\n",
                "    \"\"\"\n",
                "    Character vocabulary for HTR.\n",
                "    \n",
                "    Includes:\n",
                "    - Lowercase letters (a-z)\n",
                "    - Uppercase letters (A-Z)\n",
                "    - Digits (0-9)\n",
                "    - Common punctuation and math symbols\n",
                "    - Special tokens: <blank>, <unk>\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        # Character vocabulary\n",
                "        self.chars = (\n",
                "            'abcdefghijklmnopqrstuvwxyz'\n",
                "            'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
                "            '0123456789'\n",
                "            ' .,:;!?\\'-+=*/()[]{}@#$%&'\n",
                "        )\n",
                "        \n",
                "        # Special tokens\n",
                "        self.blank_token = '<blank>'  # CTC blank\n",
                "        self.unk_token = '<unk>'      # Unknown character\n",
                "        \n",
                "        # Create mappings\n",
                "        self.char_to_idx = {c: i + 1 for i, c in enumerate(self.chars)}\n",
                "        self.char_to_idx[self.blank_token] = 0\n",
                "        self.idx_to_char = {v: k for k, v in self.char_to_idx.items()}\n",
                "        \n",
                "        self.vocab_size = len(self.char_to_idx)\n",
                "    \n",
                "    def encode(self, text: str) -> List[int]:\n",
                "        \"\"\"Convert text to index sequence.\"\"\"\n",
                "        return [self.char_to_idx.get(c, self.char_to_idx.get(self.unk_token, 0)) for c in text]\n",
                "    \n",
                "    def decode(self, indices: List[int], remove_blanks: bool = True) -> str:\n",
                "        \"\"\"Convert index sequence to text.\"\"\"\n",
                "        if remove_blanks:\n",
                "            # CTC decoding: remove consecutive duplicates and blanks\n",
                "            prev = None\n",
                "            chars = []\n",
                "            for idx in indices:\n",
                "                if idx != prev and idx != 0:  # Not blank and not duplicate\n",
                "                    chars.append(self.idx_to_char.get(idx, ''))\n",
                "                prev = idx\n",
                "            return ''.join(chars)\n",
                "        else:\n",
                "            return ''.join(self.idx_to_char.get(idx, '') for idx in indices)\n",
                "\n",
                "\n",
                "# Test\n",
                "charset = CharacterSet()\n",
                "print(f\"Vocabulary size: {charset.vocab_size}\")\n",
                "print(f\"Encode 'F = ma': {charset.encode('F = ma')}\")\n",
                "print(f\"Decode [6, 0, 62, 0, 13, 1]: {charset.decode([6, 0, 62, 0, 13, 1])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. HTR Model Architecture\n",
                "\n",
                "**CNN + BiLSTM + CTC**\n",
                "\n",
                "```\n",
                "Image → CNN (feature extraction) → BiLSTM (sequence modeling) → Linear → CTC Loss\n",
                "```\n",
                "\n",
                "| Component | Purpose |\n",
                "|-----------|--------|\n",
                "| CNN | Extract visual features from image regions |\n",
                "| BiLSTM | Model left-to-right and right-to-left context |\n",
                "| CTC | Alignment-free sequence loss |\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Model architecture:\n",
                        "  Vocab size: 87\n",
                        "  Parameters: 4,851,607\n"
                    ]
                },
                {
                    "ename": "RuntimeError",
                    "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 141\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Test forward pass\u001b[39;00m\n\u001b[32m    140\u001b[39m test_input = torch.randn(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m256\u001b[39m)  \u001b[38;5;66;03m# (batch=2, channels=1, H=64, W=256)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m test_output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTest:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    143\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Input shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_input.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ensureStudy/venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/projects/ensureStudy/venv/lib/python3.14/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mHTRModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    119\u001b[39m batch_size = features.size(\u001b[32m0\u001b[39m)\n\u001b[32m    120\u001b[39m features = features.squeeze(\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# (batch, 512, W)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m features = \u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (W, batch, 512)\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# BiLSTM: (seq, batch, hidden*2)\u001b[39;00m\n\u001b[32m    124\u001b[39m lstm_out, _ = \u001b[38;5;28mself\u001b[39m.lstm(features)\n",
                        "\u001b[31mRuntimeError\u001b[39m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3"
                    ]
                }
            ],
            "source": [
                "class CNNBackbone(nn.Module):\n",
                "    \"\"\"\n",
                "    CNN feature extractor for HTR.\n",
                "    \n",
                "    Architecture:\n",
                "    - 7 Conv layers with batch norm and ReLU\n",
                "    - Max pooling to reduce spatial dimensions\n",
                "    - Output: (batch, channels, 1, width)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, input_channels: int = 1):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Layer specifications: (out_channels, kernel_size, stride, padding)\n",
                "        self.cnn = nn.Sequential(\n",
                "            # Layer 1: 32 -> 64x32\n",
                "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
                "            nn.BatchNorm2d(32),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2),  # H/2, W/2\n",
                "            \n",
                "            # Layer 2: 64 -> 32x16\n",
                "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
                "            nn.BatchNorm2d(64),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=2, stride=2),  # H/4, W/4\n",
                "            \n",
                "            # Layer 3: 128\n",
                "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
                "            nn.BatchNorm2d(128),\n",
                "            nn.ReLU(inplace=True),\n",
                "            \n",
                "            # Layer 4: 128 -> 16x8\n",
                "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
                "            nn.BatchNorm2d(128),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=(2, 1)),  # H/8, W/4\n",
                "            \n",
                "            # Layer 5: 256\n",
                "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
                "            nn.BatchNorm2d(256),\n",
                "            nn.ReLU(inplace=True),\n",
                "            \n",
                "            # Layer 6: 256 -> 8x4\n",
                "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
                "            nn.BatchNorm2d(256),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.MaxPool2d(kernel_size=(2, 1)),  # H/16, W/4\n",
                "            \n",
                "            # Layer 7: 512\n",
                "            nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=0),\n",
                "            nn.BatchNorm2d(512),\n",
                "            nn.ReLU(inplace=True),\n",
                "        )\n",
                "    \n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        return self.cnn(x)\n",
                "\n",
                "\n",
                "class HTRModel(nn.Module):\n",
                "    \"\"\"\n",
                "    Complete HTR Model: CNN + BiLSTM + CTC\n",
                "    \n",
                "    Input: (batch, 1, height, width) grayscale image\n",
                "    Output: (seq_len, batch, vocab_size) log probabilities\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        vocab_size: int,\n",
                "        input_height: int = 64,\n",
                "        hidden_size: int = 256,\n",
                "        num_lstm_layers: int = 2,\n",
                "        dropout: float = 0.3\n",
                "    ):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.vocab_size = vocab_size\n",
                "        self.hidden_size = hidden_size\n",
                "        \n",
                "        # CNN backbone\n",
                "        self.cnn = CNNBackbone(input_channels=1)\n",
                "        \n",
                "        # Calculate CNN output size\n",
                "        # After CNN: height becomes ~1, width becomes W/4\n",
                "        # Features per time step: 512 channels * remaining_height\n",
                "        self.cnn_output_size = 512  # Channels from last conv\n",
                "        \n",
                "        # BiLSTM\n",
                "        self.lstm = nn.LSTM(\n",
                "            input_size=self.cnn_output_size,\n",
                "            hidden_size=hidden_size,\n",
                "            num_layers=num_lstm_layers,\n",
                "            batch_first=False,\n",
                "            bidirectional=True,\n",
                "            dropout=dropout if num_lstm_layers > 1 else 0\n",
                "        )\n",
                "        \n",
                "        # Output projection\n",
                "        self.fc = nn.Linear(hidden_size * 2, vocab_size)  # *2 for bidirectional\n",
                "        \n",
                "        # Log softmax for CTC\n",
                "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
                "    \n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass.\n",
                "        \n",
                "        Args:\n",
                "            x: (batch, 1, H, W) input images\n",
                "            \n",
                "        Returns:\n",
                "            (seq_len, batch, vocab_size) log probabilities\n",
                "        \"\"\"\n",
                "        # CNN features: (batch, 512, H, W)\n",
                "        features = self.cnn(x)\n",
                "        \n",
                "        # Reshape for LSTM: (batch, channels, H, W) -> (W, batch, channels)\n",
                "        # Collapse the height dimension (H) robustly (works whether H==1 or >1)\n",
                "        batch_size = features.size(0)\n",
                "        features = features.mean(dim=2)  # (batch, 512, W)\n",
                "        features = features.permute(2, 0, 1)  # (W, batch, 512)\n",
                "        \n",
                "        # BiLSTM: (seq, batch, hidden*2)\n",
                "        lstm_out, _ = self.lstm(features)\n",
                "        \n",
                "        # Projection: (seq, batch, vocab)\n",
                "        output = self.fc(lstm_out)\n",
                "        \n",
                "        # Log softmax\n",
                "        return self.log_softmax(output)\n",
                "\n",
                "\n",
                "# Test model\n",
                "model = HTRModel(vocab_size=charset.vocab_size)\n",
                "print(f\"\\nModel architecture:\")\n",
                "print(f\"  Vocab size: {charset.vocab_size}\")\n",
                "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "\n",
                "# Test forward pass\n",
                "test_input = torch.randn(2, 1, 64, 256)  # (batch=2, channels=1, H=64, W=256)\n",
                "test_output = model(test_input)\n",
                "print(f\"\\nTest:\")\n",
                "print(f\"  Input shape: {test_input.shape}\")\n",
                "print(f\"  Output shape: {test_output.shape}  # (seq_len, batch, vocab)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Synthetic Dataset for Training\n",
                "\n",
                "For initial development, we create synthetic handwriting-like data using PIL/OpenCV."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "import cv2\n",
                "from PIL import Image, ImageDraw, ImageFont\n",
                "\n",
                "class SyntheticHTRDataset(Dataset):\n",
                "    \"\"\"\n",
                "    Generate synthetic handwriting-like images for training.\n",
                "    \n",
                "    Uses PIL to render text with various fonts and augmentations.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        charset: CharacterSet,\n",
                "        num_samples: int = 10000,\n",
                "        img_height: int = 64,\n",
                "        img_width: int = 256,\n",
                "        max_text_len: int = 20\n",
                "    ):\n",
                "        self.charset = charset\n",
                "        self.num_samples = num_samples\n",
                "        self.img_height = img_height\n",
                "        self.img_width = img_width\n",
                "        self.max_text_len = max_text_len\n",
                "        \n",
                "        # Sample texts (formulas, words, numbers)\n",
                "        self.sample_texts = [\n",
                "            \"F = ma\", \"E = mc2\", \"a + b = c\", \"x = 5\",\n",
                "            \"Answer\", \"Calculate\", \"Solve\", \"Find x\",\n",
                "            \"Step 1\", \"Step 2\", \"Therefore\", \"Hence\",\n",
                "            \"123\", \"456\", \"789\", \"100\",\n",
                "            \"y = mx + b\", \"Area = lw\", \"V = IR\",\n",
                "            \"sin x\", \"cos x\", \"tan x\", \"log x\",\n",
                "        ]\n",
                "    \n",
                "    def __len__(self):\n",
                "        return self.num_samples\n",
                "    \n",
                "    def generate_text(self) -> str:\n",
                "        \"\"\"Generate random text sample.\"\"\"\n",
                "        if random.random() < 0.5:\n",
                "            # Use predefined sample\n",
                "            return random.choice(self.sample_texts)\n",
                "        else:\n",
                "            # Generate random string\n",
                "            length = random.randint(3, self.max_text_len)\n",
                "            chars = [random.choice(self.charset.chars) for _ in range(length)]\n",
                "            return ''.join(chars)\n",
                "    \n",
                "    def render_text(self, text: str) -> np.ndarray:\n",
                "        \"\"\"Render text to image with augmentations.\"\"\"\n",
                "        # Create blank image\n",
                "        img = Image.new('L', (self.img_width, self.img_height), color=255)\n",
                "        draw = ImageDraw.Draw(img)\n",
                "        \n",
                "        # Use default font (for simplicity - in production use handwriting fonts)\n",
                "        try:\n",
                "            font_size = random.randint(20, 36)\n",
                "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", font_size)\n",
                "        except:\n",
                "            font = ImageFont.load_default()\n",
                "        \n",
                "        # Calculate text position\n",
                "        bbox = draw.textbbox((0, 0), text, font=font)\n",
                "        text_width = bbox[2] - bbox[0]\n",
                "        text_height = bbox[3] - bbox[1]\n",
                "        \n",
                "        x = random.randint(5, max(5, self.img_width - text_width - 5))\n",
                "        y = (self.img_height - text_height) // 2 + random.randint(-5, 5)\n",
                "        \n",
                "        # Draw text\n",
                "        draw.text((x, y), text, font=font, fill=random.randint(0, 50))\n",
                "        \n",
                "        # Convert to numpy\n",
                "        img_array = np.array(img)\n",
                "        \n",
                "        # Add augmentations\n",
                "        img_array = self.augment(img_array)\n",
                "        \n",
                "        return img_array\n",
                "    \n",
                "    def augment(self, img: np.ndarray) -> np.ndarray:\n",
                "        \"\"\"Apply random augmentations.\"\"\"\n",
                "        # Random noise\n",
                "        if random.random() < 0.3:\n",
                "            noise = np.random.normal(0, 10, img.shape).astype(np.int16)\n",
                "            img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
                "        \n",
                "        # Random blur\n",
                "        if random.random() < 0.2:\n",
                "            kernel_size = random.choice([3, 5])\n",
                "            img = cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
                "        \n",
                "        # Random erosion/dilation\n",
                "        if random.random() < 0.2:\n",
                "            kernel = np.ones((2, 2), np.uint8)\n",
                "            if random.random() < 0.5:\n",
                "                img = cv2.erode(img, kernel, iterations=1)\n",
                "            else:\n",
                "                img = cv2.dilate(img, kernel, iterations=1)\n",
                "        \n",
                "        return img\n",
                "    \n",
                "    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
                "        \"\"\"Get a sample.\"\"\"\n",
                "        text = self.generate_text()\n",
                "        img = self.render_text(text)\n",
                "        \n",
                "        # Normalize image\n",
                "        img = img.astype(np.float32) / 255.0\n",
                "        \n",
                "        # Add channel dimension\n",
                "        img = torch.FloatTensor(img).unsqueeze(0)  # (1, H, W)\n",
                "        \n",
                "        # Encode text\n",
                "        label = torch.LongTensor(self.charset.encode(text))\n",
                "        label_length = len(label)\n",
                "        \n",
                "        return img, label, label_length\n",
                "\n",
                "\n",
                "# Test dataset\n",
                "dataset = SyntheticHTRDataset(charset, num_samples=100)\n",
                "print(f\"Dataset size: {len(dataset)}\")\n",
                "\n",
                "# Visualize samples\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
                "for ax in axes.flatten():\n",
                "    idx = random.randint(0, len(dataset) - 1)\n",
                "    img, label, length = dataset[idx]\n",
                "    text = charset.decode(label.tolist(), remove_blanks=False)\n",
                "    ax.imshow(img.squeeze(), cmap='gray')\n",
                "    ax.set_title(f'\"{text}\"')\n",
                "    ax.axis('off')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Training Loop with CTC Loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def collate_fn(batch):\n",
                "    \"\"\"\n",
                "    Custom collate function for variable-length labels.\n",
                "    \"\"\"\n",
                "    images = torch.stack([item[0] for item in batch])\n",
                "    labels = [item[1] for item in batch]\n",
                "    label_lengths = torch.LongTensor([item[2] for item in batch])\n",
                "    \n",
                "    # Pad labels to max length\n",
                "    max_len = max(len(l) for l in labels)\n",
                "    padded_labels = torch.zeros(len(batch), max_len, dtype=torch.long)\n",
                "    for i, label in enumerate(labels):\n",
                "        padded_labels[i, :len(label)] = label\n",
                "    \n",
                "    return images, padded_labels, label_lengths\n",
                "\n",
                "\n",
                "def train_htr_model(\n",
                "    model: nn.Module,\n",
                "    train_dataset: Dataset,\n",
                "    epochs: int = 10,\n",
                "    batch_size: int = 32,\n",
                "    learning_rate: float = 0.001,\n",
                "    device: str = 'cpu'\n",
                "):\n",
                "    \"\"\"\n",
                "    Train HTR model with CTC loss.\n",
                "    \"\"\"\n",
                "    model = model.to(device)\n",
                "    model.train()\n",
                "    \n",
                "    # DataLoader\n",
                "    train_loader = DataLoader(\n",
                "        train_dataset,\n",
                "        batch_size=batch_size,\n",
                "        shuffle=True,\n",
                "        collate_fn=collate_fn,\n",
                "        num_workers=0\n",
                "    )\n",
                "    \n",
                "    # Loss and optimizer\n",
                "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
                "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
                "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
                "    \n",
                "    history = {'loss': []}\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        epoch_loss = 0.0\n",
                "        num_batches = 0\n",
                "        \n",
                "        for images, labels, label_lengths in train_loader:\n",
                "            images = images.to(device)\n",
                "            labels = labels.to(device)\n",
                "            \n",
                "            optimizer.zero_grad()\n",
                "            \n",
                "            # Forward pass: (seq_len, batch, vocab)\n",
                "            outputs = model(images)\n",
                "            \n",
                "            # CTC loss\n",
                "            # input_lengths: all sequences have same length (width of feature map)\n",
                "            input_lengths = torch.full(\n",
                "                (images.size(0),), outputs.size(0), dtype=torch.long, device=device\n",
                "            )\n",
                "            \n",
                "            # Flatten labels for CTC\n",
                "            labels_flat = labels.view(-1)\n",
                "            \n",
                "            loss = criterion(outputs, labels, input_lengths, label_lengths)\n",
                "            \n",
                "            # Backward pass\n",
                "            loss.backward()\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
                "            optimizer.step()\n",
                "            \n",
                "            epoch_loss += loss.item()\n",
                "            num_batches += 1\n",
                "        \n",
                "        avg_loss = epoch_loss / num_batches\n",
                "        history['loss'].append(avg_loss)\n",
                "        scheduler.step()\n",
                "        \n",
                "        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {avg_loss:.4f}\")\n",
                "    \n",
                "    return history\n",
                "\n",
                "\n",
                "# Quick training demo\n",
                "print(\"Training HTR model (demo with small dataset)...\")\n",
                "small_dataset = SyntheticHTRDataset(charset, num_samples=200)\n",
                "model = HTRModel(vocab_size=charset.vocab_size)\n",
                "history = train_htr_model(model, small_dataset, epochs=3, batch_size=16)\n",
                "\n",
                "# Plot loss\n",
                "plt.figure(figsize=(8, 4))\n",
                "plt.plot(history['loss'])\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('CTC Loss')\n",
                "plt.title('Training Loss')\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Inference: Image to Text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def recognize_text(model: nn.Module, image: np.ndarray, charset: CharacterSet) -> str:\n",
                "    \"\"\"\n",
                "    Recognize text from image.\n",
                "    \n",
                "    Args:\n",
                "        model: Trained HTR model\n",
                "        image: Grayscale image (H, W) or (1, H, W)\n",
                "        charset: Character set for decoding\n",
                "        \n",
                "    Returns:\n",
                "        Recognized text string\n",
                "    \"\"\"\n",
                "    model.eval()\n",
                "    \n",
                "    # Preprocess\n",
                "    if len(image.shape) == 2:\n",
                "        image = image[np.newaxis, ...]  # Add channel dim\n",
                "    if image.shape[0] != 1:\n",
                "        image = image[np.newaxis, ...]  # Add batch dim\n",
                "    \n",
                "    # Normalize\n",
                "    if image.max() > 1:\n",
                "        image = image.astype(np.float32) / 255.0\n",
                "    \n",
                "    # To tensor\n",
                "    image_tensor = torch.FloatTensor(image)\n",
                "    if image_tensor.dim() == 3:\n",
                "        image_tensor = image_tensor.unsqueeze(0)  # Add batch dim\n",
                "    \n",
                "    # Forward pass\n",
                "    with torch.no_grad():\n",
                "        outputs = model(image_tensor)  # (seq, batch, vocab)\n",
                "    \n",
                "    # Argmax decoding (greedy)\n",
                "    _, predictions = outputs.max(2)  # (seq, batch)\n",
                "    predictions = predictions.squeeze(1).tolist()  # (seq,)\n",
                "    \n",
                "    # Decode\n",
                "    text = charset.decode(predictions, remove_blanks=True)\n",
                "    \n",
                "    return text\n",
                "\n",
                "\n",
                "# Test inference\n",
                "print(\"Testing inference...\")\n",
                "test_img, test_label, _ = small_dataset[0]\n",
                "test_img_np = test_img.squeeze().numpy()\n",
                "true_text = charset.decode(test_label.tolist(), remove_blanks=False)\n",
                "\n",
                "predicted_text = recognize_text(model, test_img_np, charset)\n",
                "\n",
                "plt.figure(figsize=(10, 3))\n",
                "plt.imshow(test_img_np, cmap='gray')\n",
                "plt.title(f'True: \"{true_text}\"\\nPredicted: \"{predicted_text}\"')\n",
                "plt.axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Export Model\n",
                "\n",
                "Export the model code for production use."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model weights\n",
                "model_path = Path(\"../models/htr_model.pth\")\n",
                "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "torch.save(model.state_dict(), model_path)\n",
                "print(f\"✅ Model saved to: {model_path.resolve()}\")\n",
                "\n",
                "# Export model class to Python file\n",
                "HTR_MODEL_CODE = '''\n",
                "\"\"\"\n",
                "HTR Model: CNN + BiLSTM + CTC\n",
                "\n",
                "Handwriting Text Recognition model for answer sheet evaluation.\n",
                "\"\"\"\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "from typing import List\n",
                "\n",
                "\n",
                "class CharacterSet:\n",
                "    \"\"\"Character vocabulary for HTR.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.chars = (\n",
                "            \\'abcdefghijklmnopqrstuvwxyz\\'\n",
                "            \\'ABCDEFGHIJKLMNOPQRSTUVWXYZ\\'\n",
                "            \\'0123456789\\'\n",
                "            \\' .,:;!?\\\\\\'\\'-+=*/()[]{}@#$%&\\'\n",
                "        )\n",
                "        self.blank_token = \\'<blank>\\'\n",
                "        self.char_to_idx = {c: i + 1 for i, c in enumerate(self.chars)}\n",
                "        self.char_to_idx[self.blank_token] = 0\n",
                "        self.idx_to_char = {v: k for k, v in self.char_to_idx.items()}\n",
                "        self.vocab_size = len(self.char_to_idx)\n",
                "    \n",
                "    def encode(self, text: str) -> List[int]:\n",
                "        return [self.char_to_idx.get(c, 0) for c in text]\n",
                "    \n",
                "    def decode(self, indices: List[int], remove_blanks: bool = True) -> str:\n",
                "        if remove_blanks:\n",
                "            prev = None\n",
                "            chars = []\n",
                "            for idx in indices:\n",
                "                if idx != prev and idx != 0:\n",
                "                    chars.append(self.idx_to_char.get(idx, \\'\\'))\n",
                "                prev = idx\n",
                "            return \\'\\'.join(chars)\n",
                "        return \\'\\'.join(self.idx_to_char.get(idx, \\'\\') for idx in indices)\n",
                "\n",
                "\n",
                "class CNNBackbone(nn.Module):\n",
                "    \"\"\"CNN feature extractor.\"\"\"\n",
                "    \n",
                "    def __init__(self, input_channels: int = 1):\n",
                "        super().__init__()\n",
                "        self.cnn = nn.Sequential(\n",
                "            nn.Conv2d(input_channels, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
                "            nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
                "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(),\n",
                "            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d((2, 1)),\n",
                "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
                "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d((2, 1)),\n",
                "            nn.Conv2d(256, 512, 2, 1, 0), nn.BatchNorm2d(512), nn.ReLU(),\n",
                "        )\n",
                "    \n",
                "    def forward(self, x): return self.cnn(x)\n",
                "\n",
                "\n",
                "class HTRModel(nn.Module):\n",
                "    \"\"\"CNN + BiLSTM + CTC model.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size: int, hidden_size: int = 256, num_layers: int = 2):\n",
                "        super().__init__()\n",
                "        self.cnn = CNNBackbone()\n",
                "        self.lstm = nn.LSTM(512, hidden_size, num_layers, batch_first=False, bidirectional=True)\n",
                "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
                "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        features = self.cnn(x).squeeze(2).permute(2, 0, 1)\n",
                "        lstm_out, _ = self.lstm(features)\n",
                "        return self.log_softmax(self.fc(lstm_out))\n",
                "\n",
                "\n",
                "def recognize_text(model: nn.Module, image: np.ndarray, charset: CharacterSet) -> str:\n",
                "    \"\"\"Recognize text from image.\"\"\"\n",
                "    model.eval()\n",
                "    if len(image.shape) == 2:\n",
                "        image = image[np.newaxis, np.newaxis, ...]\n",
                "    elif len(image.shape) == 3:\n",
                "        image = image[np.newaxis, ...]\n",
                "    \n",
                "    if image.max() > 1:\n",
                "        image = image.astype(np.float32) / 255.0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(torch.FloatTensor(image))\n",
                "    \n",
                "    _, preds = outputs.max(2)\n",
                "    return charset.decode(preds.squeeze(1).tolist())\n",
                "'''\n",
                "\n",
                "model_code_path = Path(\"../models/htr_model.py\")\n",
                "model_code_path.write_text(HTR_MODEL_CODE)\n",
                "print(f\"✅ Model code exported to: {model_code_path.resolve()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "This notebook implemented:\n",
                "\n",
                "| Component | Description |\n",
                "|-----------|-------------|\n",
                "| `CharacterSet` | Char-index mapping for 80+ characters |\n",
                "| `CNNBackbone` | 7-layer CNN for visual features |\n",
                "| `HTRModel` | CNN + BiLSTM + CTC architecture |\n",
                "| `SyntheticHTRDataset` | Generate training data |\n",
                "| `train_htr_model()` | Training loop with CTC loss |\n",
                "| `recognize_text()` | Inference function |\n",
                "\n",
                "**Next steps:**\n",
                "1. Train on real IAM dataset\n",
                "2. Add beam search decoding\n",
                "3. Integrate with evaluation pipeline\n",
                "\n",
                "**Next notebook:** Answer Scoring"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
