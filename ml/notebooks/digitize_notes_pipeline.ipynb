{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìù Digitize Notes Pipeline - Complete Processing Notebook\n",
                "\n",
                "This notebook demonstrates the full end-to-end pipeline for digitizing handwritten and printed notes.\n",
                "\n",
                "## Pipeline Steps:\n",
                "1. **Image Loading & Preprocessing** - Load, detect document corners, perspective correct\n",
                "2. **Text Detection & Layout Analysis** - Find text regions, lines, paragraphs\n",
                "3. **OCR/HTR Recognition** - Extract text with bounding boxes and confidence\n",
                "4. **Text Chunking** - Split into semantic chunks for embedding\n",
                "5. **Embedding Generation** - Create vector embeddings for search\n",
                "6. **Storage & Indexing** - Store in vector database\n",
                "\n",
                "## Prerequisites:\n",
                "```bash\n",
                "pip install opencv-python numpy pillow pytesseract transformers sentence-transformers\n",
                "pip install pdf2image qdrant-client easyocr\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard imports\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Add project root to path\n",
                "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
                "sys.path.insert(0, project_root)\n",
                "sys.path.insert(0, os.path.join(project_root, 'backend/ai-service'))\n",
                "\n",
                "import cv2\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import json\n",
                "from dataclasses import dataclass, asdict\n",
                "from typing import List, Dict, Any, Tuple, Optional\n",
                "import uuid\n",
                "import time\n",
                "\n",
                "# Set display options\n",
                "%matplotlib inline\n",
                "plt.rcParams['figure.figsize'] = [15, 10]\n",
                "\n",
                "print(f\"‚úÖ Project root: {project_root}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1Ô∏è‚É£ Image Loading & Preprocessing\n",
                "\n",
                "Load a sample image and apply our enhancement pipeline:\n",
                "- Document corner detection\n",
                "- Perspective correction\n",
                "- Deskewing\n",
                "- Noise reduction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import our image enhancer\n",
                "from app.services.image_enhancer import ImageEnhancer, A4_SIZE\n",
                "\n",
                "# Initialize enhancer\n",
                "enhancer = ImageEnhancer(\n",
                "    output_size=A4_SIZE,\n",
                "    target_brightness=200.0,\n",
                "    denoise_strength=8,\n",
                "    sharpen_strength=1.2\n",
                ")\n",
                "\n",
                "print(\"‚úÖ ImageEnhancer initialized\")\n",
                "print(f\"   Output size: {A4_SIZE[0]}x{A4_SIZE[1]} (300 DPI A4)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load a sample image (update path to your test image)\n",
                "# You can use any image of handwritten notes\n",
                "sample_image_path = \"../sample_notes.jpg\"  # UPDATE THIS PATH\n",
                "\n",
                "# Create a synthetic test image if no sample exists\n",
                "if not os.path.exists(sample_image_path):\n",
                "    print(\"‚ö†Ô∏è No sample image found. Creating synthetic test image...\")\n",
                "    \n",
                "    # Create a white page with some text-like lines\n",
                "    test_img = np.ones((800, 600, 3), dtype=np.uint8) * 255\n",
                "    \n",
                "    # Add some \"handwritten\" lines (dark strokes)\n",
                "    for y in range(100, 700, 50):\n",
                "        x_start = 50 + np.random.randint(-10, 10)\n",
                "        x_end = 550 + np.random.randint(-30, 30)\n",
                "        thickness = np.random.randint(1, 3)\n",
                "        cv2.line(test_img, (x_start, y), (x_end, y), (30, 30, 30), thickness)\n",
                "        \n",
                "        # Add some variation (simulate handwriting)\n",
                "        for _ in range(3):\n",
                "            cx = np.random.randint(x_start, x_end)\n",
                "            cy = y + np.random.randint(-5, 5)\n",
                "            cv2.circle(test_img, (cx, cy), 2, (30, 30, 30), -1)\n",
                "    \n",
                "    # Add some perspective distortion\n",
                "    h, w = test_img.shape[:2]\n",
                "    pts1 = np.float32([[0, 0], [w, 0], [0, h], [w, h]])\n",
                "    pts2 = np.float32([[10, 20], [w-30, 10], [20, h-10], [w-10, h-20]])\n",
                "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
                "    test_img = cv2.warpPerspective(test_img, M, (w, h))\n",
                "    \n",
                "    image = test_img\n",
                "    print(\"‚úÖ Created synthetic test image\")\n",
                "else:\n",
                "    image = cv2.imread(sample_image_path)\n",
                "    print(f\"‚úÖ Loaded image from: {sample_image_path}\")\n",
                "\n",
                "print(f\"   Shape: {image.shape}\")\n",
                "\n",
                "# Display original\n",
                "plt.figure(figsize=(10, 8))\n",
                "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
                "plt.title(\"Original Image\")\n",
                "plt.axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply full enhancement pipeline\n",
                "result = enhancer.enhance(\n",
                "    image,\n",
                "    crop_document=True,\n",
                "    apply_binarization=False  # Set True for pure B/W\n",
                ")\n",
                "\n",
                "print(\"üìä Enhancement Results:\")\n",
                "print(f\"   Corners detected: {result.corners_detected}\")\n",
                "print(f\"   Rotation angle: {result.rotation_angle:.2f}¬∞\")\n",
                "print(f\"\\n   Metrics BEFORE:\")\n",
                "print(f\"     Brightness: {result.metrics_before.brightness:.1f}\")\n",
                "print(f\"     Contrast: {result.metrics_before.contrast:.1f}\")\n",
                "print(f\"     Sharpness: {result.metrics_before.sharpness:.1f}\")\n",
                "print(f\"\\n   Metrics AFTER:\")\n",
                "print(f\"     Brightness: {result.metrics_after.brightness:.1f}\")\n",
                "print(f\"     Contrast: {result.metrics_after.contrast:.1f}\")\n",
                "print(f\"     Sharpness: {result.metrics_after.sharpness:.1f}\")\n",
                "\n",
                "# Display comparison\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
                "axes[0].imshow(cv2.cvtColor(result.original, cv2.COLOR_BGR2RGB))\n",
                "axes[0].set_title(\"Original\")\n",
                "axes[0].axis('off')\n",
                "\n",
                "axes[1].imshow(cv2.cvtColor(result.enhanced, cv2.COLOR_BGR2RGB))\n",
                "axes[1].set_title(\"Enhanced\")\n",
                "axes[1].axis('off')\n",
                "\n",
                "axes[2].imshow(cv2.cvtColor(result.thumbnail, cv2.COLOR_BGR2RGB))\n",
                "axes[2].set_title(\"Thumbnail\")\n",
                "axes[2].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2Ô∏è‚É£ Text Detection & Layout Analysis\n",
                "\n",
                "Detect text regions and extract line-level bounding boxes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class TextLine:\n",
                "    \"\"\"Detected text line with bounding box\"\"\"\n",
                "    bbox: Tuple[int, int, int, int]  # x, y, w, h\n",
                "    confidence: float = 0.0\n",
                "    text: str = \"\"\n",
                "    line_index: int = 0\n",
                "\n",
                "@dataclass\n",
                "class TextBlock:\n",
                "    \"\"\"A block of text (paragraph or section)\"\"\"\n",
                "    lines: List[TextLine]\n",
                "    bbox: Tuple[int, int, int, int]\n",
                "    block_type: str = \"paragraph\"  # paragraph, header, footer, table\n",
                "\n",
                "def detect_text_lines(image: np.ndarray, min_area: int = 100) -> List[TextLine]:\n",
                "    \"\"\"\n",
                "    Detect text lines using morphological operations\n",
                "    \n",
                "    Returns list of TextLine with bounding boxes\n",
                "    \"\"\"\n",
                "    # Convert to grayscale\n",
                "    if len(image.shape) == 3:\n",
                "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
                "    else:\n",
                "        gray = image.copy()\n",
                "    \n",
                "    # Threshold to get binary image\n",
                "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
                "    \n",
                "    # Dilate horizontally to connect characters in a line\n",
                "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 2))\n",
                "    dilated = cv2.dilate(binary, kernel, iterations=2)\n",
                "    \n",
                "    # Find contours\n",
                "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
                "    \n",
                "    lines = []\n",
                "    for i, cnt in enumerate(contours):\n",
                "        x, y, w, h = cv2.boundingRect(cnt)\n",
                "        area = w * h\n",
                "        \n",
                "        # Filter by area\n",
                "        if area < min_area:\n",
                "            continue\n",
                "        \n",
                "        # Filter by aspect ratio (lines are wider than tall)\n",
                "        if h > w * 0.5:  # Skip very tall/square regions\n",
                "            continue\n",
                "            \n",
                "        lines.append(TextLine(\n",
                "            bbox=(x, y, w, h),\n",
                "            line_index=i\n",
                "        ))\n",
                "    \n",
                "    # Sort by y position (top to bottom)\n",
                "    lines.sort(key=lambda l: l.bbox[1])\n",
                "    \n",
                "    # Update line indices\n",
                "    for i, line in enumerate(lines):\n",
                "        line.line_index = i\n",
                "    \n",
                "    return lines\n",
                "\n",
                "# Detect lines in enhanced image\n",
                "enhanced_img = result.enhanced\n",
                "detected_lines = detect_text_lines(enhanced_img)\n",
                "\n",
                "print(f\"‚úÖ Detected {len(detected_lines)} text lines\")\n",
                "for i, line in enumerate(detected_lines[:5]):\n",
                "    x, y, w, h = line.bbox\n",
                "    print(f\"   Line {i}: bbox=({x}, {y}, {w}x{h})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize detected lines\n",
                "def visualize_lines(image: np.ndarray, lines: List[TextLine], title: str = \"Detected Lines\"):\n",
                "    \"\"\"Draw bounding boxes around detected text lines\"\"\"\n",
                "    vis = image.copy()\n",
                "    \n",
                "    for i, line in enumerate(lines):\n",
                "        x, y, w, h = line.bbox\n",
                "        \n",
                "        # Color based on confidence (green=high, red=low)\n",
                "        if line.confidence > 0.8:\n",
                "            color = (0, 255, 0)  # Green\n",
                "        elif line.confidence > 0.5:\n",
                "            color = (0, 255, 255)  # Yellow\n",
                "        else:\n",
                "            color = (0, 165, 255)  # Orange\n",
                "        \n",
                "        cv2.rectangle(vis, (x, y), (x+w, y+h), color, 2)\n",
                "        cv2.putText(vis, str(i), (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
                "    \n",
                "    plt.figure(figsize=(12, 10))\n",
                "    plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
                "    plt.title(f\"{title} ({len(lines)} lines)\")\n",
                "    plt.axis('off')\n",
                "    plt.show()\n",
                "\n",
                "visualize_lines(enhanced_img, detected_lines)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3Ô∏è‚É£ OCR/HTR Recognition\n",
                "\n",
                "Extract text from detected lines using our OCR service (TrOCR + Tesseract fallback)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import OCR service\n",
                "from app.services.ocr_service import OCRService, OCRResult\n",
                "\n",
                "# Initialize OCR (will use HF API if key available, otherwise Tesseract)\n",
                "ocr = OCRService(\n",
                "    use_api=True,  # Try HF API first\n",
                "    fallback_to_local=True,\n",
                "    line_height_threshold=50\n",
                ")\n",
                "\n",
                "print(\"‚úÖ OCR Service initialized\")\n",
                "print(f\"   HF API available: {ocr.hf_api_available}\")\n",
                "print(f\"   Tesseract available: {ocr.tesseract_available}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract text from whole image\n",
                "start_time = time.time()\n",
                "ocr_result = ocr.extract_text(enhanced_img, preprocess=True)\n",
                "elapsed = time.time() - start_time\n",
                "\n",
                "print(f\"üìÑ OCR Results:\")\n",
                "print(f\"   Processing time: {elapsed:.2f}s\")\n",
                "print(f\"   Model used: {ocr_result.model_used}\")\n",
                "print(f\"   Confidence: {ocr_result.confidence:.2%}\")\n",
                "print(f\"   Lines detected: {len(ocr_result.lines)}\")\n",
                "print(f\"\\nüìù Extracted Text:\")\n",
                "print(\"-\" * 50)\n",
                "print(ocr_result.text[:500] if len(ocr_result.text) > 500 else ocr_result.text)\n",
                "print(\"-\" * 50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract text per line for more detailed results\n",
                "@dataclass\n",
                "class LineOCRResult:\n",
                "    \"\"\"OCR result for a single line\"\"\"\n",
                "    line_index: int\n",
                "    bbox: Tuple[int, int, int, int]\n",
                "    text: str\n",
                "    confidence: float\n",
                "    model_used: str\n",
                "\n",
                "def ocr_lines(image: np.ndarray, lines: List[TextLine], ocr_service: OCRService) -> List[LineOCRResult]:\n",
                "    \"\"\"Run OCR on each detected line\"\"\"\n",
                "    results = []\n",
                "    \n",
                "    for line in lines:\n",
                "        x, y, w, h = line.bbox\n",
                "        \n",
                "        # Crop line region with some padding\n",
                "        pad = 5\n",
                "        y1 = max(0, y - pad)\n",
                "        y2 = min(image.shape[0], y + h + pad)\n",
                "        x1 = max(0, x - pad)\n",
                "        x2 = min(image.shape[1], x + w + pad)\n",
                "        \n",
                "        line_img = image[y1:y2, x1:x2]\n",
                "        \n",
                "        if line_img.size == 0:\n",
                "            continue\n",
                "        \n",
                "        # OCR the line\n",
                "        try:\n",
                "            result = ocr_service.extract_text(line_img, preprocess=False)\n",
                "            results.append(LineOCRResult(\n",
                "                line_index=line.line_index,\n",
                "                bbox=line.bbox,\n",
                "                text=result.text.strip(),\n",
                "                confidence=result.confidence,\n",
                "                model_used=result.model_used\n",
                "            ))\n",
                "        except Exception as e:\n",
                "            print(f\"   ‚ö†Ô∏è Error on line {line.line_index}: {e}\")\n",
                "    \n",
                "    return results\n",
                "\n",
                "# Run per-line OCR (limit to first 5 lines for demo)\n",
                "print(\"üîç Running per-line OCR...\")\n",
                "line_results = ocr_lines(enhanced_img, detected_lines[:5], ocr)\n",
                "\n",
                "print(f\"\\nüìã Per-Line Results:\")\n",
                "for lr in line_results:\n",
                "    conf_emoji = \"üü¢\" if lr.confidence > 0.8 else (\"üü°\" if lr.confidence > 0.5 else \"üî¥\")\n",
                "    print(f\"   Line {lr.line_index}: {conf_emoji} [{lr.confidence:.0%}] '{lr.text[:50]}...' ({lr.model_used})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4Ô∏è‚É£ Text Chunking\n",
                "\n",
                "Split extracted text into overlapping chunks suitable for embedding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import notes embedding service (for chunking)\n",
                "from app.services.notes_embedding import NotesEmbeddingService, TextChunk\n",
                "\n",
                "# Initialize (skip Qdrant connection for now)\n",
                "embedding_service = NotesEmbeddingService(\n",
                "    chunk_size=400,\n",
                "    chunk_overlap=100\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Embedding service initialized\")\n",
                "print(f\"   Chunk size: 400 chars\")\n",
                "print(f\"   Overlap: 100 chars\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Chunk the extracted text\n",
                "doc_id = f\"doc_{uuid.uuid4().hex[:8]}\"\n",
                "page_id = f\"{doc_id}_p01\"\n",
                "job_id = f\"job_{uuid.uuid4().hex[:8]}\"\n",
                "\n",
                "chunks = embedding_service.chunk_text(\n",
                "    text=ocr_result.text,\n",
                "    page_id=page_id,\n",
                "    job_id=job_id,\n",
                "    metadata={\n",
                "        \"document_id\": doc_id,\n",
                "        \"ocr_confidence\": ocr_result.confidence,\n",
                "        \"model_used\": ocr_result.model_used\n",
                "    }\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Created {len(chunks)} chunks from {len(ocr_result.text)} characters\")\n",
                "print(f\"\\nüì¶ Chunk Details:\")\n",
                "for i, chunk in enumerate(chunks[:3]):\n",
                "    print(f\"\\n   Chunk {i}:\")\n",
                "    print(f\"     ID: {chunk.chunk_index}\")\n",
                "    print(f\"     Chars: {chunk.start_char}-{chunk.end_char}\")\n",
                "    print(f\"     Text: '{chunk.text[:100]}...'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5Ô∏è‚É£ Embedding Generation\n",
                "\n",
                "Generate vector embeddings for semantic search."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate embeddings for chunks\n",
                "texts = [chunk.text for chunk in chunks]\n",
                "\n",
                "start_time = time.time()\n",
                "embeddings = embedding_service.generate_embeddings(texts)\n",
                "elapsed = time.time() - start_time\n",
                "\n",
                "print(f\"‚úÖ Generated {len(embeddings)} embeddings\")\n",
                "print(f\"   Time: {elapsed:.2f}s\")\n",
                "print(f\"   Embedding dimension: {len(embeddings[0]) if embeddings else 0}\")\n",
                "\n",
                "# Show embedding stats\n",
                "if embeddings:\n",
                "    emb_array = np.array(embeddings)\n",
                "    print(f\"\\nüìä Embedding Statistics:\")\n",
                "    print(f\"   Mean: {emb_array.mean():.4f}\")\n",
                "    print(f\"   Std: {emb_array.std():.4f}\")\n",
                "    print(f\"   Min: {emb_array.min():.4f}\")\n",
                "    print(f\"   Max: {emb_array.max():.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize embedding similarity matrix\n",
                "if len(embeddings) > 1:\n",
                "    from sklearn.metrics.pairwise import cosine_similarity\n",
                "    \n",
                "    sim_matrix = cosine_similarity(embeddings)\n",
                "    \n",
                "    plt.figure(figsize=(8, 6))\n",
                "    plt.imshow(sim_matrix, cmap='viridis', vmin=0, vmax=1)\n",
                "    plt.colorbar(label='Cosine Similarity')\n",
                "    plt.title('Chunk Embedding Similarity Matrix')\n",
                "    plt.xlabel('Chunk Index')\n",
                "    plt.ylabel('Chunk Index')\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6Ô∏è‚É£ Complete Pipeline Summary\n",
                "\n",
                "Assemble the full document metadata structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build complete document structure\n",
                "document = {\n",
                "    \"document_id\": doc_id,\n",
                "    \"job_id\": job_id,\n",
                "    \"status\": \"processed\",\n",
                "    \"processing_time_sec\": 0,\n",
                "    \"pages\": [\n",
                "        {\n",
                "            \"page_id\": page_id,\n",
                "            \"page_index\": 0,\n",
                "            \"width\": enhanced_img.shape[1],\n",
                "            \"height\": enhanced_img.shape[0],\n",
                "            \"ocr_confidence_avg\": ocr_result.confidence,\n",
                "            \"lines\": [\n",
                "                {\n",
                "                    \"line_index\": lr.line_index,\n",
                "                    \"bbox\": lr.bbox,\n",
                "                    \"text\": lr.text,\n",
                "                    \"confidence\": lr.confidence\n",
                "                }\n",
                "                for lr in line_results\n",
                "            ],\n",
                "            \"chunks\": [\n",
                "                {\n",
                "                    \"chunk_id\": f\"{page_id}_c{chunk.chunk_index:04d}\",\n",
                "                    \"chunk_index\": chunk.chunk_index,\n",
                "                    \"text\": chunk.text,\n",
                "                    \"char_range\": [chunk.start_char, chunk.end_char],\n",
                "                    \"embedding_dim\": len(embeddings[i]) if i < len(embeddings) else 0\n",
                "                }\n",
                "                for i, chunk in enumerate(chunks)\n",
                "            ]\n",
                "        }\n",
                "    ]\n",
                "}\n",
                "\n",
                "print(\"üìÑ Complete Document Structure:\")\n",
                "print(json.dumps(document, indent=2, default=str)[:2000])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üîç Bonus: Semantic Search Demo\n",
                "\n",
                "If you have Qdrant running, you can test semantic search."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Semantic search (requires Qdrant running)\n",
                "def demo_semantic_search(query: str, embeddings: List, chunks: List[TextChunk], top_k: int = 3):\n",
                "    \"\"\"Simple in-memory semantic search demo\"\"\"\n",
                "    from sklearn.metrics.pairwise import cosine_similarity\n",
                "    \n",
                "    # Generate query embedding\n",
                "    query_emb = embedding_service.generate_embeddings([query])\n",
                "    \n",
                "    if not query_emb or not embeddings:\n",
                "        print(\"No embeddings available\")\n",
                "        return\n",
                "    \n",
                "    # Calculate similarities\n",
                "    sims = cosine_similarity(query_emb, embeddings)[0]\n",
                "    \n",
                "    # Get top-k\n",
                "    top_indices = np.argsort(sims)[::-1][:top_k]\n",
                "    \n",
                "    print(f\"üîç Query: '{query}'\")\n",
                "    print(f\"\\nüìã Top {top_k} Results:\")\n",
                "    for rank, idx in enumerate(top_indices):\n",
                "        print(f\"\\n   {rank+1}. Score: {sims[idx]:.4f}\")\n",
                "        print(f\"      Text: '{chunks[idx].text[:100]}...'\")\n",
                "\n",
                "# Demo search\n",
                "if chunks and embeddings:\n",
                "    demo_semantic_search(\"what is the main topic\", embeddings, chunks)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üìä Quality Metrics & Thresholds\n",
                "\n",
                "Evaluate OCR quality and flag pages needing review."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class QualityReport:\n",
                "    \"\"\"Quality assessment for a processed page\"\"\"\n",
                "    page_id: str\n",
                "    avg_confidence: float\n",
                "    min_confidence: float\n",
                "    max_confidence: float\n",
                "    lines_below_threshold: int\n",
                "    needs_review: bool\n",
                "    recommended_action: str\n",
                "\n",
                "def assess_quality(\n",
                "    line_results: List[LineOCRResult],\n",
                "    page_id: str,\n",
                "    confidence_threshold: float = 0.7\n",
                ") -> QualityReport:\n",
                "    \"\"\"Assess OCR quality and determine if human review needed\"\"\"\n",
                "    \n",
                "    if not line_results:\n",
                "        return QualityReport(\n",
                "            page_id=page_id,\n",
                "            avg_confidence=0.0,\n",
                "            min_confidence=0.0,\n",
                "            max_confidence=0.0,\n",
                "            lines_below_threshold=0,\n",
                "            needs_review=True,\n",
                "            recommended_action=\"No text detected - manual review required\"\n",
                "        )\n",
                "    \n",
                "    confidences = [lr.confidence for lr in line_results]\n",
                "    avg_conf = np.mean(confidences)\n",
                "    min_conf = np.min(confidences)\n",
                "    max_conf = np.max(confidences)\n",
                "    lines_below = sum(1 for c in confidences if c < confidence_threshold)\n",
                "    \n",
                "    # Determine if review needed\n",
                "    needs_review = avg_conf < confidence_threshold or lines_below > len(confidences) * 0.3\n",
                "    \n",
                "    # Recommend action\n",
                "    if avg_conf >= 0.9:\n",
                "        action = \"High quality - ready for indexing\"\n",
                "    elif avg_conf >= 0.7:\n",
                "        action = \"Good quality - minor review recommended\"\n",
                "    elif avg_conf >= 0.5:\n",
                "        action = \"Medium quality - review low-confidence lines\"\n",
                "    else:\n",
                "        action = \"Low quality - consider reprocessing or manual transcription\"\n",
                "    \n",
                "    return QualityReport(\n",
                "        page_id=page_id,\n",
                "        avg_confidence=avg_conf,\n",
                "        min_confidence=min_conf,\n",
                "        max_confidence=max_conf,\n",
                "        lines_below_threshold=lines_below,\n",
                "        needs_review=needs_review,\n",
                "        recommended_action=action\n",
                "    )\n",
                "\n",
                "# Generate quality report\n",
                "quality = assess_quality(line_results, page_id)\n",
                "\n",
                "print(\"üìä Quality Report:\")\n",
                "print(f\"   Page: {quality.page_id}\")\n",
                "print(f\"   Avg Confidence: {quality.avg_confidence:.1%}\")\n",
                "print(f\"   Range: {quality.min_confidence:.1%} - {quality.max_confidence:.1%}\")\n",
                "print(f\"   Lines below threshold: {quality.lines_below_threshold}\")\n",
                "print(f\"   Needs review: {'‚ö†Ô∏è YES' if quality.needs_review else '‚úÖ NO'}\")\n",
                "print(f\"   Recommendation: {quality.recommended_action}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üíæ Export Results\n",
                "\n",
                "Save processed results for further analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save processed results\n",
                "output_dir = Path(\"./output\")\n",
                "output_dir.mkdir(exist_ok=True)\n",
                "\n",
                "# Save enhanced image\n",
                "cv2.imwrite(str(output_dir / f\"{page_id}_enhanced.png\"), result.enhanced)\n",
                "cv2.imwrite(str(output_dir / f\"{page_id}_thumbnail.png\"), result.thumbnail)\n",
                "\n",
                "# Save document JSON\n",
                "with open(output_dir / f\"{doc_id}_metadata.json\", 'w') as f:\n",
                "    json.dump(document, f, indent=2, default=str)\n",
                "\n",
                "# Save quality report\n",
                "with open(output_dir / f\"{page_id}_quality.json\", 'w') as f:\n",
                "    json.dump(asdict(quality), f, indent=2)\n",
                "\n",
                "print(f\"‚úÖ Results saved to: {output_dir.absolute()}\")\n",
                "print(f\"   - {page_id}_enhanced.png\")\n",
                "print(f\"   - {page_id}_thumbnail.png\")\n",
                "print(f\"   - {doc_id}_metadata.json\")\n",
                "print(f\"   - {page_id}_quality.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## üéØ Summary\n",
                "\n",
                "This notebook demonstrated the complete Digitize Notes pipeline:\n",
                "\n",
                "| Step | Component | Output |\n",
                "|------|-----------|--------|\n",
                "| 1 | Image Enhancement | Perspective-corrected, denoised image |\n",
                "| 2 | Line Detection | Bounding boxes for text regions |\n",
                "| 3 | OCR/HTR | Extracted text with confidence scores |\n",
                "| 4 | Chunking | Semantic chunks for embedding |\n",
                "| 5 | Embedding | Vector representations for search |\n",
                "| 6 | Quality Assessment | Review flags and recommendations |\n",
                "\n",
                "### Next Steps:\n",
                "- Integrate with Qdrant for vector storage\n",
                "- Add PDF splitting for multi-page documents\n",
                "- Implement human correction endpoints\n",
                "- Build UI for review workflow"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}