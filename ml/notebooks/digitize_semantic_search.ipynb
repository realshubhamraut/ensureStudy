{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ” Semantic Search & Vector Storage\n",
                "\n",
                "This notebook demonstrates:\n",
                "1. **Text Chunking** - Split OCR text into semantic chunks\n",
                "2. **Embedding Generation** - Create vectors using sentence transformers\n",
                "3. **Vector Storage** - Store and index in Qdrant\n",
                "4. **Semantic Search** - Query and retrieve relevant chunks\n",
                "5. **RAG Integration** - Combine search with LLM for Q&A\n",
                "\n",
                "## Prerequisites:\n",
                "```bash\n",
                "pip install sentence-transformers qdrant-client\n",
                "# Optional for RAG: pip install openai google-generativeai\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
                "sys.path.insert(0, project_root)\n",
                "sys.path.insert(0, os.path.join(project_root, 'backend/ai-service'))\n",
                "\n",
                "import numpy as np\n",
                "import json\n",
                "from typing import List, Dict, Any, Optional, Tuple\n",
                "from dataclasses import dataclass, field, asdict\n",
                "import uuid\n",
                "import time\n",
                "\n",
                "print(f\"âœ… Project root: {project_root}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1ï¸âƒ£ Text Chunking with Metadata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ChunkMetadata:\n",
                "    \"\"\"Metadata for a text chunk - links back to source\"\"\"\n",
                "    document_id: str\n",
                "    page_id: str\n",
                "    chunk_id: str\n",
                "    chunk_index: int\n",
                "    start_char: int\n",
                "    end_char: int\n",
                "    bbox: Dict[str, int] = None  # Union of line bboxes\n",
                "    line_indices: List[int] = field(default_factory=list)\n",
                "    ocr_confidence: float = 1.0\n",
                "    language: str = \"en\"\n",
                "\n",
                "@dataclass\n",
                "class TextChunk:\n",
                "    \"\"\"A chunk of text with full metadata\"\"\"\n",
                "    text: str\n",
                "    metadata: ChunkMetadata\n",
                "    embedding: List[float] = None\n",
                "\n",
                "class SmartChunker:\n",
                "    \"\"\"\n",
                "    Intelligent text chunking that respects:\n",
                "    - Sentence boundaries\n",
                "    - Paragraph boundaries\n",
                "    - Page boundaries (never cross pages)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        max_chunk_size: int = 400,  # characters\n",
                "        min_chunk_size: int = 50,\n",
                "        overlap: int = 50\n",
                "    ):\n",
                "        self.max_chunk_size = max_chunk_size\n",
                "        self.min_chunk_size = min_chunk_size\n",
                "        self.overlap = overlap\n",
                "        \n",
                "        # Sentence endings\n",
                "        self.sentence_endings = ['. ', '! ', '? ', '.\\n', '!\\n', '?\\n']\n",
                "    \n",
                "    def chunk_text(\n",
                "        self,\n",
                "        text: str,\n",
                "        document_id: str,\n",
                "        page_id: str,\n",
                "        ocr_confidence: float = 1.0,\n",
                "        bbox: Dict[str, int] = None\n",
                "    ) -> List[TextChunk]:\n",
                "        \"\"\"\n",
                "        Split text into overlapping chunks\n",
                "        \n",
                "        Returns list of TextChunk with metadata\n",
                "        \"\"\"\n",
                "        if not text.strip():\n",
                "            return []\n",
                "        \n",
                "        chunks = []\n",
                "        start = 0\n",
                "        chunk_index = 0\n",
                "        \n",
                "        while start < len(text):\n",
                "            # Find end position\n",
                "            end = min(start + self.max_chunk_size, len(text))\n",
                "            \n",
                "            # Try to find a sentence boundary\n",
                "            if end < len(text):\n",
                "                best_break = end\n",
                "                \n",
                "                # Look for sentence ending in last 20% of chunk\n",
                "                search_start = start + int(self.max_chunk_size * 0.8)\n",
                "                for ending in self.sentence_endings:\n",
                "                    pos = text.rfind(ending, search_start, end)\n",
                "                    if pos > start:\n",
                "                        best_break = pos + len(ending)\n",
                "                        break\n",
                "                \n",
                "                # Fallback to paragraph break\n",
                "                if best_break == end:\n",
                "                    para_break = text.rfind('\\n\\n', search_start, end)\n",
                "                    if para_break > start:\n",
                "                        best_break = para_break + 2\n",
                "                \n",
                "                # Last resort: word boundary\n",
                "                if best_break == end:\n",
                "                    space_break = text.rfind(' ', search_start, end)\n",
                "                    if space_break > start:\n",
                "                        best_break = space_break + 1\n",
                "                \n",
                "                end = best_break\n",
                "            \n",
                "            # Extract chunk text\n",
                "            chunk_text = text[start:end].strip()\n",
                "            \n",
                "            # Skip if too short\n",
                "            if len(chunk_text) >= self.min_chunk_size:\n",
                "                chunk_id = f\"{page_id}_c{chunk_index:04d}\"\n",
                "                \n",
                "                chunks.append(TextChunk(\n",
                "                    text=chunk_text,\n",
                "                    metadata=ChunkMetadata(\n",
                "                        document_id=document_id,\n",
                "                        page_id=page_id,\n",
                "                        chunk_id=chunk_id,\n",
                "                        chunk_index=chunk_index,\n",
                "                        start_char=start,\n",
                "                        end_char=end,\n",
                "                        bbox=bbox,\n",
                "                        ocr_confidence=ocr_confidence\n",
                "                    )\n",
                "                ))\n",
                "                chunk_index += 1\n",
                "            \n",
                "            # Move to next chunk with overlap\n",
                "            start = end - self.overlap\n",
                "            if start >= len(text) - self.min_chunk_size:\n",
                "                break\n",
                "        \n",
                "        return chunks\n",
                "\n",
                "# Initialize\n",
                "chunker = SmartChunker(max_chunk_size=400, min_chunk_size=50, overlap=50)\n",
                "print(\"âœ… SmartChunker initialized\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test chunking with sample text\n",
                "sample_text = \"\"\"\n",
                "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It focuses on developing algorithms that can access data and use it to learn for themselves.\n",
                "\n",
                "The learning process begins with observations or data, such as examples, direct experience, or instruction. The goal is to look for patterns in data and make better decisions based on the examples we provide.\n",
                "\n",
                "There are several types of machine learning:\n",
                "\n",
                "1. Supervised Learning: The algorithm learns from labeled training data, trying to generalize from these examples to unseen situations.\n",
                "\n",
                "2. Unsupervised Learning: The algorithm tries to find hidden patterns or intrinsic structures in unlabeled data.\n",
                "\n",
                "3. Reinforcement Learning: The algorithm learns by interacting with an environment, receiving rewards or penalties for actions taken.\n",
                "\n",
                "Deep learning is a subset of machine learning that uses neural networks with many layers. These deep neural networks can learn hierarchical representations of data, enabling them to perform complex tasks like image recognition and natural language processing.\n",
                "\"\"\"\n",
                "\n",
                "doc_id = f\"doc_{uuid.uuid4().hex[:8]}\"\n",
                "page_id = f\"{doc_id}_p01\"\n",
                "\n",
                "chunks = chunker.chunk_text(\n",
                "    text=sample_text,\n",
                "    document_id=doc_id,\n",
                "    page_id=page_id,\n",
                "    ocr_confidence=0.95\n",
                ")\n",
                "\n",
                "print(f\"ðŸ“¦ Created {len(chunks)} chunks from {len(sample_text)} characters\")\n",
                "print(f\"\\nðŸ“‹ Chunk Details:\")\n",
                "for chunk in chunks:\n",
                "    meta = chunk.metadata\n",
                "    print(f\"\\n   {meta.chunk_id}:\")\n",
                "    print(f\"     Chars: {meta.start_char}-{meta.end_char}\")\n",
                "    print(f\"     Length: {len(chunk.text)}\")\n",
                "    print(f\"     Text: '{chunk.text[:80]}...'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2ï¸âƒ£ Embedding Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    from sentence_transformers import SentenceTransformer\n",
                "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
                "    print(\"âœ… sentence-transformers available\")\n",
                "except ImportError:\n",
                "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
                "    print(\"âš ï¸ sentence-transformers not installed\")\n",
                "\n",
                "class EmbeddingGenerator:\n",
                "    \"\"\"\n",
                "    Generate embeddings using sentence transformers\n",
                "    \n",
                "    Supports multiple models for different use cases:\n",
                "    - all-MiniLM-L6-v2: Fast, good quality (384 dimensions)\n",
                "    - all-mpnet-base-v2: Best quality (768 dimensions)\n",
                "    - paraphrase-multilingual-MiniLM-L12-v2: Multilingual support\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
                "        self.model_name = model_name\n",
                "        self.model = None\n",
                "        self.embedding_dim = None\n",
                "        \n",
                "        if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
                "            self._load_model()\n",
                "    \n",
                "    def _load_model(self):\n",
                "        \"\"\"Load the sentence transformer model\"\"\"\n",
                "        print(f\"   Loading model: {self.model_name}...\")\n",
                "        self.model = SentenceTransformer(self.model_name)\n",
                "        \n",
                "        # Get embedding dimension\n",
                "        test_emb = self.model.encode([\"test\"])\n",
                "        self.embedding_dim = len(test_emb[0])\n",
                "        print(f\"   âœ… Model loaded. Embedding dim: {self.embedding_dim}\")\n",
                "    \n",
                "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
                "        \"\"\"\n",
                "        Generate embeddings for a list of texts\n",
                "        \n",
                "        Returns list of embedding vectors\n",
                "        \"\"\"\n",
                "        if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
                "            # Return random embeddings for demo\n",
                "            return [np.random.randn(384).tolist() for _ in texts]\n",
                "        \n",
                "        embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
                "        return embeddings.tolist()\n",
                "    \n",
                "    def embed_query(self, query: str) -> List[float]:\n",
                "        \"\"\"Generate embedding for a single query\"\"\"\n",
                "        return self.embed_texts([query])[0]\n",
                "    \n",
                "    def similarity(self, emb1: List[float], emb2: List[float]) -> float:\n",
                "        \"\"\"Compute cosine similarity between two embeddings\"\"\"\n",
                "        a = np.array(emb1)\n",
                "        b = np.array(emb2)\n",
                "        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
                "\n",
                "# Initialize\n",
                "embedder = EmbeddingGenerator(model_name=\"all-MiniLM-L6-v2\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate embeddings for chunks\n",
                "texts = [chunk.text for chunk in chunks]\n",
                "\n",
                "start_time = time.time()\n",
                "embeddings = embedder.embed_texts(texts)\n",
                "elapsed = time.time() - start_time\n",
                "\n",
                "# Attach embeddings to chunks\n",
                "for chunk, emb in zip(chunks, embeddings):\n",
                "    chunk.embedding = emb\n",
                "\n",
                "print(f\"âœ… Generated {len(embeddings)} embeddings in {elapsed:.2f}s\")\n",
                "print(f\"   Dimension: {len(embeddings[0])}\")\n",
                "print(f\"   Avg embedding time: {elapsed/len(embeddings)*1000:.1f}ms per chunk\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3ï¸âƒ£ Vector Storage (Qdrant)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    from qdrant_client import QdrantClient\n",
                "    from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue\n",
                "    QDRANT_AVAILABLE = True\n",
                "    print(\"âœ… qdrant-client available\")\n",
                "except ImportError:\n",
                "    QDRANT_AVAILABLE = False\n",
                "    print(\"âš ï¸ qdrant-client not installed\")\n",
                "\n",
                "class VectorStore:\n",
                "    \"\"\"\n",
                "    Vector storage adapter using Qdrant\n",
                "    \n",
                "    Can use:\n",
                "    - In-memory (for testing)\n",
                "    - Local file (persistent)\n",
                "    - Remote Qdrant server\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        collection_name: str = \"notes_chunks\",\n",
                "        vector_size: int = 384,\n",
                "        host: str = None,\n",
                "        port: int = 6333,\n",
                "        in_memory: bool = True\n",
                "    ):\n",
                "        self.collection_name = collection_name\n",
                "        self.vector_size = vector_size\n",
                "        \n",
                "        if not QDRANT_AVAILABLE:\n",
                "            self.client = None\n",
                "            self._in_memory_store = []  # Fallback\n",
                "            print(\"   Using in-memory fallback (no Qdrant)\")\n",
                "            return\n",
                "        \n",
                "        # Initialize client\n",
                "        if in_memory or host is None:\n",
                "            self.client = QdrantClient(\":memory:\")\n",
                "            print(\"   Using in-memory Qdrant\")\n",
                "        else:\n",
                "            self.client = QdrantClient(host=host, port=port)\n",
                "            print(f\"   Connected to Qdrant at {host}:{port}\")\n",
                "        \n",
                "        # Create collection\n",
                "        self._create_collection()\n",
                "    \n",
                "    def _create_collection(self):\n",
                "        \"\"\"Create collection if not exists\"\"\"\n",
                "        try:\n",
                "            self.client.create_collection(\n",
                "                collection_name=self.collection_name,\n",
                "                vectors_config=VectorParams(\n",
                "                    size=self.vector_size,\n",
                "                    distance=Distance.COSINE\n",
                "                )\n",
                "            )\n",
                "            print(f\"   Created collection: {self.collection_name}\")\n",
                "        except Exception as e:\n",
                "            print(f\"   Collection exists or error: {e}\")\n",
                "    \n",
                "    def insert_chunks(self, chunks: List[TextChunk]) -> int:\n",
                "        \"\"\"\n",
                "        Insert chunks with embeddings into vector store\n",
                "        \n",
                "        Returns number of inserted points\n",
                "        \"\"\"\n",
                "        if not self.client:\n",
                "            # Fallback to in-memory\n",
                "            self._in_memory_store.extend(chunks)\n",
                "            return len(chunks)\n",
                "        \n",
                "        points = []\n",
                "        for chunk in chunks:\n",
                "            if not chunk.embedding:\n",
                "                continue\n",
                "            \n",
                "            points.append(PointStruct(\n",
                "                id=str(uuid.uuid4()),\n",
                "                vector=chunk.embedding,\n",
                "                payload={\n",
                "                    \"chunk_id\": chunk.metadata.chunk_id,\n",
                "                    \"document_id\": chunk.metadata.document_id,\n",
                "                    \"page_id\": chunk.metadata.page_id,\n",
                "                    \"chunk_index\": chunk.metadata.chunk_index,\n",
                "                    \"text\": chunk.text,\n",
                "                    \"start_char\": chunk.metadata.start_char,\n",
                "                    \"end_char\": chunk.metadata.end_char,\n",
                "                    \"ocr_confidence\": chunk.metadata.ocr_confidence\n",
                "                }\n",
                "            ))\n",
                "        \n",
                "        if points:\n",
                "            self.client.upsert(\n",
                "                collection_name=self.collection_name,\n",
                "                points=points\n",
                "            )\n",
                "        \n",
                "        return len(points)\n",
                "    \n",
                "    def search(\n",
                "        self,\n",
                "        query_embedding: List[float],\n",
                "        limit: int = 5,\n",
                "        document_id: str = None,\n",
                "        min_score: float = 0.0\n",
                "    ) -> List[Dict[str, Any]]:\n",
                "        \"\"\"\n",
                "        Search for similar chunks\n",
                "        \n",
                "        Returns list of results with score and metadata\n",
                "        \"\"\"\n",
                "        if not self.client:\n",
                "            # In-memory fallback search\n",
                "            return self._in_memory_search(query_embedding, limit, document_id)\n",
                "        \n",
                "        # Build filter\n",
                "        query_filter = None\n",
                "        if document_id:\n",
                "            query_filter = Filter(\n",
                "                must=[FieldCondition(\n",
                "                    key=\"document_id\",\n",
                "                    match=MatchValue(value=document_id)\n",
                "                )]\n",
                "            )\n",
                "        \n",
                "        results = self.client.search(\n",
                "            collection_name=self.collection_name,\n",
                "            query_vector=query_embedding,\n",
                "            limit=limit,\n",
                "            query_filter=query_filter,\n",
                "            score_threshold=min_score\n",
                "        )\n",
                "        \n",
                "        return [\n",
                "            {\n",
                "                \"score\": hit.score,\n",
                "                **hit.payload\n",
                "            }\n",
                "            for hit in results\n",
                "        ]\n",
                "    \n",
                "    def _in_memory_search(\n",
                "        self,\n",
                "        query_embedding: List[float],\n",
                "        limit: int,\n",
                "        document_id: str\n",
                "    ) -> List[Dict[str, Any]]:\n",
                "        \"\"\"Fallback in-memory search\"\"\"\n",
                "        from sklearn.metrics.pairwise import cosine_similarity\n",
                "        \n",
                "        filtered_chunks = self._in_memory_store\n",
                "        if document_id:\n",
                "            filtered_chunks = [c for c in filtered_chunks if c.metadata.document_id == document_id]\n",
                "        \n",
                "        if not filtered_chunks:\n",
                "            return []\n",
                "        \n",
                "        # Calculate similarities\n",
                "        embeddings = [c.embedding for c in filtered_chunks]\n",
                "        sims = cosine_similarity([query_embedding], embeddings)[0]\n",
                "        \n",
                "        # Sort and return top-k\n",
                "        indices = np.argsort(sims)[::-1][:limit]\n",
                "        \n",
                "        return [\n",
                "            {\n",
                "                \"score\": float(sims[idx]),\n",
                "                \"chunk_id\": filtered_chunks[idx].metadata.chunk_id,\n",
                "                \"document_id\": filtered_chunks[idx].metadata.document_id,\n",
                "                \"page_id\": filtered_chunks[idx].metadata.page_id,\n",
                "                \"text\": filtered_chunks[idx].text,\n",
                "                \"ocr_confidence\": filtered_chunks[idx].metadata.ocr_confidence\n",
                "            }\n",
                "            for idx in indices\n",
                "        ]\n",
                "\n",
                "# Initialize\n",
                "vector_store = VectorStore(\n",
                "    collection_name=\"notes_demo\",\n",
                "    vector_size=embedder.embedding_dim or 384,\n",
                "    in_memory=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Insert chunks into vector store\n",
                "inserted = vector_store.insert_chunks(chunks)\n",
                "print(f\"âœ… Inserted {inserted} chunks into vector store\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4ï¸âƒ£ Semantic Search"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SemanticSearcher:\n",
                "    \"\"\"Semantic search over indexed notes\"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        embedder: EmbeddingGenerator,\n",
                "        vector_store: VectorStore\n",
                "    ):\n",
                "        self.embedder = embedder\n",
                "        self.vector_store = vector_store\n",
                "    \n",
                "    def search(\n",
                "        self,\n",
                "        query: str,\n",
                "        limit: int = 5,\n",
                "        document_id: str = None,\n",
                "        min_score: float = 0.3\n",
                "    ) -> List[Dict[str, Any]]:\n",
                "        \"\"\"\n",
                "        Search for relevant chunks\n",
                "        \n",
                "        Returns list of results with score and context\n",
                "        \"\"\"\n",
                "        # Generate query embedding\n",
                "        query_emb = self.embedder.embed_query(query)\n",
                "        \n",
                "        # Search\n",
                "        results = self.vector_store.search(\n",
                "            query_embedding=query_emb,\n",
                "            limit=limit,\n",
                "            document_id=document_id,\n",
                "            min_score=min_score\n",
                "        )\n",
                "        \n",
                "        return results\n",
                "    \n",
                "    def search_with_context(\n",
                "        self,\n",
                "        query: str,\n",
                "        limit: int = 5\n",
                "    ) -> str:\n",
                "        \"\"\"\n",
                "        Search and return formatted context for RAG\n",
                "        \"\"\"\n",
                "        results = self.search(query, limit=limit)\n",
                "        \n",
                "        if not results:\n",
                "            return \"No relevant information found.\"\n",
                "        \n",
                "        context_parts = []\n",
                "        for i, result in enumerate(results):\n",
                "            context_parts.append(\n",
                "                f\"[Source {i+1}] (Relevance: {result['score']:.2f})\\n{result['text']}\"\n",
                "            )\n",
                "        \n",
                "        return \"\\n\\n\".join(context_parts)\n",
                "\n",
                "# Initialize\n",
                "searcher = SemanticSearcher(embedder, vector_store)\n",
                "print(\"âœ… SemanticSearcher ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test semantic search\n",
                "queries = [\n",
                "    \"What is machine learning?\",\n",
                "    \"types of learning algorithms\",\n",
                "    \"deep neural networks\"\n",
                "]\n",
                "\n",
                "for query in queries:\n",
                "    print(f\"\\nðŸ” Query: '{query}'\")\n",
                "    print(\"-\" * 50)\n",
                "    \n",
                "    results = searcher.search(query, limit=3)\n",
                "    \n",
                "    for i, result in enumerate(results):\n",
                "        score_emoji = \"ðŸŸ¢\" if result['score'] > 0.7 else (\"ðŸŸ¡\" if result['score'] > 0.5 else \"ðŸ”´\")\n",
                "        print(f\"\\n   {i+1}. {score_emoji} Score: {result['score']:.4f}\")\n",
                "        print(f\"      Chunk: {result['chunk_id']}\")\n",
                "        print(f\"      Text: '{result['text'][:100]}...'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5ï¸âƒ£ RAG (Retrieval Augmented Generation)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RAGAnswerer:\n",
                "    \"\"\"\n",
                "    RAG-based Q&A over notes\n",
                "    \n",
                "    Uses semantic search + LLM to answer questions\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, searcher: SemanticSearcher):\n",
                "        self.searcher = searcher\n",
                "        self.llm = None\n",
                "        self._init_llm()\n",
                "    \n",
                "    def _init_llm(self):\n",
                "        \"\"\"Initialize LLM (try Gemini, fallback to OpenAI)\"\"\"\n",
                "        # Try Gemini\n",
                "        try:\n",
                "            import google.generativeai as genai\n",
                "            api_key = os.environ.get('GOOGLE_API_KEY')\n",
                "            if api_key:\n",
                "                genai.configure(api_key=api_key)\n",
                "                self.llm = genai.GenerativeModel('gemini-pro')\n",
                "                print(\"   âœ… Using Gemini\")\n",
                "                return\n",
                "        except ImportError:\n",
                "            pass\n",
                "        \n",
                "        # Try OpenAI\n",
                "        try:\n",
                "            import openai\n",
                "            if os.environ.get('OPENAI_API_KEY'):\n",
                "                self.llm = openai\n",
                "                print(\"   âœ… Using OpenAI\")\n",
                "                return\n",
                "        except ImportError:\n",
                "            pass\n",
                "        \n",
                "        print(\"   âš ï¸ No LLM available - will return context only\")\n",
                "    \n",
                "    def answer(\n",
                "        self,\n",
                "        question: str,\n",
                "        top_k: int = 3\n",
                "    ) -> Dict[str, Any]:\n",
                "        \"\"\"\n",
                "        Answer a question using RAG\n",
                "        \n",
                "        Returns answer with sources\n",
                "        \"\"\"\n",
                "        # Retrieve context\n",
                "        results = self.searcher.search(question, limit=top_k)\n",
                "        context = self.searcher.search_with_context(question, limit=top_k)\n",
                "        \n",
                "        if not self.llm:\n",
                "            return {\n",
                "                \"answer\": f\"Based on the notes:\\n\\n{context}\",\n",
                "                \"sources\": results,\n",
                "                \"model\": \"none\"\n",
                "            }\n",
                "        \n",
                "        # Generate answer with LLM\n",
                "        prompt = f\"\"\"Based on the following notes content, answer the question.\n",
                "If the answer is not in the notes, say so.\n",
                "\n",
                "NOTES CONTENT:\n",
                "{context}\n",
                "\n",
                "QUESTION: {question}\n",
                "\n",
                "ANSWER:\"\"\"\n",
                "        \n",
                "        try:\n",
                "            if hasattr(self.llm, 'generate_content'):\n",
                "                # Gemini\n",
                "                response = self.llm.generate_content(prompt)\n",
                "                answer = response.text\n",
                "                model = \"gemini-pro\"\n",
                "            else:\n",
                "                # OpenAI\n",
                "                response = self.llm.ChatCompletion.create(\n",
                "                    model=\"gpt-3.5-turbo\",\n",
                "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
                "                )\n",
                "                answer = response.choices[0].message.content\n",
                "                model = \"gpt-3.5-turbo\"\n",
                "        except Exception as e:\n",
                "            answer = f\"Error generating answer: {e}\\n\\nContext:\\n{context}\"\n",
                "            model = \"error\"\n",
                "        \n",
                "        return {\n",
                "            \"answer\": answer,\n",
                "            \"sources\": results,\n",
                "            \"model\": model\n",
                "        }\n",
                "\n",
                "# Initialize\n",
                "rag = RAGAnswerer(searcher)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test RAG Q&A\n",
                "question = \"What are the different types of machine learning?\"\n",
                "\n",
                "print(f\"â“ Question: {question}\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "result = rag.answer(question)\n",
                "\n",
                "print(f\"\\nðŸ’¬ Answer ({result['model']}):\")\n",
                "print(result['answer'])\n",
                "\n",
                "print(f\"\\nðŸ“š Sources:\")\n",
                "for source in result['sources']:\n",
                "    print(f\"   - {source['chunk_id']}: score={source['score']:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6ï¸âƒ£ API Response Format (for Integration)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_search_response(results: List[Dict], query: str) -> Dict[str, Any]:\n",
                "    \"\"\"Format search results for API response\"\"\"\n",
                "    return {\n",
                "        \"query\": query,\n",
                "        \"total_results\": len(results),\n",
                "        \"results\": [\n",
                "            {\n",
                "                \"chunk_id\": r[\"chunk_id\"],\n",
                "                \"document_id\": r[\"document_id\"],\n",
                "                \"page_id\": r[\"page_id\"],\n",
                "                \"score\": round(r[\"score\"], 4),\n",
                "                \"text\": r[\"text\"],\n",
                "                \"highlight_bbox\": r.get(\"bbox\")  # For UI highlighting\n",
                "            }\n",
                "            for r in results\n",
                "        ]\n",
                "    }\n",
                "\n",
                "# Demo\n",
                "results = searcher.search(\"neural networks\")\n",
                "api_response = format_search_response(results, \"neural networks\")\n",
                "\n",
                "print(\"ðŸ“¡ API Response:\")\n",
                "print(json.dumps(api_response, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ðŸŽ¯ Summary\n",
                "\n",
                "This notebook demonstrated:\n",
                "\n",
                "| Component | Purpose | Technology |\n",
                "|-----------|---------|------------|\n",
                "| SmartChunker | Semantic text chunking | Custom algorithm |\n",
                "| EmbeddingGenerator | Vector embeddings | Sentence Transformers |\n",
                "| VectorStore | Storage & indexing | Qdrant |\n",
                "| SemanticSearcher | Similarity search | Cosine similarity |\n",
                "| RAGAnswerer | Q&A with LLM | Gemini/OpenAI |\n",
                "\n",
                "### Integration Points:\n",
                "- `SmartChunker` â†’ After OCR text extraction\n",
                "- `VectorStore` â†’ Qdrant service adapter\n",
                "- `SemanticSearcher` â†’ `/api/search` endpoint\n",
                "- `RAGAnswerer` â†’ `/api/notes/ask` endpoint"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}