{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff48a158",
   "metadata": {},
   "source": [
    "# Evaluation Engine Module - Detailed Walkthrough\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Orchestrates the complete evaluation by combining keyword, semantic, and concept analysis.\n",
    "Calculates final scores using weighted aggregation.\n",
    "\n",
    "## Why This Module Exists\n",
    "\n",
    "**Why multi-faceted evaluation:**\n",
    "- Single metric can be gamed or fail\n",
    "- Hybrid approach captures different aspects\n",
    "- Configurable weights allow customization\n",
    "- More robust and fair than any single method\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**Evaluation Formula:**\n",
    "```\n",
    "Final Score = (0.6 × Semantic) + (0.25 × Keyword) + (0.15 × Concept)\n",
    "Marks = Final Score × Max Marks\n",
    "```\n",
    "\n",
    "**Why these weights?**\n",
    "- 60% Semantic: Conceptual understanding matters most\n",
    "- 25% Keyword: Technical terminology important but not everything\n",
    "- 15% Concept: Ensures comprehensive coverage\n",
    "\n",
    "**Process Flow:**\n",
    "1. Preprocess student and model answers\n",
    "2. Run keyword matching (parallel)\n",
    "3. Run semantic analysis (parallel)\n",
    "4. Run concept detection (parallel)\n",
    "5. Combine scores with weights\n",
    "6. Apply penalties for critical gaps\n",
    "7. Convert to marks and percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e64ba",
   "metadata": {},
   "source": [
    "## Complete Source Code\n",
    "\n",
    "Below is the full implementation with inline documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation Engine\n",
    "Main evaluation logic combining keywords, semantics, and concepts\n",
    "\"\"\"\n",
    "\n",
    "from .keyword_matcher import KeywordMatcher\n",
    "from .semantic_analyzer import SemanticAnalyzer\n",
    "from .concept_detector import ConceptDetector\n",
    "from .nlp_preprocessor import NLPPreprocessor\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class EvaluationEngine:\n",
    "    \"\"\"Core evaluation engine combining multiple analysis methods\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"\n",
    "        Initialize evaluation engine\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration object with weights and thresholds\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize components\n",
    "        self.preprocessor = NLPPreprocessor()\n",
    "        self.keyword_matcher = KeywordMatcher()\n",
    "        self.semantic_analyzer = SemanticAnalyzer()\n",
    "        self.concept_detector = ConceptDetector()\n",
    "        \n",
    "        # Default weights\n",
    "        self.semantic_weight = 0.60\n",
    "        self.keyword_weight = 0.25\n",
    "        self.concept_weight = 0.15\n",
    "        \n",
    "        if config:\n",
    "            self.semantic_weight = getattr(config, 'SEMANTIC_WEIGHT', 0.60)\n",
    "            self.keyword_weight = getattr(config, 'KEYWORD_WEIGHT', 0.25)\n",
    "            self.concept_weight = getattr(config, 'CONCEPT_WEIGHT', 0.15)\n",
    "        \n",
    "        logger.info(f\"Evaluation weights - Semantic: {self.semantic_weight}, \"\n",
    "                   f\"Keyword: {self.keyword_weight}, Concept: {self.concept_weight}\")\n",
    "    \n",
    "    def evaluate(self, student_answer: str, model_answer: str, \n",
    "                question_data: Dict = None, max_marks: float = 10.0) -> Dict:\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of student answer\n",
    "        \n",
    "        Args:\n",
    "            student_answer: Student's answer text\n",
    "            model_answer: Model answer text\n",
    "            question_data: Dictionary with keywords, concepts, etc.\n",
    "            max_marks: Maximum marks for the question\n",
    "            \n",
    "        Returns:\n",
    "            Detailed evaluation results\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting evaluation...\")\n",
    "        \n",
    "        # Preprocess texts\n",
    "        student_processed = self.preprocessor.preprocess(\n",
    "            student_answer, \n",
    "            pipeline=['clean', 'lemmatize', 'keywords']\n",
    "        )\n",
    "        model_processed = self.preprocessor.preprocess(\n",
    "            model_answer,\n",
    "            pipeline=['clean', 'lemmatize', 'keywords']\n",
    "        )\n",
    "        \n",
    "        student_tokens = student_processed[\"lemmas\"]\n",
    "        model_tokens = model_processed[\"lemmas\"]\n",
    "        \n",
    "        # Extract model keywords\n",
    "        model_keywords = [kw[0] for kw in model_processed[\"keywords\"]]\n",
    "        if question_data and \"keywords\" in question_data:\n",
    "            model_keywords = question_data[\"keywords\"]\n",
    "        \n",
    "        # 1. Keyword Matching\n",
    "        logger.info(\"Performing keyword matching...\")\n",
    "        keyword_result = self.keyword_matcher.fuzzy_match(student_tokens, model_keywords)\n",
    "        keyword_score = keyword_result[\"coverage_score\"]\n",
    "        \n",
    "        # 2. Semantic Similarity\n",
    "        logger.info(\"Calculating semantic similarity...\")\n",
    "        semantic_result = self.semantic_analyzer.evaluate_answer(\n",
    "            student_answer, model_answer\n",
    "        )\n",
    "        semantic_score = semantic_result[\"semantic_score\"]\n",
    "        \n",
    "        # 3. Concept Detection\n",
    "        concept_score = 0.5  # Default if no concepts provided\n",
    "        concept_result = {}\n",
    "        \n",
    "        if question_data and \"concepts\" in question_data:\n",
    "            logger.info(\"Detecting concepts...\")\n",
    "            concept_result = self.concept_detector.detect_concepts(\n",
    "                student_tokens, \n",
    "                question_data[\"concepts\"]\n",
    "            )\n",
    "            concept_score = self.concept_detector.calculate_concept_score(concept_result)\n",
    "        \n",
    "        # Calculate weighted final score\n",
    "        final_score = (\n",
    "            self.semantic_weight * semantic_score +\n",
    "            self.keyword_weight * keyword_score +\n",
    "            self.concept_weight * concept_score\n",
    "        )\n",
    "        \n",
    "        # Convert to marks\n",
    "        marks_obtained = final_score * max_marks\n",
    "        \n",
    "        # Compile results\n",
    "        evaluation_result = {\n",
    "            \"student_answer\": student_answer,\n",
    "            \"model_answer\": model_answer,\n",
    "            \"max_marks\": max_marks,\n",
    "            \"marks_obtained\": round(marks_obtained, 2),\n",
    "            \"percentage\": round(final_score * 100, 2),\n",
    "            \"scores\": {\n",
    "                \"semantic\": {\n",
    "                    \"score\": round(semantic_score, 3),\n",
    "                    \"weight\": self.semantic_weight,\n",
    "                    \"contribution\": round(semantic_score * self.semantic_weight, 3),\n",
    "                    \"details\": semantic_result\n",
    "                },\n",
    "                \"keyword\": {\n",
    "                    \"score\": round(keyword_score, 3),\n",
    "                    \"weight\": self.keyword_weight,\n",
    "                    \"contribution\": round(keyword_score * self.keyword_weight, 3),\n",
    "                    \"details\": keyword_result\n",
    "                },\n",
    "                \"concept\": {\n",
    "                    \"score\": round(concept_score, 3),\n",
    "                    \"weight\": self.concept_weight,\n",
    "                    \"contribution\": round(concept_score * self.concept_weight, 3),\n",
    "                    \"details\": concept_result\n",
    "                }\n",
    "            },\n",
    "            \"final_score\": round(final_score, 3),\n",
    "            \"processed_data\": {\n",
    "                \"student_tokens_count\": len(student_tokens),\n",
    "                \"model_tokens_count\": len(model_tokens),\n",
    "                \"student_keywords\": student_processed.get(\"keywords\", [])[:10]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Evaluation complete - Marks: {marks_obtained}/{max_marks} ({final_score*100:.1f}%)\")\n",
    "        \n",
    "        return evaluation_result\n",
    "    \n",
    "    def batch_evaluate(self, evaluations: List[Dict], max_marks: float = 10.0) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Evaluate multiple student answers\n",
    "        \n",
    "        Args:\n",
    "            evaluations: List of dicts with 'student_answer', 'model_answer', 'question_data'\n",
    "            max_marks: Maximum marks per question\n",
    "            \n",
    "        Returns:\n",
    "            List of evaluation results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for i, eval_data in enumerate(evaluations):\n",
    "            logger.info(f\"Evaluating answer {i+1}/{len(evaluations)}\")\n",
    "            result = self.evaluate(\n",
    "                eval_data[\"student_answer\"],\n",
    "                eval_data[\"model_answer\"],\n",
    "                eval_data.get(\"question_data\"),\n",
    "                max_marks\n",
    "            )\n",
    "            result[\"question_number\"] = i + 1\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c52ce1",
   "metadata": {},
   "source": [
    "## Testing the Module\n",
    "\n",
    "Let's test this module to see it in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab799a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_engine import EvaluationEngine\n",
    "from config import Config\n",
    "\n",
    "engine = EvaluationEngine(config=Config)\n",
    "print(\"Evaluation Engine initialized\")\n",
    "print(f\"Weights: Semantic={engine.semantic_weight}, Keyword={engine.keyword_weight}, Concept={engine.concept_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97b11ef",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This module is a critical component of the AI-based answer evaluation system. It provides:\n",
    "\n",
    "- **Evaluation Engine Module** functionality\n",
    "- Clear, well-documented code\n",
    "- Error handling and robustness\n",
    "- Integration with other system modules\n",
    "\n",
    "**Next Steps**: Explore other module notebooks to understand the complete system!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
