{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9381e866",
   "metadata": {},
   "source": [
    "# NLP Preprocessing Module - Detailed Walkthrough\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Cleans and normalizes text through tokenization, lemmatization, and stopword removal.\n",
    "Prepares text for accurate analysis by evaluation components.\n",
    "\n",
    "## Why This Module Exists\n",
    "\n",
    "**Why preprocessing is essential:**\n",
    "- Raw text contains noise (punctuation, extra spaces)\n",
    "- Word variations need normalization (running -> run)\n",
    "- Stopwords dilute analysis importance\n",
    "- Consistent format improves accuracy\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**Pipeline Components:**\n",
    "1. **Tokenization**: Split into sentences and words (NLTK)\n",
    "2. **Lemmatization**: Convert to root forms using spaCy (more accurate than stemming)\n",
    "3. **Stopword Removal**: Filter common words like \"the\", \"is\", \"a\"\n",
    "4. **Text Cleaning**: Remove special characters and normalize whitespace\n",
    "5. **Keyword Extraction**: Identify important terms by frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec704f4b",
   "metadata": {},
   "source": [
    "## Complete Source Code\n",
    "\n",
    "Below is the full implementation with inline documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4de54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NLP Preprocessing Module\n",
    "Handles tokenization, lemmatization, and text normalization\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from typing import List, Dict\n",
    "import re\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NLPPreprocessor:\n",
    "    \"\"\"Preprocess text using NLP techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, spacy_model=\"en_core_web_sm\"):\n",
    "        \"\"\"\n",
    "        Initialize NLP preprocessor\n",
    "        \n",
    "        Args:\n",
    "            spacy_model: Name of spaCy model to use\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.nlp = spacy.load(spacy_model)\n",
    "            logger.info(f\"Loaded spaCy model: {spacy_model}\")\n",
    "        except OSError:\n",
    "            logger.warning(f\"spaCy model '{spacy_model}' not found. Download it using:\")\n",
    "            logger.warning(f\"python -m spacy download {spacy_model}\")\n",
    "            self.nlp = None\n",
    "        \n",
    "        # Download required NLTK data\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "        except LookupError:\n",
    "            logger.info(\"Downloading NLTK punkt tokenizer...\")\n",
    "            nltk.download('punkt', quiet=True)\n",
    "        \n",
    "        try:\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "        except LookupError:\n",
    "            logger.info(\"Downloading NLTK stopwords...\")\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "        \n",
    "        self.stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Basic text cleaning\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove special characters but keep basic punctuation\n",
    "        text = re.sub(r'[^\\w\\s.,!?;:()\\-]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def tokenize_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into sentences\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            List of sentences\n",
    "        \"\"\"\n",
    "        try:\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            return [s.strip() for s in sentences if s.strip()]\n",
    "        except:\n",
    "            # Fallback: simple split\n",
    "            return [s.strip() for s in text.split('.') if s.strip()]\n",
    "    \n",
    "    def tokenize_words(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into words\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            List of words\n",
    "        \"\"\"\n",
    "        try:\n",
    "            words = nltk.word_tokenize(text)\n",
    "            return words\n",
    "        except:\n",
    "            # Fallback: simple split\n",
    "            return text.split()\n",
    "    \n",
    "    def lemmatize(self, text: str, remove_stopwords=True, lowercase=True) -> List[str]:\n",
    "        \"\"\"\n",
    "        Lemmatize text using spaCy\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            remove_stopwords: Remove stopwords if True\n",
    "            lowercase: Convert to lowercase if True\n",
    "            \n",
    "        Returns:\n",
    "            List of lemmatized tokens\n",
    "        \"\"\"\n",
    "        if self.nlp is None:\n",
    "            logger.error(\"spaCy model not loaded. Cannot lemmatize.\")\n",
    "            return self.tokenize_words(text.lower() if lowercase else text)\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Skip punctuation and whitespace\n",
    "            if token.is_punct or token.is_space:\n",
    "                continue\n",
    "            \n",
    "            # Skip stopwords if requested\n",
    "            if remove_stopwords and token.text.lower() in self.stop_words:\n",
    "                continue\n",
    "            \n",
    "            # Get lemma\n",
    "            lemma = token.lemma_.lower() if lowercase else token.lemma_\n",
    "            tokens.append(lemma)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def extract_keywords(self, text: str, top_n=10) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        Extract important keywords using simple frequency\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            top_n: Number of top keywords to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (word, frequency) tuples\n",
    "        \"\"\"\n",
    "        tokens = self.lemmatize(text, remove_stopwords=True)\n",
    "        \n",
    "        # Count frequency\n",
    "        freq_dist = {}\n",
    "        for token in tokens:\n",
    "            if len(token) > 2:  # Ignore very short words\n",
    "                freq_dist[token] = freq_dist.get(token, 0) + 1\n",
    "        \n",
    "        # Sort by frequency\n",
    "        sorted_keywords = sorted(freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_keywords[:top_n]\n",
    "    \n",
    "    def preprocess(self, text: str, pipeline=['clean', 'lemmatize']) -> Dict:\n",
    "        \"\"\"\n",
    "        Full preprocessing pipeline\n",
    "        \n",
    "        Args:\n",
    "            text: Raw input text\n",
    "            pipeline: List of steps to apply\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processed results\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            \"original\": text,\n",
    "            \"cleaned\": text,\n",
    "            \"sentences\": [],\n",
    "            \"tokens\": [],\n",
    "            \"lemmas\": [],\n",
    "            \"keywords\": []\n",
    "        }\n",
    "        \n",
    "        if 'clean' in pipeline:\n",
    "            result[\"cleaned\"] = self.clean_text(text)\n",
    "        \n",
    "        if 'sentences' in pipeline:\n",
    "            result[\"sentences\"] = self.tokenize_sentences(result[\"cleaned\"])\n",
    "        \n",
    "        if 'tokenize' in pipeline:\n",
    "            result[\"tokens\"] = self.tokenize_words(result[\"cleaned\"])\n",
    "        \n",
    "        if 'lemmatize' in pipeline:\n",
    "            result[\"lemmas\"] = self.lemmatize(result[\"cleaned\"])\n",
    "        \n",
    "        if 'keywords' in pipeline:\n",
    "            result[\"keywords\"] = self.extract_keywords(result[\"cleaned\"])\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def quick_preprocess(text: str) -> List[str]:\n",
    "    \"\"\"Quick preprocessing for simple use cases\"\"\"\n",
    "    preprocessor = NLPPreprocessor()\n",
    "    return preprocessor.lemmatize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f0ca3",
   "metadata": {},
   "source": [
    "## Testing the Module\n",
    "\n",
    "Let's test this module to see it in action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63495806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_preprocessor import NLPPreprocessor\n",
    "\n",
    "preprocessor = NLPPreprocessor()\n",
    "sample_text = \"Binary Search Trees are efficient data structures used for searching.\"\n",
    "\n",
    "processed = preprocessor.preprocess(sample_text, pipeline=['clean', 'lemmatize', 'keywords'])\n",
    "print(f\"Cleaned: {processed['cleaned']}\")\n",
    "print(f\"Lemmas: {processed['lemmas']}\")\n",
    "print(f\"Keywords: {processed['keywords']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe3066",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This module is a critical component of the AI-based answer evaluation system. It provides:\n",
    "\n",
    "- **NLP Preprocessing Module** functionality\n",
    "- Clear, well-documented code\n",
    "- Error handling and robustness\n",
    "- Integration with other system modules\n",
    "\n",
    "**Next Steps**: Explore other module notebooks to understand the complete system!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
