{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AI-Based Answer Evaluation System\n",
                "## Complete Implementation and Demonstration\n",
                "\n",
                "This notebook demonstrates the complete AI-based written answer evaluation system for online examination platforms.\n",
                "\n",
                "### Features:\n",
                "- PDF text extraction\n",
                "- NLP preprocessing (tokenization, lemmatization, stopword removal)\n",
                "- Keyword-based matching with fuzzy logic\n",
                "- Semantic similarity analysis using sentence transformers\n",
                "- Concept coverage detection\n",
                "- Automated marking with partial credit\n",
                "- Constructive feedback generation\n",
                "- Topic-wise performance analysis\n",
                "\n",
                "### Table of Contents:\n",
                "1. [Setup and Installation](#setup)\n",
                "2. [Module Overview](#modules)\n",
                "3. [Single Answer Evaluation](#single)\n",
                "4. [Batch Evaluation](#batch)\n",
                "5. [Performance Analysis](#performance)\n",
                "6. [Custom Configuration](#config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Installation <a id='setup'></a>\n",
                "\n",
                "First, let's install all required dependencies and download necessary models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (run once)\n",
                "!pip install -q pdfplumber nltk spacy sentence-transformers scikit-learn fuzzywuzzy python-Levenshtein reportlab matplotlib seaborn\n",
                "\n",
                "# Download spaCy model\n",
                "!python -m spacy download en_core_web_sm\n",
                "\n",
                "print(\"‚úì All dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download NLTK data\n",
                "import nltk\n",
                "nltk.download('punkt')\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "\n",
                "print(\"‚úì NLTK data downloaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create sample PDFs for testing\n",
                "import sys\n",
                "sys.path.append('.')\n",
                "\n",
                "!python create_sample_pdfs.py\n",
                "\n",
                "print(\"‚úì Sample PDFs created!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Module Overview <a id='modules'></a>\n",
                "\n",
                "Let's import and explore our evaluation system modules."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import all modules\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "# Add src to path\n",
                "sys.path.insert(0, str(Path('.').absolute() / 'src'))\n",
                "\n",
                "from config import Config\n",
                "from pdf_processor import PDFProcessor\n",
                "from nlp_preprocessor import NLPPreprocessor\n",
                "from keyword_matcher import KeywordMatcher\n",
                "from semantic_analyzer import SemanticAnalyzer\n",
                "from concept_detector import ConceptDetector\n",
                "from evaluation_engine import EvaluationEngine\n",
                "from feedback_generator import FeedbackGenerator\n",
                "from performance_analyzer import PerformanceAnalyzer\n",
                "\n",
                "import json\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(\"‚úì All modules imported successfully!\")\n",
                "print(f\"\\nConfiguration:\")\n",
                "print(f\"  Semantic Weight: {Config.SEMANTIC_WEIGHT*100}%\")\n",
                "print(f\"  Keyword Weight: {Config.KEYWORD_WEIGHT*100}%\")\n",
                "print(f\"  Concept Weight: {Config.CONCEPT_WEIGHT*100}%\")\n",
                "print(f\"  Transformer Model: {Config.SENTENCE_TRANSFORMER_MODEL}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Test Individual Modules\n",
                "\n",
                "Let's test each module independently."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test PDF Processor\n",
                "pdf_processor = PDFProcessor()\n",
                "sample_pdf = \"data/sample_student_answers/Q1_Student_Excellent.pdf\"\n",
                "\n",
                "result = pdf_processor.extract_text(sample_pdf)\n",
                "print(\"üìÑ PDF Processing Test:\")\n",
                "print(f\"  Status: {'‚úì Success' if result['success'] else '‚úó Failed'}\")\n",
                "print(f\"  Pages: {result['pages']}\")\n",
                "print(f\"  Text Length: {len(result['text'])} characters\")\n",
                "print(f\"\\n  First 200 characters:\\n  {result['text'][:200]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test NLP Preprocessor\n",
                "preprocessor = NLPPreprocessor()\n",
                "sample_text = result['text']\n",
                "\n",
                "processed = preprocessor.preprocess(sample_text, pipeline=['clean', 'lemmatize', 'keywords'])\n",
                "\n",
                "print(\"üî§ NLP Preprocessing Test:\")\n",
                "print(f\"  Cleaned Text Length: {len(processed['cleaned'])} chars\")\n",
                "print(f\"  Lemmatized Tokens: {len(processed['lemmas'])}\")\n",
                "print(f\"\\n  Top Keywords:\")\n",
                "for kw, freq in processed['keywords'][:10]:\n",
                "    print(f\"    {kw}: {freq}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Semantic Analyzer\n",
                "print(\"üß† Loading Semantic Analyzer (this may take a minute)...\")\n",
                "semantic_analyzer = SemanticAnalyzer()\n",
                "\n",
                "text1 = \"Binary search tree is a data structure\"\n",
                "text2 = \"BST is a hierarchical data structure\"\n",
                "\n",
                "similarity = semantic_analyzer.calculate_similarity(text1, text2)\n",
                "print(f\"\\n  Similarity between texts: {similarity:.3f}\")\n",
                "print(f\"  Status: ‚úì Semantic analyzer working!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Single Answer Evaluation <a id='single'></a>\n",
                "\n",
                "Now let's evaluate a single student answer end-to-end."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load knowledge base and model answer\n",
                "with open('data/knowledge_base.json', 'r') as f:\n",
                "    knowledge_base = json.load(f)\n",
                "\n",
                "with open('data/model_answers/Q1_model_answer.txt', 'r') as f:\n",
                "    model_answer = f.read()\n",
                "\n",
                "# Load question data\n",
                "question_data = knowledge_base['questions']['Q1']\n",
                "\n",
                "print(\"üìö Loaded Knowledge Base\")\n",
                "print(f\"\\nQuestion: {question_data['question_text']}\")\n",
                "print(f\"Topic: {question_data['topic']}\")\n",
                "print(f\"Max Marks: {question_data['max_marks']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract student answer from PDF\n",
                "student_pdf = \"data/sample_student_answers/Q1_Student_Good.pdf\"\n",
                "pdf_result = pdf_processor.extract_text(student_pdf)\n",
                "student_answer = pdf_result['text']\n",
                "\n",
                "print(f\"üìù Student Answer Extracted:\")\n",
                "print(f\"\\n{student_answer}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize evaluation engine\n",
                "engine = EvaluationEngine(config=Config)\n",
                "engine.concept_detector.knowledge_base = knowledge_base\n",
                "\n",
                "print(\"üîß Evaluation Engine Initialized\")\n",
                "print(\"\\n‚öôÔ∏è Evaluating answer...\\n\")\n",
                "\n",
                "# Perform evaluation\n",
                "evaluation_result = engine.evaluate(\n",
                "    student_answer=student_answer,\n",
                "    model_answer=model_answer,\n",
                "    question_data=question_data,\n",
                "    max_marks=10.0\n",
                ")\n",
                "\n",
                "print(\"‚úì Evaluation Complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display evaluation results\n",
                "print(\"‚ïê\" * 70)\n",
                "print(\"üìä EVALUATION RESULTS\")\n",
                "print(\"‚ïê\" * 70)\n",
                "print(f\"\\nMarks Obtained: {evaluation_result['marks_obtained']}/{evaluation_result['max_marks']}\")\n",
                "print(f\"Percentage: {evaluation_result['percentage']}%\")\n",
                "print(f\"Final Score: {evaluation_result['final_score']:.3f}\")\n",
                "\n",
                "print(\"\\n\" + \"‚îÄ\" * 70)\n",
                "print(\"Component Breakdown:\")\n",
                "print(\"‚îÄ\" * 70)\n",
                "\n",
                "for component, data in evaluation_result['scores'].items():\n",
                "    print(f\"\\n{component.upper()}:\")\n",
                "    print(f\"  Score: {data['score']:.3f}\")\n",
                "    print(f\"  Weight: {data['weight']*100}%\")\n",
                "    print(f\"  Contribution: {data['contribution']:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate feedback\n",
                "feedback_gen = FeedbackGenerator()\n",
                "feedback = feedback_gen.generate_feedback(evaluation_result, verbose=False)\n",
                "\n",
                "# Display formatted feedback\n",
                "feedback_text = feedback_gen.format_feedback_text(feedback)\n",
                "print(feedback_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Batch Evaluation <a id='batch'></a>\n",
                "\n",
                "Evaluate multiple student answers at once."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get all student PDFs for Q1\n",
                "from pathlib import Path\n",
                "\n",
                "pdf_dir = Path(\"data/sample_student_answers\")\n",
                "student_pdfs = list(pdf_dir.glob(\"Q1_Student_*.pdf\"))\n",
                "\n",
                "print(f\"üìÅ Found {len(student_pdfs)} student answers to evaluate:\\n\")\n",
                "for pdf in student_pdfs:\n",
                "    print(f\"  ‚Ä¢ {pdf.name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Batch evaluation\n",
                "batch_results = []\n",
                "\n",
                "print(\"\\n‚öôÔ∏è Starting batch evaluation...\\n\")\n",
                "print(\"‚îÄ\" * 70)\n",
                "\n",
                "for i, pdf_path in enumerate(sorted(student_pdfs), 1):\n",
                "    print(f\"\\nEvaluating {pdf_path.name}...\")\n",
                "    \n",
                "    # Extract text\n",
                "    pdf_result = pdf_processor.extract_text(str(pdf_path))\n",
                "    if not pdf_result['success']:\n",
                "        print(f\"  ‚úó Error: {pdf_result['error']}\")\n",
                "        continue\n",
                "    \n",
                "    # Evaluate\n",
                "    result = engine.evaluate(\n",
                "        student_answer=pdf_result['text'],\n",
                "        model_answer=model_answer,\n",
                "        question_data=question_data,\n",
                "        max_marks=10.0\n",
                "    )\n",
                "    \n",
                "    result['student_name'] = pdf_path.stem  # Use filename as student ID\n",
                "    batch_results.append(result)\n",
                "    \n",
                "    print(f\"  ‚úì Marks: {result['marks_obtained']}/10 ({result['percentage']:.1f}%)\")\n",
                "\n",
                "print(\"\\n\" + \"‚îÄ\" * 70)\n",
                "print(f\"‚úì Batch evaluation complete! Evaluated {len(batch_results)} answers.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display batch results summary\n",
                "import pandas as pd\n",
                "\n",
                "summary_data = []\n",
                "for result in batch_results:\n",
                "    summary_data.append({\n",
                "        'Student': result['student_name'].replace('Q1_Student_', ''),\n",
                "        'Marks': f\"{result['marks_obtained']}/10\",\n",
                "        'Percentage': f\"{result['percentage']:.1f}%\",\n",
                "        'Semantic': f\"{result['scores']['semantic']['score']*100:.1f}%\",\n",
                "        'Keyword': f\"{result['scores']['keyword']['score']*100:.1f}%\",\n",
                "        'Concept': f\"{result['scores']['concept']['score']*100:.1f}%\"\n",
                "    })\n",
                "\n",
                "df = pd.DataFrame(summary_data)\n",
                "print(\"\\nüìä BATCH EVALUATION SUMMARY\")\n",
                "print(\"‚ïê\" * 100)\n",
                "print(df.to_string(index=False))\n",
                "print(\"‚ïê\" * 100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Performance Analysis <a id='performance'></a>\n",
                "\n",
                "Generate topic-wise performance analysis and student profiles."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Performance analysis\n",
                "perf_analyzer = PerformanceAnalyzer()\n",
                "\n",
                "performance = perf_analyzer.analyze_single_exam(batch_results)\n",
                "\n",
                "print(\"üìà PERFORMANCE ANALYSIS\")\n",
                "print(\"‚ïê\" * 70)\n",
                "\n",
                "overall = performance['overall_performance']\n",
                "print(f\"\\nOverall Performance:\")\n",
                "print(f\"  Total Marks: {overall['obtained_marks']}/{overall['total_marks']}\")\n",
                "print(f\"  Percentage: {overall['percentage']}%\")\n",
                "print(f\"  Grade: {overall['grade']}\")\n",
                "\n",
                "components = performance['component_performance']\n",
                "print(f\"\\nComponent Averages:\")\n",
                "print(f\"  Semantic: {components['semantic_avg']}%\")\n",
                "print(f\"  Keyword: {components['keyword_avg']}%\")\n",
                "print(f\"  Concept: {components['concept_avg']}%\")\n",
                "\n",
                "if performance['strong_areas']:\n",
                "    print(f\"\\n‚úì Strong Areas:\")\n",
                "    for area in performance['strong_areas']:\n",
                "        print(f\"  ‚Ä¢ {area['concept']} ({area['average_coverage']}%)\")\n",
                "\n",
                "if performance['weak_areas']:\n",
                "    print(f\"\\n‚úó Weak Areas:\")\n",
                "    for area in performance['weak_areas']:\n",
                "        print(f\"  ‚Ä¢ {area['concept']} ({area['average_coverage']}%)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate student profile for first student\n",
                "if batch_results:\n",
                "    # Create performance data for single student\n",
                "    single_student = [batch_results[0]]\n",
                "    student_perf = perf_analyzer.analyze_single_exam(single_student)\n",
                "    \n",
                "    profile = perf_analyzer.generate_student_profile(\n",
                "        student_perf,\n",
                "        student_id=batch_results[0]['student_name']\n",
                "    )\n",
                "    \n",
                "    print(\"\\nüë§ STUDENT PROFILE\")\n",
                "    print(\"‚ïê\" * 70)\n",
                "    print(f\"Student ID: {profile['student_id']}\")\n",
                "    print(f\"Grade: {profile['overall_grade']} ({profile['percentage']}%)\")\n",
                "    print(f\"\\n{profile['performance_summary']}\")\n",
                "    \n",
                "    print(f\"\\n‚úì Strengths:\")\n",
                "    for strength in profile['strengths']:\n",
                "        print(f\"  ‚Ä¢ {strength}\")\n",
                "    \n",
                "    print(f\"\\nüìö Recommendations:\")\n",
                "    for rec in profile['recommendations']:\n",
                "        print(f\"  [{rec['priority']}] {rec['area']}: {rec['suggestion']}\")\n",
                "    \n",
                "    # Export profile\n",
                "    output_path = \"output/student_profile.json\"\n",
                "    Path(\"output\").mkdir(exist_ok=True)\n",
                "    perf_analyzer.export_report(profile, output_path)\n",
                "    print(f\"\\n‚úì Profile exported to: {output_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Custom Configuration <a id='config'></a>\n",
                "\n",
                "Customize evaluation weights and parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try different weight configurations\n",
                "print(\"üîß Testing Different Weight Configurations\\n\")\n",
                "\n",
                "test_configs = [\n",
                "    {\"name\": \"Semantic-Heavy\", \"semantic\": 0.7, \"keyword\": 0.2, \"concept\": 0.1},\n",
                "    {\"name\": \"Balanced\", \"semantic\": 0.5, \"keyword\": 0.3, \"concept\": 0.2},\n",
                "    {\"name\": \"Keyword-Heavy\", \"semantic\": 0.4, \"keyword\": 0.4, \"concept\": 0.2},\n",
                "]\n",
                "\n",
                "# Use first student answer for testing\n",
                "test_answer = batch_results[0]['student_answer']\n",
                "\n",
                "comparison_results = []\n",
                "\n",
                "for config in test_configs:\n",
                "    # Update config\n",
                "    Config.update_weights(\n",
                "        semantic=config['semantic'],\n",
                "        keyword=config['keyword'],\n",
                "        concept=config['concept']\n",
                "    )\n",
                "    \n",
                "    # Re-initialize engine\n",
                "    test_engine = EvaluationEngine(config=Config)\n",
                "    test_engine.concept_detector.knowledge_base = knowledge_base\n",
                "    \n",
                "    # Evaluate\n",
                "    result = test_engine.evaluate(\n",
                "        student_answer=test_answer,\n",
                "        model_answer=model_answer,\n",
                "        question_data=question_data,\n",
                "        max_marks=10.0\n",
                "    )\n",
                "    \n",
                "    comparison_results.append({\n",
                "        'Configuration': config['name'],\n",
                "        'Weights': f\"S:{config['semantic']:.1f} K:{config['keyword']:.1f} C:{config['concept']:.1f}\",\n",
                "        'Marks': f\"{result['marks_obtained']:.2f}/10\",\n",
                "        'Percentage': f\"{result['percentage']:.1f}%\"\n",
                "    })\n",
                "\n",
                "df_comparison = pd.DataFrame(comparison_results)\n",
                "print(df_comparison.to_string(index=False))\n",
                "\n",
                "print(\"\\nüí° Different weight configurations can significantly impact final marks!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook demonstrated the complete AI-based answer evaluation system with:\n",
                "\n",
                "‚úì Automated PDF processing and text extraction  \n",
                "‚úì Advanced NLP preprocessing  \n",
                "‚úì Multi-faceted evaluation (semantic + keyword + concept)  \n",
                "‚úì Constructive feedback generation  \n",
                "‚úì Performance analysis and student profiling  \n",
                "‚úì Configurable marking schemes  \n",
                "\n",
                "### Next Steps:\n",
                "1. Upload your own question bank and model answers\n",
                "2. Customize the knowledge base for your subjects\n",
                "3. Adjust weights based on your marking philosophy\n",
                "4. Integrate with web platform using Flask/FastAPI\n",
                "5. Add OCR support for handwritten answers\n",
                "6. Implement LLM-based feedback enhancement"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}