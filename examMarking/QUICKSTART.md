# AI Answer Evaluation System - Quick Start Guide

## üöÄ Get Started in 3 Steps

### Step 1: Install Dependencies

```bash
cd c:\Users\ASUS\Desktop\examMarking
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

### Step 2: Create Sample PDFs

```bash
python create_sample_pdfs.py
```

### Step 3: Run the Jupyter Notebook

```bash
jupyter notebook
# Then open: AI_Answer_Evaluation_Complete.ipynb
```

---

## üìÇ What's Included

### Core Modules (src/)
1. **config.py** - Configuration management
2. **pdf_processor.py** - PDF text extraction
3. **nlp_preprocessor.py** - Text preprocessing
4. **keyword_matcher.py** - Keyword matching
5. **semantic_analyzer.py** - Semantic similarity
6. **concept_detector.py** - Concept detection
7. **evaluation_engine.py** - Main evaluation logic
8. **feedback_generator.py** - Feedback generation
9. **performance_analyzer.py** - Performance tracking

### Data Files
- **knowledge_base.json** - 3 sample questions (BST, Normalization, Scheduling)
- **model_answers/** - Reference answers for all questions
- **sample_student_answers/** - Test PDFs (generated by script)

### Documentation
- **README.md** - Complete documentation
- **AI_Answer_Evaluation_Complete.ipynb** - Interactive demonstration
- **walkthrough.md** - Implementation details

---

## üéØ Quick Demo

```python
from src.evaluation_engine import EvaluationEngine
from src.pdf_processor import PDFProcessor
from src.feedback_generator import FeedbackGenerator
import json

# Load knowledge base
with open('data/knowledge_base.json') as f:
    kb = json.load(f)

# Load model answer
with open('data/model_answers/Q1_model_answer.txt') as f:
    model_answer = f.read()

# Extract student answer from PDF
pdf = PDFProcessor()
result = pdf.extract_text('data/sample_student_answers/Q1_Student_Good.pdf')
student_answer = result['text']

# Evaluate
engine = EvaluationEngine()
engine.concept_detector.knowledge_base = kb

evaluation = engine.evaluate(
    student_answer=student_answer,
    model_answer=model_answer,
    question_data=kb['questions']['Q1'],
    max_marks=10.0
)

# Generate feedback
feedback_gen = FeedbackGenerator()
feedback = feedback_gen.generate_feedback(evaluation)
print(feedback_gen.format_feedback_text(feedback))

print(f"\n‚úÖ Final Score: {evaluation['marks_obtained']}/10 ({evaluation['percentage']}%)")
```

---

## üìä Evaluation Weights

- **Semantic Similarity**: 60% (Conceptual understanding)
- **Keyword Matching**: 25% (Technical terminology)
- **Concept Coverage**: 15% (Comprehensive answers)

*All weights are configurable in config.py*

---

## üéì Sample Questions Included

1. **Q1**: Binary Search Trees (Data Structures)
2. **Q2**: Database Normalization 1NF-3NF (DBMS)
3. **Q3**: CPU Scheduling Algorithms (Operating Systems)

---

## üìö Key Features

‚úÖ PDF text extraction  
‚úÖ Advanced NLP preprocessing  
‚úÖ Semantic similarity using sentence transformers  
‚úÖ Fuzzy keyword matching  
‚úÖ Concept-based evaluation  
‚úÖ Constructive feedback generation  
‚úÖ Topic-wise performance analysis  
‚úÖ Batch evaluation support  
‚úÖ Fully configurable weights  

---

## üîß Customization

### Change Evaluation Weights

Edit `src/config.py`:

```python
SEMANTIC_WEIGHT = 0.60  # Adjust these values
KEYWORD_WEIGHT = 0.25
CONCEPT_WEIGHT = 0.15
```

### Add New Questions

Edit `data/knowledge_base.json` and add:
- Question text
- Model answer file
- Keywords list
- Concepts with weights

---

## üìñ Full Documentation

- **README.md** - Installation, usage, architecture
- **walkthrough.md** - Complete implementation details
- **Jupyter Notebook** - Interactive step-by-step demo

---

## üöÄ Ready to Deploy!

This system is:
- ‚úÖ Modular and extensible
- ‚úÖ Production-ready code
- ‚úÖ Well-documented
- ‚úÖ Final-year project quality
- ‚úÖ Startup MVP ready

**Next Steps**:
1. Run the Jupyter notebook to see it in action
2. Test with your own questions and answers
3. Customize weights for your grading philosophy
4. Extend with OCR, web API, or LLM integration

---

**Built with ‚ù§Ô∏è using Python, NLP, and Machine Learning**
